{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Modeling instead of classification head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The paper [MATH-SHEPHERD](https://huggingface.co/datasets/peiyi9979/Math-Shepherd) present a more elegant solution, which is use language modeling then directly estimate from the turn tokens. Although their code is not released but it is quite simple to implement compared to my previous method for PRM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use native unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jqi/anaconda3/envs/llmrl/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth: Fast Llama patching release 2024.4\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 4090. Max memory: 23.642 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.2.2. CUDA = 8.9. CUDA Toolkit = 11.8.\n",
      "\\        /    Bfloat16 = TRUE. Xformers = 0.0.25.post1. FA = True.\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  2.46it/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Unsloth 2024.4 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "from transformers import LlamaForSequenceClassification, AutoTokenizer, LlamaForCausalLM\n",
    "import torch\n",
    "import wandb\n",
    "import os\n",
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = False # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = 'unsloth/llama-3-8b', # \"unsloth/tinyllama\" for 16bit loading\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")\n",
    "\n",
    "#Use LoRA to reduce memory usage:\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 32, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 32,\n",
    "    lora_dropout = 0, # Currently only supports dropout = 0\n",
    "    bias = \"none\",    # Currently only supports bias = \"none\"\n",
    "    use_gradient_checkpointing = \"unsloth\", # @@@ IF YOU GET OUT OF MEMORY - set to True @@@\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# prompt = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "# You are a helpful assistant to solve math problems step by step <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "# {}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "# {}\"\"\"\n",
    "\n",
    "# def formatting_prompts_func(examples):\n",
    "#     texts = []\n",
    "    \n",
    "#     for instruction, responses, next_response, rating in zip(examples['instruction'], examples['responses'], examples['next_response'], examples['rating']):\n",
    "#         # Combine all responses and the next response into a single string with newline separation\n",
    "#         combined_responses = \" + \\n\".join(responses) + \" + \\n\" + next_response\n",
    "#         if rating == -1:\n",
    "#             combined_responses = combined_responses + \" - \\n\"\n",
    "#         else:\n",
    "#             combined_responses = combined_responses + \" + \\n\"\n",
    "        \n",
    "#         # Format the text with the prompt template\n",
    "#         text = prompt.format(instruction, combined_responses) \n",
    "#         texts.append(text)\n",
    "\n",
    "    \n",
    "#     return {\"text\": texts,}\n",
    "\n",
    "#     # # Tokenize all texts at once using the tokenizer\n",
    "#     # model_inputs = tokenizer(texts, padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "#     # # Add labels to the model inputs\n",
    "#     # model_inputs['labels'] = labels\n",
    "    \n",
    "#     # return model_inputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we treat it as a language model task, then the incorrect steps that ends there should also be included in the dataset. It means we still need to append all the incorrect steps.\n",
    "\n",
    "We can use a token to represent each step's correctness, such as + and -.\n",
    "For the prediction of the probability of + and - during inference. We use the special token to represent the correctness of the step. We then use softmax to get the probability of correctness.\n",
    "\n",
    "During training, because decoder is auto-regressive, we don't need to predict the correctness of the step. We can just use the token to represent the correctness of the step. We can use the special token to represent the correctness of the step. We then use softmax to get the probability of the special token, which is the correctness of the step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 1015027/1015027 [00:11<00:00, 89580.28 examples/s]\n",
      "Map: 100%|██████████| 369283/369283 [00:16<00:00, 21922.39 examples/s]\n",
      "Map: 100%|██████████| 369283/369283 [00:01<00:00, 189351.47 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "369283"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from datasets import load_dataset\n",
    "\n",
    "# # Load and preprocess the dataset\n",
    "# dataset = load_dataset(\"Birchlabs/openai-prm800k-stepwise-critic\", split='train')\n",
    "# dataset = dataset.filter(lambda x: x['rating'] is not None)  # Filter entries without ratings\n",
    "\n",
    "# #filter out the examples that has 'next_response' in the responses of the solution\n",
    "# dataset = dataset.filter(lambda x: not(x['rating'] == 1 and x['is_solution'] == False))\n",
    "\n",
    "# #convert ratings of 0 to 1 so we have only binary labels\n",
    "# dataset = dataset.map(lambda x: {'rating': 1 if x['rating'] == 0 else x['rating']})\n",
    "\n",
    "# dataset = dataset.map(formatting_prompts_func, batched=True)  # Apply the preprocessing function\n",
    "\n",
    "# len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, None, None]\n"
     ]
    }
   ],
   "source": [
    "good_token = '+'\n",
    "bad_token = '-'\n",
    "step_tag = 'ки'\n",
    "# tokenizer.add_special_tokens({'additional_special_tokens': [good_token, bad_token, step_tag]})\n",
    "print(tokenizer.convert_tokens_to_ids([good_token, bad_token, step_tag]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[128000, 116624, 720]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(' ки \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1000/1000 [00:00<00:00, 10791.57 examples/s]\n",
      "Map: 100%|██████████| 1000/1000 [00:00<00:00, 4940.18 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"peiyi9979/Math-Shepherd\", split='train')\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    inputs = examples[\"input\"]\n",
    "    # Replace the ки with step_tag for each input example\n",
    "    inputs = [input.replace('ки \\n', 'ки \\n') for input in inputs]\n",
    "    return tokenizer(inputs, padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "def tokenize_labels_function(examples):\n",
    "    labels_list = examples[\"label\"]\n",
    "    tokenized_labels = []\n",
    "    \n",
    "    for labels in labels_list:\n",
    "        # Replace the + and - with good_token and bad_token, while keeping them in the solution\n",
    "        labels = labels.replace('+\\n', '+ \\n')\n",
    "        labels = labels.replace('-\\n', '- \\n')\n",
    "        \n",
    "        # # Replace the last token with the appropriate special token\n",
    "        # if labels[-1] == '+':\n",
    "        #     labels = labels[:-1] + good_token\n",
    "        # else:\n",
    "        #     labels = labels[:-1] + bad_token\n",
    "        \n",
    "        tokenized_label = tokenizer(labels, padding=\"max_length\", truncation=True, max_length=512)\n",
    "        tokenized_labels.append(tokenized_label[\"input_ids\"])\n",
    "    \n",
    "    return {\"labels\": tokenized_labels}\n",
    "\n",
    "dataset = dataset.map(tokenize_function, batched=True)\n",
    "dataset = dataset.map(tokenize_labels_function, batched=True)\n",
    "dataset= dataset.remove_columns(['input', 'label', 'task'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([     0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0, 116142,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0, 116142,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i=2\n",
    "torch.tensor(dataset[i]['input_ids']) - torch.tensor(dataset[i]['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[128000,\n",
       " 18820,\n",
       " 295,\n",
       " 21935,\n",
       " 400,\n",
       " 1272,\n",
       " 7682,\n",
       " 414,\n",
       " 369,\n",
       " 220,\n",
       " 18,\n",
       " 4207,\n",
       " 824,\n",
       " 2046,\n",
       " 315,\n",
       " 20064,\n",
       " 30976,\n",
       " 18872,\n",
       " 323,\n",
       " 400,\n",
       " 1591,\n",
       " 7682,\n",
       " 414,\n",
       " 369,\n",
       " 220,\n",
       " 20,\n",
       " 4207,\n",
       " 264,\n",
       " 2046,\n",
       " 315,\n",
       " 27374,\n",
       " 18872,\n",
       " 13,\n",
       " 2650,\n",
       " 1790,\n",
       " 810,\n",
       " 1587,\n",
       " 1364,\n",
       " 8493,\n",
       " 389,\n",
       " 27374,\n",
       " 18872,\n",
       " 1109,\n",
       " 20064,\n",
       " 30976,\n",
       " 18872,\n",
       " 304,\n",
       " 264,\n",
       " 1060,\n",
       " 30,\n",
       " 15166,\n",
       " 220,\n",
       " 16,\n",
       " 25,\n",
       " 54765,\n",
       " 38202,\n",
       " 220,\n",
       " 18,\n",
       " 4207,\n",
       " 489,\n",
       " 220,\n",
       " 20,\n",
       " 4207,\n",
       " 284,\n",
       " 1134,\n",
       " 18,\n",
       " 10,\n",
       " 20,\n",
       " 28,\n",
       " 23,\n",
       " 2511,\n",
       " 23,\n",
       " 4207,\n",
       " 824,\n",
       " 2046,\n",
       " 389,\n",
       " 4731,\n",
       " 18872,\n",
       " 13,\n",
       " 489,\n",
       " 720,\n",
       " 8468,\n",
       " 220,\n",
       " 17,\n",
       " 25,\n",
       " 3005,\n",
       " 38202,\n",
       " 220,\n",
       " 1272,\n",
       " 353,\n",
       " 220,\n",
       " 18,\n",
       " 284,\n",
       " 1134,\n",
       " 1272,\n",
       " 9,\n",
       " 18,\n",
       " 28,\n",
       " 4364,\n",
       " 2511,\n",
       " 4364,\n",
       " 389,\n",
       " 20064,\n",
       " 30976,\n",
       " 18872,\n",
       " 824,\n",
       " 2046,\n",
       " 13,\n",
       " 489,\n",
       " 720,\n",
       " 8468,\n",
       " 220,\n",
       " 18,\n",
       " 25,\n",
       " 3005,\n",
       " 38202,\n",
       " 220,\n",
       " 1591,\n",
       " 353,\n",
       " 220,\n",
       " 20,\n",
       " 284,\n",
       " 1134,\n",
       " 1591,\n",
       " 9,\n",
       " 20,\n",
       " 28,\n",
       " 6860,\n",
       " 2511,\n",
       " 6860,\n",
       " 389,\n",
       " 27374,\n",
       " 18872,\n",
       " 824,\n",
       " 2046,\n",
       " 13,\n",
       " 489,\n",
       " 720,\n",
       " 8468,\n",
       " 220,\n",
       " 19,\n",
       " 25,\n",
       " 54765,\n",
       " 38202,\n",
       " 220,\n",
       " 4364,\n",
       " 489,\n",
       " 220,\n",
       " 6860,\n",
       " 284,\n",
       " 1134,\n",
       " 4364,\n",
       " 10,\n",
       " 6860,\n",
       " 28,\n",
       " 11387,\n",
       " 2511,\n",
       " 11387,\n",
       " 389,\n",
       " 4731,\n",
       " 18872,\n",
       " 824,\n",
       " 2046,\n",
       " 13,\n",
       " 489,\n",
       " 720,\n",
       " 8468,\n",
       " 220,\n",
       " 20,\n",
       " 25,\n",
       " 3005,\n",
       " 38202,\n",
       " 220,\n",
       " 11387,\n",
       " 353,\n",
       " 220,\n",
       " 4103,\n",
       " 284,\n",
       " 1134,\n",
       " 11387,\n",
       " 9,\n",
       " 4103,\n",
       " 28,\n",
       " 8878,\n",
       " 508,\n",
       " 2511,\n",
       " 8878,\n",
       " 508,\n",
       " 389,\n",
       " 4731,\n",
       " 18872,\n",
       " 304,\n",
       " 264,\n",
       " 1060,\n",
       " 13,\n",
       " 578,\n",
       " 4320,\n",
       " 374,\n",
       " 25,\n",
       " 220,\n",
       " 8878,\n",
       " 508,\n",
       " 482,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[128000, 489]\n",
      "[128000, 10]\n",
      "[128000, 489, 220]\n",
      "[128000, 15192, 64, 489, 720]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.encode(f' +'))\n",
    "print(tokenizer.encode(f'+'))\n",
    "print(tokenizer.encode(f' + '))\n",
    "print(tokenizer.encode(f'fasa + \\n'))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[128000, 482]\n",
      "[128000, 12]\n",
      "[128000, 482, 220]\n",
      "[128000, 15192, 64, 482, 720]\n",
      "[128000, 300, 65934, 64, 482]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.encode(f' -'))\n",
    "print(tokenizer.encode(f'-'))\n",
    "print(tokenizer.encode(f' - '))\n",
    "print(tokenizer.encode(f'fasa - \\n'))\n",
    "print(tokenizer.encode(f'asfdsa -'))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[128000, 82, 15192, 489, 720, 67618]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.encode(f'sfas + \\n fas'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset[\"labels\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!set CUDA_LAUNCH_BLOCKING=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using auto half precision backend\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers.utils import logging\n",
    "logging.set_verbosity_info()\n",
    "\n",
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    train_dataset = dataset,\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 4,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_ratio = 0.1,\n",
    "        num_train_epochs = 1,\n",
    "        learning_rate = 2e-6,\n",
    "        fp16 = not torch.cuda.is_bf16_supported(),\n",
    "        bf16 = torch.cuda.is_bf16_supported(),\n",
    "        logging_steps = 10,\n",
    "        save_steps= 5000,\n",
    "        save_total_limit=2,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.1,\n",
    "        lr_scheduler_type = \"cosine\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"checkpoints/llama3-8b-critic-lora-4-28\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #@title Show current memory stats\n",
    "# gpu_stats = torch.cuda.get_device_properties(0)\n",
    "# start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "# max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "# print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "# print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 1,000 | Num Epochs = 1\n",
      "O^O/ \\_/ \\    Batch size per device = 4 | Gradient Accumulation steps = 4\n",
      "\\        /    Total batch size = 16 | Total steps = 62\n",
      " \"-____-\"     Number of trainable parameters = 83,886,080\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjq394\u001b[0m (\u001b[33mneurorunner\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jqi/Github/LLMRL/wandb/run-20240428_231408-u7ebsgah</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/neurorunner/huggingface/runs/u7ebsgah' target=\"_blank\">autumn-music-107</a></strong> to <a href='https://wandb.ai/neurorunner/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/neurorunner/huggingface' target=\"_blank\">https://wandb.ai/neurorunner/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/neurorunner/huggingface/runs/u7ebsgah' target=\"_blank\">https://wandb.ai/neurorunner/huggingface/runs/u7ebsgah</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "Triton Error [CUDA]: device-side assert triggered",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trainer_stats \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/llmrl/lib/python3.10/site-packages/transformers/trainer.py:1780\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1778\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1780\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1781\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1782\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1783\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1784\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1785\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<string>:355\u001b[0m, in \u001b[0;36m_fast_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/llmrl/lib/python3.10/site-packages/transformers/trainer.py:3036\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   3033\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   3035\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 3036\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3038\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mn_gpu \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   3039\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mmean()  \u001b[38;5;66;03m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/llmrl/lib/python3.10/site-packages/transformers/trainer.py:3059\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   3057\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3058\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 3059\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3060\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   3061\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   3062\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/llmrl/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/llmrl/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/llmrl/lib/python3.10/site-packages/accelerate/utils/operations.py:825\u001b[0m, in \u001b[0;36mconvert_outputs_to_fp32.<locals>.forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/llmrl/lib/python3.10/site-packages/accelerate/utils/operations.py:813\u001b[0m, in \u001b[0;36mConvertOutputsToFp32.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 813\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m convert_to_fp32(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/anaconda3/envs/llmrl/lib/python3.10/site-packages/torch/amp/autocast_mode.py:16\u001b[0m, in \u001b[0;36mautocast_decorator.<locals>.decorate_autocast\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_autocast\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m autocast_instance:\n\u001b[0;32m---> 16\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/llmrl/lib/python3.10/site-packages/unsloth/models/llama.py:882\u001b[0m, in \u001b[0;36mPeftModelForCausalLM_fast_forward\u001b[0;34m(self, input_ids, causal_mask, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001b[0m\n\u001b[1;32m    869\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mPeftModelForCausalLM_fast_forward\u001b[39m(\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    871\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    881\u001b[0m ):\n\u001b[0;32m--> 882\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    883\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    884\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    885\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    886\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    887\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    888\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    889\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    891\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    892\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/llmrl/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/llmrl/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/llmrl/lib/python3.10/site-packages/peft/tuners/tuners_utils.py:161\u001b[0m, in \u001b[0;36mBaseTuner.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any):\n\u001b[0;32m--> 161\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/llmrl/lib/python3.10/site-packages/unsloth/models/llama.py:813\u001b[0m, in \u001b[0;36mCausalLM_fast_forward.<locals>._CausalLM_fast_forward\u001b[0;34m(self, input_ids, causal_mask, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, *args, **kwargs)\u001b[0m\n\u001b[1;32m    810\u001b[0m     \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[1;32m    811\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39m_has_no_labels \u001b[38;5;241m=\u001b[39m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 813\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    814\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    815\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    816\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    817\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    818\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    819\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    820\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    821\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    822\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    823\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    824\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    827\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/llmrl/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/llmrl/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/llmrl/lib/python3.10/site-packages/unsloth/models/llama.py:650\u001b[0m, in \u001b[0;36mLlamaModel_fast_forward\u001b[0;34m(self, input_ids, causal_mask, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, *args, **kwargs)\u001b[0m\n\u001b[1;32m    647\u001b[0m past_key_value \u001b[38;5;241m=\u001b[39m past_key_values[idx] \u001b[38;5;28;01mif\u001b[39;00m past_key_values \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    649\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m offloaded_gradient_checkpointing:\n\u001b[0;32m--> 650\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[43mUnsloth_Offloaded_Gradient_Checkpointer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    651\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecoder_layer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    652\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    653\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    654\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    655\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    656\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    657\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    658\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    659\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    661\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m gradient_checkpointing:\n\u001b[1;32m    662\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_custom_forward\u001b[39m(module):\n",
      "File \u001b[0;32m~/anaconda3/envs/llmrl/lib/python3.10/site-packages/torch/autograd/function.py:553\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    550\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    551\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    552\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    555\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[1;32m    556\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    557\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    558\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    559\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    560\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/master/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    561\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/llmrl/lib/python3.10/site-packages/torch/cuda/amp/autocast_mode.py:115\u001b[0m, in \u001b[0;36mcustom_fwd.<locals>.decorate_fwd\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cast_inputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    114\u001b[0m     args[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39m_fwd_used_autocast \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mis_autocast_enabled()\n\u001b[0;32m--> 115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfwd\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    117\u001b[0m     autocast_context \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mis_autocast_enabled()\n",
      "File \u001b[0;32m~/anaconda3/envs/llmrl/lib/python3.10/site-packages/unsloth/models/_utils.py:333\u001b[0m, in \u001b[0;36mUnsloth_Offloaded_Gradient_Checkpointer.forward\u001b[0;34m(ctx, forward_function, hidden_states, *args)\u001b[0m\n\u001b[1;32m    331\u001b[0m saved_hidden_states \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m, non_blocking \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 333\u001b[0m     (output,) \u001b[38;5;241m=\u001b[39m \u001b[43mforward_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    334\u001b[0m ctx\u001b[38;5;241m.\u001b[39msave_for_backward(saved_hidden_states)\n\u001b[1;32m    335\u001b[0m ctx\u001b[38;5;241m.\u001b[39mforward_function \u001b[38;5;241m=\u001b[39m forward_function\n",
      "File \u001b[0;32m~/anaconda3/envs/llmrl/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/llmrl/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/llmrl/lib/python3.10/site-packages/unsloth/models/llama.py:432\u001b[0m, in \u001b[0;36mLlamaDecoderLayer_fast_forward\u001b[0;34m(self, hidden_states, causal_mask, attention_mask, position_ids, past_key_value, output_attentions, use_cache, padding_mask, *args, **kwargs)\u001b[0m\n\u001b[1;32m    430\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    431\u001b[0m     residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[0;32m--> 432\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[43mfast_rms_layernorm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_layernorm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    433\u001b[0m     hidden_states, self_attn_weights, present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn(\n\u001b[1;32m    434\u001b[0m         hidden_states\u001b[38;5;241m=\u001b[39mhidden_states,\n\u001b[1;32m    435\u001b[0m         causal_mask\u001b[38;5;241m=\u001b[39mcausal_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    441\u001b[0m         padding_mask\u001b[38;5;241m=\u001b[39mpadding_mask,\n\u001b[1;32m    442\u001b[0m     )\n\u001b[1;32m    443\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n",
      "File \u001b[0;32m~/anaconda3/envs/llmrl/lib/python3.10/site-packages/unsloth/kernels/rms_layernorm.py:190\u001b[0m, in \u001b[0;36mfast_rms_layernorm\u001b[0;34m(layernorm, X, gemma)\u001b[0m\n\u001b[1;32m    188\u001b[0m W   \u001b[38;5;241m=\u001b[39m layernorm\u001b[38;5;241m.\u001b[39mweight\n\u001b[1;32m    189\u001b[0m eps \u001b[38;5;241m=\u001b[39m layernorm\u001b[38;5;241m.\u001b[39mvariance_epsilon\n\u001b[0;32m--> 190\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mFast_RMS_Layernorm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgemma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/anaconda3/envs/llmrl/lib/python3.10/site-packages/torch/autograd/function.py:553\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    550\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    551\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    552\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    555\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[1;32m    556\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    557\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    558\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    559\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    560\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/master/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    561\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/llmrl/lib/python3.10/site-packages/unsloth/kernels/rms_layernorm.py:144\u001b[0m, in \u001b[0;36mFast_RMS_Layernorm.forward\u001b[0;34m(ctx, X, W, eps, gemma)\u001b[0m\n\u001b[1;32m    141\u001b[0m r \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mempty(n_rows, dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfloat32, device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    143\u001b[0m fx \u001b[38;5;241m=\u001b[39m _gemma_rms_layernorm_forward \u001b[38;5;28;01mif\u001b[39;00m gemma \u001b[38;5;28;01melse\u001b[39;00m _rms_layernorm_forward\n\u001b[0;32m--> 144\u001b[0m \u001b[43mfx\u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_rows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m    \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m    \u001b[49m\u001b[43mW\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m    \u001b[49m\u001b[43mr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_cols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[43m    \u001b[49m\u001b[43mBLOCK_SIZE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mBLOCK_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_warps\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnum_warps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    153\u001b[0m ctx\u001b[38;5;241m.\u001b[39meps \u001b[38;5;241m=\u001b[39m eps\n\u001b[1;32m    154\u001b[0m ctx\u001b[38;5;241m.\u001b[39mBLOCK_SIZE \u001b[38;5;241m=\u001b[39m BLOCK_SIZE\n",
      "File \u001b[0;32m~/anaconda3/envs/llmrl/lib/python3.10/site-packages/triton/runtime/jit.py:550\u001b[0m, in \u001b[0;36mJITFunction.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m \u001b[38;5;28mbin\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache[device][key]\n\u001b[1;32m    549\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m warmup:\n\u001b[0;32m--> 550\u001b[0m     \u001b[38;5;28;43mbin\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_wrapper\u001b[49m(\n\u001b[1;32m    551\u001b[0m         grid_0,\n\u001b[1;32m    552\u001b[0m         grid_1,\n\u001b[1;32m    553\u001b[0m         grid_2,\n\u001b[1;32m    554\u001b[0m         \u001b[38;5;28mbin\u001b[39m\u001b[38;5;241m.\u001b[39mnum_warps,\n\u001b[1;32m    555\u001b[0m         \u001b[38;5;28mbin\u001b[39m\u001b[38;5;241m.\u001b[39mnum_ctas,\n\u001b[1;32m    556\u001b[0m         \u001b[38;5;28mbin\u001b[39m\u001b[38;5;241m.\u001b[39mclusterDims[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    557\u001b[0m         \u001b[38;5;28mbin\u001b[39m\u001b[38;5;241m.\u001b[39mclusterDims[\u001b[38;5;241m1\u001b[39m],\n\u001b[1;32m    558\u001b[0m         \u001b[38;5;28mbin\u001b[39m\u001b[38;5;241m.\u001b[39mclusterDims[\u001b[38;5;241m2\u001b[39m],\n\u001b[1;32m    559\u001b[0m         \u001b[38;5;28mbin\u001b[39m\u001b[38;5;241m.\u001b[39mshared,\n\u001b[1;32m    560\u001b[0m         stream,\n\u001b[1;32m    561\u001b[0m         \u001b[38;5;28mbin\u001b[39m\u001b[38;5;241m.\u001b[39mcu_function,\n\u001b[1;32m    562\u001b[0m         CompiledKernel\u001b[38;5;241m.\u001b[39mlaunch_enter_hook,\n\u001b[1;32m    563\u001b[0m         CompiledKernel\u001b[38;5;241m.\u001b[39mlaunch_exit_hook,\n\u001b[1;32m    564\u001b[0m         \u001b[38;5;28mbin\u001b[39m,\n\u001b[1;32m    565\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mbin\u001b[39m\u001b[38;5;241m.\u001b[39massemble_tensormap_to_arg(non_constexpr_arg_values),\n\u001b[1;32m    566\u001b[0m     )\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbin\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/llmrl/lib/python3.10/site-packages/triton/compiler/compiler.py:692\u001b[0m, in \u001b[0;36mCompiledKernel.__getattribute__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattribute__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name):\n\u001b[1;32m    691\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mc_wrapper\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 692\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_handles\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    693\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(name)\n",
      "File \u001b[0;32m~/anaconda3/envs/llmrl/lib/python3.10/site-packages/triton/compiler/compiler.py:683\u001b[0m, in \u001b[0;36mCompiledKernel._init_handles\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    680\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshared \u001b[38;5;241m>\u001b[39m max_shared:\n\u001b[1;32m    681\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m OutOfResources(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshared, max_shared, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshared memory\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 683\u001b[0m mod, func, n_regs, n_spills \u001b[38;5;241m=\u001b[39m \u001b[43mfn_load_binary\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mname\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masm\u001b[49m\u001b[43m[\u001b[49m\u001b[43mbin_path\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshared\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    685\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_spills \u001b[38;5;241m=\u001b[39m n_spills\n\u001b[1;32m    686\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_regs \u001b[38;5;241m=\u001b[39m n_regs\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Triton Error [CUDA]: device-side assert triggered"
     ]
    }
   ],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"checkpoints/llama3-8b-critic-lora\") # Local saving\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[128000, 198]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Janet pays $40/hour for 3 hours per week of clarinet lessons and $28/hour for 5 hours a week of piano lessons. How much more does she spend on piano lessons than clarinet lessons in a year? Step 1: Janet spends 3 hours + 5 hours = <<3+5=8>>8 hours per week on music lessons. + Step 2: She spends 40 * 3 = <<40*3=120>>120 on clarinet lessons per week. + Step 3: She spends 28 * 5 = <<28*5=140>>140 on piano lessons per week. + Step 4: Janet spends 120 + 140 = <<120+140=260>>260 on music lessons per week. + Step 5: She spends 260 * 52 = <<260*52=13520>>13520 on music lessons in a year. The answer is: 13520 <->'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label = 'Janet pays $40/hour for 3 hours per week of clarinet lessons and $28/hour for 5 hours a week of piano lessons. How much more does she spend on piano lessons than clarinet lessons in a year? Janet spends 3 hours + 5 hours = <<3+5=8>>8 hours per week on music lessons. + She spends 40 * 3 = <<40*3=120>>120 on clarinet lessons per week. + She spends 28 * 5 = <<28*5=140>>140 on piano lessons per week. + Janet spends 120 + 140 = <<120+140=260>>260 on music lessons per week. + Step 5: She spends 260 * 52 = <<260*52=13520>>13520 on music lessons in a year. The answer is: 13520 -'\n",
    "label = label[:-1] + '<->'\n",
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[128256, 128257, 128258]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 444655/444655 [00:46<00:00, 9555.80 examples/s] \n",
      "Map: 100%|██████████| 444655/444655 [01:55<00:00, 3845.67 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"peiyi9979/Math-Shepherd\", split='train')\n",
    "good_token = '<+>'\n",
    "bad_token = '<->'\n",
    "step_tag = '<ки>'\n",
    "tokenizer.add_special_tokens({'additional_special_tokens': [good_token, bad_token, step_tag]})\n",
    "print(tokenizer.convert_tokens_to_ids([good_token, bad_token, step_tag]))\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    inputs = examples[\"input\"]\n",
    "    # Replace the ки with step_tag for each input example\n",
    "    inputs = [input.replace('ки', step_tag) for input in inputs]\n",
    "    return tokenizer(inputs, padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "def tokenize_labels_function(examples):\n",
    "    labels_list = examples[\"label\"]\n",
    "    tokenized_labels = []\n",
    "    \n",
    "    for labels in labels_list:\n",
    "        # Replace the + and - with good_token and bad_token, while keeping them in the solution\n",
    "        labels = labels.replace('+\\n', good_token + '\\n')\n",
    "        labels = labels.replace('-\\n', bad_token + '\\n')\n",
    "        \n",
    "        # Replace the last token with the appropriate special token\n",
    "        if labels[-1] == '+':\n",
    "            labels = labels[:-1] + good_token\n",
    "        else:\n",
    "            labels = labels[:-1] + bad_token\n",
    "        \n",
    "        tokenized_label = tokenizer(labels, padding=\"max_length\", truncation=True, max_length=512)\n",
    "        tokenized_labels.append(tokenized_label[\"input_ids\"])\n",
    "    \n",
    "    return {\"labels\": tokenized_labels}\n",
    "\n",
    "dataset = dataset.map(tokenize_function, batched=True)\n",
    "dataset = dataset.map(tokenize_labels_function, batched=True)\n",
    "dataset= dataset.remove_columns(['input', 'label', 'task'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128000,\n",
       " 18820,\n",
       " 295,\n",
       " 21935,\n",
       " 400,\n",
       " 1272,\n",
       " 7682,\n",
       " 414,\n",
       " 369,\n",
       " 220,\n",
       " 18,\n",
       " 4207,\n",
       " 824,\n",
       " 2046,\n",
       " 315,\n",
       " 20064,\n",
       " 30976,\n",
       " 18872,\n",
       " 323,\n",
       " 400,\n",
       " 1591,\n",
       " 7682,\n",
       " 414,\n",
       " 369,\n",
       " 220,\n",
       " 20,\n",
       " 4207,\n",
       " 264,\n",
       " 2046,\n",
       " 315,\n",
       " 27374,\n",
       " 18872,\n",
       " 13,\n",
       " 2650,\n",
       " 1790,\n",
       " 810,\n",
       " 1587,\n",
       " 1364,\n",
       " 8493,\n",
       " 389,\n",
       " 27374,\n",
       " 18872,\n",
       " 1109,\n",
       " 20064,\n",
       " 30976,\n",
       " 18872,\n",
       " 304,\n",
       " 264,\n",
       " 1060,\n",
       " 30,\n",
       " 15166,\n",
       " 220,\n",
       " 16,\n",
       " 25,\n",
       " 54765,\n",
       " 38202,\n",
       " 220,\n",
       " 18,\n",
       " 4207,\n",
       " 489,\n",
       " 220,\n",
       " 20,\n",
       " 4207,\n",
       " 284,\n",
       " 1134,\n",
       " 18,\n",
       " 10,\n",
       " 20,\n",
       " 28,\n",
       " 23,\n",
       " 2511,\n",
       " 23,\n",
       " 4207,\n",
       " 824,\n",
       " 2046,\n",
       " 389,\n",
       " 4731,\n",
       " 18872,\n",
       " 13,\n",
       " 220,\n",
       " 128258,\n",
       " 198,\n",
       " 8468,\n",
       " 220,\n",
       " 17,\n",
       " 25,\n",
       " 3005,\n",
       " 38202,\n",
       " 220,\n",
       " 1272,\n",
       " 353,\n",
       " 220,\n",
       " 18,\n",
       " 284,\n",
       " 1134,\n",
       " 1272,\n",
       " 9,\n",
       " 18,\n",
       " 28,\n",
       " 4364,\n",
       " 2511,\n",
       " 4364,\n",
       " 389,\n",
       " 20064,\n",
       " 30976,\n",
       " 18872,\n",
       " 824,\n",
       " 2046,\n",
       " 13,\n",
       " 220,\n",
       " 128258,\n",
       " 198,\n",
       " 8468,\n",
       " 220,\n",
       " 18,\n",
       " 25,\n",
       " 3005,\n",
       " 38202,\n",
       " 220,\n",
       " 1591,\n",
       " 353,\n",
       " 220,\n",
       " 20,\n",
       " 284,\n",
       " 1134,\n",
       " 1591,\n",
       " 9,\n",
       " 20,\n",
       " 28,\n",
       " 6860,\n",
       " 2511,\n",
       " 6860,\n",
       " 389,\n",
       " 27374,\n",
       " 18872,\n",
       " 824,\n",
       " 2046,\n",
       " 13,\n",
       " 220,\n",
       " 128258,\n",
       " 198,\n",
       " 8468,\n",
       " 220,\n",
       " 19,\n",
       " 25,\n",
       " 54765,\n",
       " 38202,\n",
       " 220,\n",
       " 4364,\n",
       " 489,\n",
       " 220,\n",
       " 6860,\n",
       " 284,\n",
       " 1134,\n",
       " 4364,\n",
       " 10,\n",
       " 6860,\n",
       " 28,\n",
       " 11387,\n",
       " 2511,\n",
       " 11387,\n",
       " 389,\n",
       " 4731,\n",
       " 18872,\n",
       " 824,\n",
       " 2046,\n",
       " 13,\n",
       " 220,\n",
       " 128258,\n",
       " 198,\n",
       " 8468,\n",
       " 220,\n",
       " 20,\n",
       " 25,\n",
       " 3005,\n",
       " 38202,\n",
       " 220,\n",
       " 11387,\n",
       " 353,\n",
       " 220,\n",
       " 4103,\n",
       " 284,\n",
       " 1134,\n",
       " 11387,\n",
       " 9,\n",
       " 4103,\n",
       " 28,\n",
       " 8878,\n",
       " 508,\n",
       " 2511,\n",
       " 8878,\n",
       " 508,\n",
       " 389,\n",
       " 4731,\n",
       " 18872,\n",
       " 304,\n",
       " 264,\n",
       " 1060,\n",
       " 13,\n",
       " 578,\n",
       " 4320,\n",
       " 374,\n",
       " 25,\n",
       " 220,\n",
       " 8878,\n",
       " 508,\n",
       " 220,\n",
       " 128258]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128001,\n",
       " 128000,\n",
       " 18820,\n",
       " 295,\n",
       " 21935,\n",
       " 400,\n",
       " 1272,\n",
       " 7682,\n",
       " 414,\n",
       " 369,\n",
       " 220,\n",
       " 18,\n",
       " 4207,\n",
       " 824,\n",
       " 2046,\n",
       " 315,\n",
       " 20064,\n",
       " 30976,\n",
       " 18872,\n",
       " 323,\n",
       " 400,\n",
       " 1591,\n",
       " 7682,\n",
       " 414,\n",
       " 369,\n",
       " 220,\n",
       " 20,\n",
       " 4207,\n",
       " 264,\n",
       " 2046,\n",
       " 315,\n",
       " 27374,\n",
       " 18872,\n",
       " 13,\n",
       " 2650,\n",
       " 1790,\n",
       " 810,\n",
       " 1587,\n",
       " 1364,\n",
       " 8493,\n",
       " 389,\n",
       " 27374,\n",
       " 18872,\n",
       " 1109,\n",
       " 20064,\n",
       " 30976,\n",
       " 18872,\n",
       " 304,\n",
       " 264,\n",
       " 1060,\n",
       " 30,\n",
       " 15166,\n",
       " 220,\n",
       " 16,\n",
       " 25,\n",
       " 54765,\n",
       " 38202,\n",
       " 220,\n",
       " 18,\n",
       " 4207,\n",
       " 489,\n",
       " 220,\n",
       " 20,\n",
       " 4207,\n",
       " 284,\n",
       " 1134,\n",
       " 18,\n",
       " 10,\n",
       " 20,\n",
       " 28,\n",
       " 23,\n",
       " 2511,\n",
       " 23,\n",
       " 4207,\n",
       " 824,\n",
       " 2046,\n",
       " 389,\n",
       " 4731,\n",
       " 18872,\n",
       " 13,\n",
       " 220,\n",
       " 128256,\n",
       " 198,\n",
       " 8468,\n",
       " 220,\n",
       " 17,\n",
       " 25,\n",
       " 3005,\n",
       " 38202,\n",
       " 220,\n",
       " 1272,\n",
       " 353,\n",
       " 220,\n",
       " 18,\n",
       " 284,\n",
       " 1134,\n",
       " 1272,\n",
       " 9,\n",
       " 18,\n",
       " 28,\n",
       " 4364,\n",
       " 2511,\n",
       " 4364,\n",
       " 389,\n",
       " 20064,\n",
       " 30976,\n",
       " 18872,\n",
       " 824,\n",
       " 2046,\n",
       " 13,\n",
       " 220,\n",
       " 128256,\n",
       " 198,\n",
       " 8468,\n",
       " 220,\n",
       " 18,\n",
       " 25,\n",
       " 3005,\n",
       " 38202,\n",
       " 220,\n",
       " 1591,\n",
       " 353,\n",
       " 220,\n",
       " 20,\n",
       " 284,\n",
       " 1134,\n",
       " 1591,\n",
       " 9,\n",
       " 20,\n",
       " 28,\n",
       " 6860,\n",
       " 2511,\n",
       " 6860,\n",
       " 389,\n",
       " 27374,\n",
       " 18872,\n",
       " 824,\n",
       " 2046,\n",
       " 13,\n",
       " 220,\n",
       " 128256,\n",
       " 198,\n",
       " 8468,\n",
       " 220,\n",
       " 19,\n",
       " 25,\n",
       " 54765,\n",
       " 38202,\n",
       " 220,\n",
       " 4364,\n",
       " 489,\n",
       " 220,\n",
       " 6860,\n",
       " 284,\n",
       " 1134,\n",
       " 4364,\n",
       " 10,\n",
       " 6860,\n",
       " 28,\n",
       " 11387,\n",
       " 2511,\n",
       " 11387,\n",
       " 389,\n",
       " 4731,\n",
       " 18872,\n",
       " 824,\n",
       " 2046,\n",
       " 13,\n",
       " 220,\n",
       " 128256,\n",
       " 198,\n",
       " 8468,\n",
       " 220,\n",
       " 20,\n",
       " 25,\n",
       " 3005,\n",
       " 38202,\n",
       " 220,\n",
       " 11387,\n",
       " 353,\n",
       " 220,\n",
       " 4103,\n",
       " 284,\n",
       " 1134,\n",
       " 11387,\n",
       " 9,\n",
       " 4103,\n",
       " 28,\n",
       " 8878,\n",
       " 508,\n",
       " 2511,\n",
       " 8878,\n",
       " 508,\n",
       " 389,\n",
       " 4731,\n",
       " 18872,\n",
       " 304,\n",
       " 264,\n",
       " 1060,\n",
       " 13,\n",
       " 578,\n",
       " 4320,\n",
       " 374,\n",
       " 25,\n",
       " 220,\n",
       " 8878,\n",
       " 508,\n",
       " 220,\n",
       " 128257]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]['labels']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/llmrl/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth: Fast Llama patching release 2024.4\n",
      "   \\\\   /|    GPU: NVIDIA A100-SXM4-40GB. Max memory: 39.394 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.2.2. CUDA = 8.0. CUDA Toolkit = 11.8.\n",
      "\\        /    Bfloat16 = TRUE. Xformers = 0.0.25.post1. FA = True.\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.37s/it]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Unsloth 2024.4 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "from transformers import LlamaForSequenceClassification, AutoTokenizer, LlamaForCausalLM\n",
    "import torch\n",
    "import wandb\n",
    "import os\n",
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = False # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"/home/jianingqi/LLMRL/checkpoints/llama3-8b-critic-lora-4-29\", # \"unsloth/tinyllama\" for 16bit loading\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "tokenizer.padding_side = \"left\" # Padding side for faster inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LlamaForSequenceClassification, AutoTokenizer, LlamaForCausalLM\n",
    "import torch\n",
    "import wandb\n",
    "import os\n",
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = False # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/tinyllama\", # \"unsloth/tinyllama\" for 16bit loading\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "tokenizer.padding_side = \"left\" # Padding side for faster inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[489, 482]\n",
      "116624\n"
     ]
    }
   ],
   "source": [
    "\n",
    "good_token = ' +'\n",
    "bad_token = '-'\n",
    "step_tag = ' ки'\n",
    "\n",
    "candidate_tokens = tokenizer.encode(f\"{good_token} {bad_token}\")[1:] # [489, 482]\n",
    "step_tag_id = tokenizer.encode(f\"{step_tag}\")[-1] # 12902\n",
    "print(candidate_tokens)\n",
    "print(step_tag_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenizer.add_special_tokens({'additional_special_tokens': [good_token, bad_token, step_tag]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# candidate_tokens = tokenizer.encode(f\"{good_token} {bad_token}\")[1:] # [648, 387]\n",
    "# step_tag_id = tokenizer.encode(f\"{step_tag}\")[-1] # 12902"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10, 482]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidate_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17165"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step_tag_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' +\\n'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(3694)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3694, 482]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidate_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(8.7917e-07, device='cuda:0', dtype=torch.bfloat16)\n",
      "tensor([0.0503, 0.1406, 0.1011, 0.0103, 0.1191], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n",
      "tensor(3.8743e-06, device='cuda:0', dtype=torch.bfloat16)\n",
      "tensor([0.0535, 0.1641, 0.0052, 0.0850], device='cuda:0', dtype=torch.bfloat16)\n"
     ]
    }
   ],
   "source": [
    "question = \"\"\"Josh decides to try flipping a house.  He buys a house for $80,000 and then puts in $50,000 in repairs.  This increased the value of the house by 150%.  How much profit did he make \\n\"\"\"\n",
    "output1 = \"\"\"The cost of the house and repairs came out to 80,000+50,000=$<<80000+50000=130000>>130,000 ки \\nHe increased the value of the house by 80,000*1.5=<<80000*1.5=120000>>120,000 ки \\nSo the new value of the house is 120,000+80,000=$<<120000+80000=200000>>200,000 ки \\nSo he made a profit of 200,000-130,000=$<<200000-130000=70000>>70,000 ки \\n#### 70000 ки\"\"\" # 18 is right\n",
    "\n",
    "output2 = \"\"\"The house was worth 80,000 + 50,000 = $<<80000+50000=130000>>130,000 after the repairs ки \\nThe house is now worth 130,000 x 150% = $<<130000*150*.01=195000>>195,000 ки \\nHe made a profit of 195,000 - 130,000 = $<<195000-130000=65000>>65,000 ки \\n#### 65 ки\"\"\" #wrong\n",
    "\n",
    "for output in [output1, output2]:\n",
    "    input_for_prm = f\"{question} {output}\"\n",
    "    input_id = torch.tensor([tokenizer.encode(input_for_prm)]).to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_id).logits[:,:,candidate_tokens]\n",
    "        scores = logits.softmax(dim=-1)[:,:,0] \n",
    "        step_scores = scores[input_id == step_tag_id]\n",
    "        score_product = step_scores.prod()\n",
    "        print(score_product)\n",
    "        print(step_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[128000,  18820,    295,    753,  78878,  11203,    220,    845,  19335,\n",
       "            824,   1938,     13,   3005,  50777,   2380,    369,  17954,   1475,\n",
       "           6693,    323,    293,   2094,  55404,   1354,    369,   1077,   4885,\n",
       "           1475,   1938,    449,   3116,     13,   3005,  31878,    279,  27410,\n",
       "            520,    279,  20957,      6,   3157,   7446,    369,    400,     17,\n",
       "            824,   7878,  37085,  19151,     13,   2650,   1790,    304,  11441,\n",
       "           1587,   1364,   1304,   1475,   1938,    520,    279,  20957,      6,\n",
       "           3157,     30,  15166,    220,     16,     25,  54765,    596,  78878,\n",
       "          11203,    220,    845,  19335,    824,   1938,     13, 116624,    198,\n",
       "           8468,    220,     17,     25,   3005,  50777,   2380,    369,  17954,\n",
       "           1475,   6693,     11,    779,   1364,    706,    220,    845,    482,\n",
       "            220,     18,    284,    220,   1032,  19335,   2163,     13, 116624,\n",
       "            198,   8468,    220,     18,     25,   3005,    293,   2094,  55404,\n",
       "           1354,    369,   1077,   4885,   1475,   1938,    449,   3116,  19335,\n",
       "             11,    779,   1364,    706,    220,   1032,    482,    220,     19,\n",
       "            284,    220,     24,  19335,   2163,     13, 116624,    198,   8468,\n",
       "            220,     19,     25,   3005,  31878,    279,  27410,    520,    279,\n",
       "          20957,      6,   3157,   7446,    369,    400,     17,    824,   7878,\n",
       "          37085,  19151,     11,    779,   1364,   3727,    220,     24,    353,\n",
       "            400,     17,    284,    400,   1114,   1475,   1938,    520,    279,\n",
       "          20957,      6,   3157,     13,    578,   4320,    374,     25,    220,\n",
       "           1114, 116624]], device='cuda:0')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0728, device='cuda:0', dtype=torch.bfloat16)\n",
      "tensor([0.2021, 0.8281, 0.5469, 0.7969], device='cuda:0', dtype=torch.bfloat16)\n",
      "tensor(0.0457, device='cuda:0', dtype=torch.bfloat16)\n",
      "tensor([0.2021, 0.8281, 0.5469, 0.5000], device='cuda:0', dtype=torch.bfloat16)\n"
     ]
    }
   ],
   "source": [
    "question = \"\"\"Janet\\u2019s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?\"\"\"\n",
    "output1 = \"\"\"Janet's ducks lay 16 eggs per day. ки \\nShe eats three for breakfast every morning, so she has 16 - 3 = 13 eggs left. ки \\nShe bakes muffins for her friends every day with four eggs, so she has 13 - 4 = 9 eggs left. ки \\nShe sells the remainder at the farmers' market daily for $2 per fresh duck egg, so she makes 9 * $2 = $18 every day at the farmers' market. The answer is: 18 ки\"\"\" # 18 is right\n",
    "output2 = \"\"\"Janet's ducks lay 16 eggs per day. ки \\nShe eats three for breakfast every morning, so she has 16 - 3 = 13 eggs left. ки \\nShe bakes muffins for her friends every day with four eggs, so she has 13 - 4 = 9 eggs left. ки \\nShe sells the remainder at the farmers' market daily for $2 per fresh duck egg, so she makes 9 * $2 = $17 every day at the farmers' market. The answer is: 19 ки\"\"\" # 17 is wrong\n",
    "\n",
    "\n",
    "for output in [output1, output2]:\n",
    "    input_for_prm = f\"{question} {output}\"\n",
    "    input_id = torch.tensor([tokenizer.encode(input_for_prm)]).to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_id).logits[:,:,candidate_tokens]\n",
    "        scores = logits.softmax(dim=-1)[:,:,0] \n",
    "        step_scores = scores[input_id == step_tag_id]\n",
    "        score_product = step_scores.prod()\n",
    "        print(score_product)\n",
    "        print(step_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"\"\"Janet\\u2019s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market? \\n\"\"\"\n",
    "output1 = \"\"\"Janet's ducks lay 16 eggs per day. ки \\nShe eats three for breakfast every morning, so she has 16 - 3 = 13 eggs left. ки \\nShe bakes muffins for her friends every day with four eggs, so she has 13 - 4 = 9 eggs left. ки \\nShe sells the remainder at the farmers' market daily for $2 per fresh duck egg, so she makes 9 * $2 = $18 every day at the farmers' market. The answer is: 18 ки\"\"\" # 18 is right\n",
    "output2 = \"\"\"Janet's ducks lay 16 eggs per day. ки \\nShe eats three for breakfast every morning, so she has 16 - 3 = 13 eggs left. ки \\nShe bakes muffins for her friends every day with four eggs, so she has 13 - 4 = 9 eggs left. ки \\nShe sells the remainder at the farmers' market daily for $2 per fresh duck egg, so she makes 9 * $2 = $17 every day at the farmers' market. The answer is: 17 ки\"\"\" # 17 is wrong\n",
    "input_for_prm = []\n",
    "for output in [output1, output2]:\n",
    "    input_for_prm.append(f\"{question} {output}\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    inputs = critic_tokenizer(input_for_prm, padding=\"max_length\", truncation=True, max_length=256, return_tensors=\"pt\").to(\"cuda\")\n",
    "    outputs = critic(**inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 256, 128256])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.logits[:,:,candidate_tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation on generated dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Mistral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "sys.path.append('Data/prm800k/prm800k')\n",
    "from grading import grader\n",
    "\n",
    "def load_json_answers(directory, num_files=None):\n",
    "    json_files = [file for file in os.listdir(directory) if file.endswith(\".json\")]\n",
    "    \n",
    "    if num_files is not None:\n",
    "        json_files = json_files[:num_files]\n",
    "    \n",
    "    all_answers = []\n",
    "    for file in json_files:\n",
    "        file_path = os.path.join(directory, file)\n",
    "        with open(file_path, \"r\") as f:\n",
    "            answers = json.load(f)\n",
    "            all_answers.append(answers)\n",
    "    \n",
    "    return all_answers\n",
    "\n",
    "def extract_answers(all_answers):\n",
    "    extracted_answers = [[] for _ in range(len(all_answers[0]))]\n",
    "    for answers in all_answers:\n",
    "        for i, answer in enumerate(answers):\n",
    "            match = re.search(r\"####\\s*(.*)\", answer)\n",
    "            if match:\n",
    "                extracted_answers[i].append(match.group(1).strip())\n",
    "            else:\n",
    "                extracted_answers[i].append(\"\")\n",
    "    return extracted_answers\n",
    "\n",
    "def compute_probabilities(all_answers, critic_tokenizer, critic, batch_size=32, is_llama = True):\n",
    "    answers_prob = [[] for _ in range(len(all_answers[0]))]\n",
    "    \n",
    "    good_token = '+'\n",
    "    bad_token = '-'\n",
    "    step_tag = ' ки'\n",
    "\n",
    "    candidate_tokens = critic_tokenizer.encode(f\"{good_token} {bad_token}\")[1:] # [648, 387]\n",
    "    step_tag_id = critic_tokenizer.encode(f\"{step_tag}\")[-1] # 12902\n",
    "    print(candidate_tokens)\n",
    "    print(step_tag_id)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for answers in tqdm(all_answers, desc=\"Processing answers\"):\n",
    "            results = []\n",
    "            if is_llama:\n",
    "                for answer in answers:\n",
    "                    result = answer.split('assistant\\n\\n')[0].split('You are a helpful assistant to solve math problems step by step user\\n\\n')[1] + '\\n'\n",
    "                    responses = answer.split('assistant\\n\\n')[1].split('\\n')\n",
    "                    for response in responses:\n",
    "                        result += response + \" ки \\n\"\n",
    "                    results.append(result)\n",
    "            else:\n",
    "                for answer in answers:\n",
    "                    result = answer.split('### Response:')[0].split('\\n### Input:\\n')[1]\n",
    "                    responses = answer.split('### Response:\\n')[1].split('\\n')\n",
    "                    for response in responses:\n",
    "                        result += response + \" ки \\n\"\n",
    "                    results.append(result)\n",
    "                                \n",
    "            correct_probabilities = []\n",
    "            for i in range(0, len(results), batch_size):\n",
    "                batch_results = results[i:i+batch_size]\n",
    "                \n",
    "                inputs = critic_tokenizer(batch_results, padding=\"max_length\", truncation=True, max_length=256, return_tensors=\"pt\").to(\"cuda\")\n",
    "                logits = critic(**inputs).logits[:,:,candidate_tokens]\n",
    "                scores = logits.softmax(dim=-1)[:,:,0] \n",
    "                step_scores = scores[inputs['input_ids'] == step_tag_id]\n",
    "                correct_probabilities.extend(step_scores.tolist())\n",
    "                # score_product = step_scores.prod()\n",
    "                \n",
    "                \n",
    "                # probabilities = F.softmax(outputs.logits, dim=1)\n",
    "                # correct_probability = probabilities[:, 1:]\n",
    "                # correct_probability = torch.sum(correct_probability, dim=1)\n",
    "                # correct_probabilities.extend(correct_probability.tolist())\n",
    "            \n",
    "            response_counts = []\n",
    "            for answer in answers:\n",
    "                if is_llama:\n",
    "                    num_responses = len(answer.split('assistant\\n\\n')[1].split('\\n'))\n",
    "                else:\n",
    "                    num_responses = len(answer.split('### Response:\\n')[1].split('\\n'))\n",
    "                response_counts.append(num_responses)\n",
    "            probability_index = 0\n",
    "            for i, count in enumerate(response_counts):\n",
    "                answer_probs = correct_probabilities[probability_index:probability_index+count]\n",
    "                answer_prob = min(answer_probs)\n",
    "                answers_prob[i].append(answer_prob)\n",
    "                probability_index += count\n",
    "                \n",
    "    \n",
    "    return answers_prob\n",
    "\n",
    "def select_highest_probability_answers(extracted_answers, answers_prob):\n",
    "    highest_probability_answers = []\n",
    "    for i, question_answers in enumerate(extracted_answers):\n",
    "        question_probs = answers_prob[i]\n",
    "        if question_probs:\n",
    "            max_prob_index = question_probs.index(max(question_probs))\n",
    "            highest_probability_answer = question_answers[max_prob_index]\n",
    "        else:\n",
    "            highest_probability_answer = \"\"\n",
    "        highest_probability_answers.append(highest_probability_answer)\n",
    "    return highest_probability_answers\n",
    "\n",
    "def compare_with_ground_truth(majority_answers, ground_truth_answers):\n",
    "    correct_count = 0\n",
    "    for majority_answer, ground_truth_answer in zip(majority_answers, ground_truth_answers):\n",
    "        if grader.grade_answer(majority_answer, ground_truth_answer):\n",
    "            correct_count += 1\n",
    "    accuracy = correct_count / len(ground_truth_answers)\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10\n",
      "10\n",
      "15\n",
      "15\n",
      "19\n"
     ]
    }
   ],
   "source": [
    "answers = [\"\\n### Input:\\nJanet\\u2019s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?\\n\\n### Response:\\n day 1: 16 eggs * 3 = <<16*3=48>>48 eggs\\nday 2: 48 eggs + 4 eggs = <<48+4=52>>52 eggs\\nday 3: 52 eggs + 4 eggs = <<52+4=56>>56 eggs\\nday 4: 56 eggs + 4 eggs = <<56+4=60>>60 eggs\\nday 5: 60 eggs + 4 eggs = <<60+4=64>>64 eggs\\nday 6: 64 eggs + 4 eggs = <<64+4=68>>68 eggs\\nday 7: 68 eggs + 4 eggs = <<68+4=72>>72 eggs\\nday 8: 72 eggs + 4 eggs = <<72+4=76>>76 eggs\\nday 9: 76 eggs + 4 eggs = <<76+4=80>>80 eggs\\nday 10: 80 eggs + 4 eggs = <<80+4=84>>\", \"\\n### Input:\\nA robe takes 2 bolts of blue fiber and half that much white fiber.  How many bolts in total does it take?\\n\\n### Response:\\n robe takes 2*2=<<2*2=4>>4 bolt of blue fiber\\nSo it needs 4/2=<<4/2=2>>2 bolt of white fiber\\nIt takes 2*2=<<2*2=4>>4 bolt of white fiber\\nSo in total it takes 4+2=<<4+2=6>>6 bolt of color\\n#### 6\", \"\\n### Input:\\nJosh decides to try flipping a house.  He buys a house for $80,000 and then puts in $50,000 in repairs.  This increased the value of the house by 150%.  How much profit did he make?\\n\\n### Response:\\n house value: 80000*1.5=$<<80000*1.5=120000>>120000\\nvalue after repairs: 120000+50000=$<<120000+50000=170000>>170000\\nSo he made $170000-$120000=$<<170000-120000=50000>>50000 profit\\n#### 50000\"]\n",
    "response_counts = []\n",
    "for answer in answers:\n",
    "    num_responses = len(answer.split('### Response:\\n')[1].split('\\n'))\n",
    "    response_counts.append(num_responses)\n",
    "\n",
    "probability_index = 0\n",
    "for i, count in enumerate(response_counts):\n",
    "    print(probability_index)\n",
    "    print(probability_index+count)\n",
    "    # answer_probs = correct_probabilities[probability_index:probability_index+count]\n",
    "    # answer_prob = min(answer_probs)\n",
    "    # answers_prob[i].append(answer_prob)\n",
    "    probability_index += count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_probabilities(all_answers, critic_tokenizer, critic, answer_num = 1, batch_size=32, is_llama = True):\n",
    "    answers_prob = [[] for _ in range(len(all_answers[0]))]\n",
    "    \n",
    "    good_token = '+'\n",
    "    bad_token = '-'\n",
    "    step_tag = ' ки'\n",
    "\n",
    "    candidate_tokens = critic_tokenizer.encode(f\"{good_token} {bad_token}\")[1:] # [648, 387]\n",
    "    step_tag_id = critic_tokenizer.encode(f\"{step_tag}\")[-1] # 12902\n",
    "    print(candidate_tokens)\n",
    "    print(step_tag_id)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for answers in tqdm(all_answers, desc=\"Processing answers\"):\n",
    "            results = []\n",
    "            if is_llama:\n",
    "                answer= answers[answer_num]\n",
    "                result = answer.split('assistant\\n\\n')[0].split('You are a helpful assistant to solve math problems step by step user\\n\\n')[1] + '\\n'\n",
    "                responses = answer.split('assistant\\n\\n')[1].split('\\n')\n",
    "                for response in responses:\n",
    "                    result += response + \" ки \\n\"\n",
    "                results.append(result)\n",
    "            else:\n",
    "                answer= answers[answer_num]\n",
    "                result = answer.split('### Response:')[0].split('\\n### Input:\\n')[1]\n",
    "                responses = answer.split('### Response:\\n')[1].split('\\n')\n",
    "                for response in responses:\n",
    "                    result += response + \" ки \\n\"\n",
    "                results.append(result)\n",
    "                                \n",
    "            correct_probabilities = []\n",
    "            for i in range(0, len(results), batch_size):\n",
    "                batch_results = results[i:i+batch_size]\n",
    "                \n",
    "                inputs = critic_tokenizer(batch_results, padding=\"max_length\", truncation=True, max_length=256, return_tensors=\"pt\").to(\"cuda\")\n",
    "                logits = critic(**inputs).logits[:,:,candidate_tokens]\n",
    "                scores = logits.softmax(dim=-1)[:,:,0] \n",
    "                step_scores = scores[inputs['input_ids'] == step_tag_id]\n",
    "                correct_probabilities.extend(step_scores.tolist())\n",
    "                # score_product = step_scores.prod()\n",
    "                \n",
    "                \n",
    "                # probabilities = F.softmax(outputs.logits, dim=1)\n",
    "                # correct_probability = probabilities[:, 1:]\n",
    "                # correct_probability = torch.sum(correct_probability, dim=1)\n",
    "                # correct_probabilities.extend(correct_probability.tolist())\n",
    "            \n",
    "            answer= answers[answer_num]\n",
    "            if is_llama:\n",
    "                num_responses = len(answer.split('assistant\\n\\n')[1].split('\\n'))\n",
    "            else:\n",
    "                num_responses = len(answer.split('### Response:\\n')[1].split('\\n'))\n",
    "            # answer_prob = torch.tensor(correct_probabilities[i:i+num_responses]).prod().item()\n",
    "            answer_prob = torch.tensor(correct_probabilities).min().item()\n",
    "            answers_prob[i].append(answer_prob)\n",
    "    \n",
    "    return answers_prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9930, 0.9853, 0.9504])\n",
      "tensor([0.6967, 0.8318, 0.4640, 0.7615, 0.6703])\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "good_token = '+'\n",
    "bad_token = '-'\n",
    "step_tag = 'ки'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('peiyi9979/math-shepherd-mistral-7b-prm')\n",
    "candidate_tokens = tokenizer.encode(f\"{good_token} {bad_token}\")[1:] # [648, 387]\n",
    "step_tag_id = tokenizer.encode(f\"{step_tag}\")[-1] # 12902\n",
    "model = AutoModelForCausalLM.from_pretrained('peiyi9979/math-shepherd-mistral-7b-prm').eval()\n",
    "\n",
    "question = \"\"\"A robe takes 2 bolts of blue fiber and half that much white fiber.  How many bolts in total does it take?\"\"\"\n",
    "output1 = \"\"\"It takes 2 bolts of blue fiber and half that much white fiber. 2 * 1/2 = <<2*1/2=1>>1 bolt of white fiber ки \n",
    "So it takes 1 + 2 = <<1+2=3>>3 bolts of fiber ки \n",
    "#### 3 ки \n",
    "\"\"\" # 18 is right\n",
    "output2 = \"\"\"robe takes 2*2=<<2*2=4>>4 bolt of blue fiber ки \\nSo it needs 4/2=<<4/2=2>>2 bolt of white fiber ки \\nIt takes 2*2=<<2*2=4>>4 bolt of white fiber ки \\nSo in total it takes 4+2=<<4+2=6>>6 bolt of color ки \\n#### 6 ки\"\"\" # 17 is wrong\n",
    "\n",
    "\n",
    "for output in [output1, output2]:\n",
    "    input_for_prm = f\"{question} {output}\"\n",
    "    input_id = torch.tensor([tokenizer.encode(input_for_prm)])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_id).logits[:,:,candidate_tokens]\n",
    "        scores = logits.softmax(dim=-1)[:,:,0] \n",
    "        step_scores = scores[input_id == step_tag_id]\n",
    "        print(step_scores)\n",
    "        \n",
    "# tensor([0.9955, 0.9958, 0.9983, 0.9957])\n",
    "# tensor([0.9955, 0.9958, 0.9983, 0.0240])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9298)\n",
      "tensor(0.1372)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for output in [output1, output2]:\n",
    "    input_for_prm = f\"{question} {output}\"\n",
    "    input_id = torch.tensor([tokenizer.encode(input_for_prm)])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_id).logits[:,:,candidate_tokens]\n",
    "        scores = logits.softmax(dim=-1)[:,:,0] \n",
    "        step_scores = scores[input_id == step_tag_id]\n",
    "        print(step_scores.prod())\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing answers:   0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing answers: 100%|██████████| 5/5 [21:57<00:00, 263.58s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import LlamaForSequenceClassification, AutoTokenizer, LlamaForCausalLM\n",
    "import torch\n",
    "\n",
    "model.to(\"cuda\")\n",
    "\n",
    "# Usage example\n",
    "json_directory = \"generated_answers\"\n",
    "\n",
    "\n",
    "all_answers = load_json_answers(json_directory, num_files= 5)\n",
    "extracted_answers = extract_answers(all_answers)\n",
    "answers_prob = compute_probabilities(all_answers, tokenizer, model)\n",
    "highest_probability_answers = select_highest_probability_answers(extracted_answers, answers_prob)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy: 0.06\n"
     ]
    }
   ],
   "source": [
    "#change to the other probability\n",
    "#tiny majority vote 0.08 for 5 files\n",
    "accuracy = compare_with_ground_truth(highest_probability_answers, dataset['final_answer'])\n",
    "print(f\"\\nAccuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy: 0.65\n"
     ]
    }
   ],
   "source": [
    "#change to the other probability\n",
    "#llama3\n",
    "accuracy = compare_with_ground_truth(highest_probability_answers, dataset['final_answer'])\n",
    "print(f\"\\nAccuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## llama3 finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "sys.path.append('Data/prm800k/prm800k')\n",
    "from grading import grader\n",
    "\n",
    "\n",
    "def load_json_answers(directory, num_files=None):\n",
    "    json_files = [file for file in os.listdir(directory) if file.endswith(\".json\")]\n",
    "    \n",
    "    if num_files is not None:\n",
    "        json_files = json_files[:num_files]\n",
    "    \n",
    "    all_answers = []\n",
    "    for file in json_files:\n",
    "        file_path = os.path.join(directory, file)\n",
    "        with open(file_path, \"r\") as f:\n",
    "            answers = json.load(f)\n",
    "            all_answers.append(answers)\n",
    "    \n",
    "    return all_answers\n",
    "\n",
    "\n",
    "def extract_answers(all_answers):\n",
    "    extracted_answers = [[] for _ in range(len(all_answers[0]))]\n",
    "    for answers in all_answers:\n",
    "        for i, answer in enumerate(answers):\n",
    "            match = re.search(r\"####\\s*(.*)\", answer)\n",
    "            if match:\n",
    "                extracted_answers[i].append(match.group(1).strip())\n",
    "            else:\n",
    "                extracted_answers[i].append(\"\")\n",
    "    return extracted_answers\n",
    "\n",
    "def compute_probabilities(all_answers, critic_tokenizer, critic, batch_size=32, is_llama = True):\n",
    "    answers_prob = [[] for _ in range(len(all_answers[0]))]\n",
    "    \n",
    "    good_token = ' +'\n",
    "    bad_token = '-'\n",
    "    step_tag = ' ки'\n",
    "\n",
    "    candidate_tokens = critic_tokenizer.encode(f\"{good_token} {bad_token}\")[1:] # [648, 387]\n",
    "    step_tag_id = critic_tokenizer.encode(f\"{step_tag}\")[-1] # 12902\n",
    "    print(candidate_tokens)\n",
    "    print(step_tag_id)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for answers in tqdm(all_answers, desc=\"Processing answers\"):\n",
    "            results = []\n",
    "            if is_llama:\n",
    "                for answer in answers:\n",
    "                    result = answer.split('assistant\\n\\n')[0].split('You are a helpful assistant to solve math problems step by step user\\n\\n')[1] + '\\n'\n",
    "                    responses = answer.split('assistant\\n\\n')[1].split('\\n')\n",
    "                    for response in responses:\n",
    "                        result += response + \" ки \\n\"\n",
    "                    results.append(result)\n",
    "            else:\n",
    "                for answer in answers:\n",
    "                    result = answer.split('### Response:')[0].split('\\n### Input:\\n')[1]\n",
    "                    responses = answer.split('### Response:\\n')[1].split('\\n')\n",
    "                    for response in responses:\n",
    "                        result += response + \" ки \\n\"\n",
    "                    results.append(result)\n",
    "                                \n",
    "            correct_probabilities = []\n",
    "            for i in tqdm(range(0, len(results), batch_size), desc=\"Computing probabilities\", leave=False):\n",
    "                batch_results = results[i:i+batch_size]\n",
    "                \n",
    "                inputs = critic_tokenizer(batch_results, padding=\"max_length\", truncation=True, max_length=512, return_tensors=\"pt\").to(\"cuda\")\n",
    "                logits = critic(**inputs).logits[:,:,candidate_tokens]\n",
    "                scores = logits.softmax(dim=-1)[:,:,0] \n",
    "                step_scores = scores[inputs['input_ids'] == step_tag_id]\n",
    "                correct_probabilities.extend(step_scores.tolist())\n",
    "                # score_product = step_scores.prod()\n",
    "                \n",
    "                \n",
    "                # probabilities = F.softmax(outputs.logits, dim=1)\n",
    "                # correct_probability = probabilities[:, 1:]\n",
    "                # correct_probability = torch.sum(correct_probability, dim=1)\n",
    "                # correct_probabilities.extend(correct_probability.tolist())\n",
    "            \n",
    "            response_counts = []\n",
    "            for answer in answers:\n",
    "                if is_llama:\n",
    "                    num_responses = len(answer.split('assistant\\n\\n')[1].split('\\n'))\n",
    "                else:\n",
    "                    num_responses = len(answer.split('### Response:\\n')[1].split('\\n'))\n",
    "                response_counts.append(num_responses)\n",
    "            \n",
    "            probability_index = 0\n",
    "            for i, count in enumerate(response_counts):\n",
    "                answer_probs = correct_probabilities[probability_index:probability_index+count]\n",
    "                if answer_probs:\n",
    "                    answer_prob = min(answer_probs)\n",
    "                    answers_prob[i].append(answer_prob)\n",
    "                else:\n",
    "                    print('len of prob')\n",
    "                    print(len(correct_probabilities))\n",
    "                    print('len of responses')\n",
    "                    print(sum(response_counts))\n",
    "                    print('There is a length mismatch')\n",
    "                    print('-----', i)\n",
    "                    print(answers[i])\n",
    "                    answers_prob[i].append(0.0)\n",
    "                probability_index += count\n",
    "    \n",
    "    return answers_prob\n",
    "\n",
    "def select_highest_probability_answers(extracted_answers, answers_prob):\n",
    "    highest_probability_answers = []\n",
    "    for i, question_answers in enumerate(extracted_answers):\n",
    "        question_probs = answers_prob[i]\n",
    "        if question_probs:\n",
    "            max_prob_index = question_probs.index(max(question_probs))\n",
    "            highest_probability_answer = question_answers[max_prob_index]\n",
    "        else:\n",
    "            highest_probability_answer = \"\"\n",
    "        highest_probability_answers.append(highest_probability_answer)\n",
    "    return highest_probability_answers\n",
    "\n",
    "def compare_with_ground_truth(majority_answers, ground_truth_answers):\n",
    "    correct_count = 0\n",
    "    for majority_answer, ground_truth_answer in zip(majority_answers, ground_truth_answers):\n",
    "        if grader.grade_answer(majority_answer, ground_truth_answer):\n",
    "            correct_count += 1\n",
    "    accuracy = correct_count / len(ground_truth_answers)\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[489, 482]\n",
      "116624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing answers: 100%|██████████| 100/100 [1:40:17<00:00, 60.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy: 0.14\n"
     ]
    }
   ],
   "source": [
    "from transformers import LlamaForSequenceClassification, AutoTokenizer, LlamaForCausalLM\n",
    "import torch\n",
    "\n",
    "# Usage example\n",
    "json_directory = \"generated_answers_\"\n",
    "\n",
    "\n",
    "all_answers = load_json_answers(json_directory, num_files = 100)\n",
    "extracted_answers = extract_answers(all_answers)\n",
    "answers_prob = compute_probabilities(all_answers, critic_tokenizer, critic, is_llama = False)\n",
    "highest_probability_answers = select_highest_probability_answers(extracted_answers, answers_prob)\n",
    "#change to the other probability\n",
    "accuracy = compare_with_ground_truth(highest_probability_answers, dataset['final_answer'])\n",
    "print(f\"\\nAccuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy: 0.68\n"
     ]
    }
   ],
   "source": [
    "accuracy = compare_with_ground_truth(highest_probability_answers, dataset['final_answer'])\n",
    "print(f\"\\nAccuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth: Fast Llama patching release 2024.4\n",
      "   \\\\   /|    GPU: NVIDIA A100-SXM4-40GB. Max memory: 39.394 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.2.2. CUDA = 8.0. CUDA Toolkit = 11.8.\n",
      "\\        /    Bfloat16 = TRUE. Xformers = 0.0.25.post1. FA = True.\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:56<00:00, 14.10s/it]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Unsloth 2024.4 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "from transformers import LlamaForSequenceClassification, AutoTokenizer, LlamaForCausalLM\n",
    "import torch\n",
    "import wandb\n",
    "import os\n",
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = False # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "\n",
    "critic, critic_tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"/home/jianingqi/LLMRL/checkpoints/llama3-8b-critic-lora-4-29\", # \"unsloth/tinyllama\" for 16bit loading\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")\n",
    "FastLanguageModel.for_inference(critic) # Enable native 2x faster inference\n",
    "critic_tokenizer.padding_side = \"left\" # Padding side for faster inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4531, 0.2949, 0.2227, 0.1191, 0.4531, 0.2949, 0.2227, 0.1069],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "good_token = ' +'\n",
    "bad_token = '-'\n",
    "step_tag = ' ки'\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained('peiyi9979/math-shepherd-mistral-7b-prm')\n",
    "candidate_tokens = critic_tokenizer.encode(f\"{good_token} {bad_token}\")[1:] # [648, 387]\n",
    "step_tag_id = critic_tokenizer.encode(f\"{step_tag}\")[-1] # 12902\n",
    "# model = AutoModelForCausalLM.from_pretrained('peiyi9979/math-shepherd-mistral-7b-prm').eval()\n",
    "\n",
    "question = \"\"\"Janet\\u2019s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?\"\"\"\n",
    "output1 = \"\"\"Step 1: Janet's ducks lay 16 eggs per day. ки\\nStep 2: She eats three for breakfast every morning, so she has 16 - 3 = 13 eggs left. ки\\nStep 3: She bakes muffins for her friends every day with four eggs, so she has 13 - 4 = 9 eggs left. ки\\nStep 4: She sells the remainder at the farmers' market daily for $2 per fresh duck egg, so she makes 9 * $2 = $18 every day at the farmers' market. The answer is: 18 ки\"\"\" # 18 is right\n",
    "output2 = \"\"\"Step 1: Janet's ducks lay 16 eggs per day. ки\\nStep 2: She eats three for breakfast every morning, so she has 16 - 3 = 13 eggs left. ки\\nStep 3: She bakes muffins for her friends every day with four eggs, so she has 13 - 4 = 9 eggs left. ки\\nStep 4: She sells the remainder at the farmers' market daily for $2 per fresh duck egg, so she makes 9 * $2 = $17 every day at the farmers' market. The answer is: 17 ки\"\"\" # 17 is wrong\n",
    "\n",
    "inputs = []\n",
    "for output in [output1, output2]:\n",
    "    input_for_prm = f\"{question} {output}\"\n",
    "    inputs.append(input_for_prm)\n",
    "\n",
    "inputs_id = critic_tokenizer(inputs, padding=\"max_length\", truncation=True, max_length=256, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = critic(inputs_id['input_ids']).logits[:,:,candidate_tokens]\n",
    "    scores = logits.softmax(dim=-1)[:,:,0] \n",
    "    step_scores = scores[inputs_id['input_ids'] == step_tag_id]\n",
    "    print(step_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3926, 0.4844, 0.2949, 0.4219], device='cuda:0', dtype=torch.bfloat16)\n",
      "tensor([0.3926, 0.4844, 0.2949, 0.3203], device='cuda:0', dtype=torch.bfloat16)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for output in [output1, output2]:\n",
    "    input_for_prm = f\"{question} {output}\"\n",
    "    input_id = torch.tensor([critic_tokenizer.encode(input_for_prm)]).to(\"cuda\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = critic(input_id).logits[:,:,candidate_tokens]\n",
    "        scores = logits.softmax(dim=-1)[:,:,0] \n",
    "        step_scores = scores[input_id == step_tag_id]\n",
    "        print(step_scores)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 256, 128256])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    print(critic(inputs_id['input_ids']).logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing answers: 100%|██████████| 100/100 [50:50<00:00, 30.51s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import LlamaForSequenceClassification, AutoTokenizer, LlamaForCausalLM\n",
    "import torch\n",
    "\n",
    "# Usage example\n",
    "json_directory = \"generated_answers\"\n",
    "\n",
    "\n",
    "all_answers = load_json_answers(json_directory)\n",
    "extracted_answers = extract_answers(all_answers)\n",
    "answers_prob = compute_probabilities(all_answers, critic_tokenizer, critic)\n",
    "highest_probability_answers = select_highest_probability_answers(extracted_answers, answers_prob)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alpaca_prompt = You MUST copy from above!\n",
    "prompt = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "You are a helpful assistant to solve math problems step by step <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "{}\"\"\"\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    texts = []\n",
    "    final_answer = []\n",
    "    for instruction, answer in zip(examples['question'], examples['answer']):\n",
    "        # Combine all responses and the next response into a single string with newline separation\n",
    "        extracted_answer = answer.split('### ')[1]\n",
    "        final_answer.append(extracted_answer)\n",
    "        # Format the text with the prompt template\n",
    "        text = prompt.format(instruction, '')\n",
    "        texts.append(text)\n",
    "    \n",
    "    return {'input_text': texts, 'final_answer': final_answer}\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "dataset = load_dataset(\"gsm8k\", 'main', split='test')\n",
    "dataset = dataset.map(formatting_prompts_func, batched=True)  # Apply the preprocessing function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "product score method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy: 0.06\n"
     ]
    }
   ],
   "source": [
    "#change to the other probability\n",
    "accuracy = compare_with_ground_truth(highest_probability_answers, dataset['final_answer'])\n",
    "print(f\"\\nAccuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[489, 482]\n",
      "116624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing answers:   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing answers: 100%|██████████| 10/10 [05:04<00:00, 30.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy: 0.60\n"
     ]
    }
   ],
   "source": [
    "from transformers import LlamaForSequenceClassification, AutoTokenizer, LlamaForCausalLM\n",
    "import torch\n",
    "\n",
    "# Usage example\n",
    "json_directory = \"generated_answers_llama3\"\n",
    "\n",
    "\n",
    "all_answers = load_json_answers(json_directory, num_files = 10)\n",
    "extracted_answers = extract_answers(all_answers)\n",
    "answers_prob = compute_probabilities(all_answers, critic_tokenizer, critic, is_llama = True)\n",
    "highest_probability_answers = select_highest_probability_answers(extracted_answers, answers_prob)\n",
    "#change to the other probability\n",
    "accuracy = compare_with_ground_truth(highest_probability_answers, dataset['final_answer'])\n",
    "print(f\"\\nAccuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "min score method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[489, 482]\n",
      "116624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing answers:   0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing answers:   0%|          | 0/5 [00:31<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "min() arg is an empty sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m all_answers \u001b[38;5;241m=\u001b[39m load_json_answers(json_directory, num_files \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m      9\u001b[0m extracted_answers \u001b[38;5;241m=\u001b[39m extract_answers(all_answers)\n\u001b[0;32m---> 10\u001b[0m answers_prob \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_probabilities\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_answers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcritic_tokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcritic\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_llama\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m highest_probability_answers \u001b[38;5;241m=\u001b[39m select_highest_probability_answers(extracted_answers, answers_prob)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m#change to the other probability\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[6], line 96\u001b[0m, in \u001b[0;36mcompute_probabilities\u001b[0;34m(all_answers, critic_tokenizer, critic, batch_size, is_llama)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, count \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(response_counts):\n\u001b[1;32m     95\u001b[0m     answer_probs \u001b[38;5;241m=\u001b[39m correct_probabilities[probability_index:probability_index\u001b[38;5;241m+\u001b[39mcount]\n\u001b[0;32m---> 96\u001b[0m     answer_prob \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mmin\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43manswer_probs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m     answers_prob[i]\u001b[38;5;241m.\u001b[39mappend(answer_prob)\n\u001b[1;32m     98\u001b[0m     probability_index \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m count\n",
      "\u001b[0;31mValueError\u001b[0m: min() arg is an empty sequence"
     ]
    }
   ],
   "source": [
    "from transformers import LlamaForSequenceClassification, AutoTokenizer, LlamaForCausalLM\n",
    "import torch\n",
    "\n",
    "# Usage example\n",
    "json_directory = \"generated_answers_llama3\"\n",
    "\n",
    "\n",
    "all_answers = load_json_answers(json_directory, num_files = 5)\n",
    "extracted_answers = extract_answers(all_answers)\n",
    "answers_prob = compute_probabilities(all_answers, critic_tokenizer, critic, is_llama = True)\n",
    "highest_probability_answers = select_highest_probability_answers(extracted_answers, answers_prob)\n",
    "#change to the other probability\n",
    "accuracy = compare_with_ground_truth(highest_probability_answers, dataset['final_answer'])\n",
    "print(f\"\\nAccuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy: 0.66\n"
     ]
    }
   ],
   "source": [
    "#change to the other probability\n",
    "accuracy = compare_with_ground_truth(highest_probability_answers, dataset['final_answer'])\n",
    "print(f\"\\nAccuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing answers:   0%|          | 0/1 [01:29<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "min(): Expected reduction dim to be specified for input.numel() == 0. Specify the reduction dim with the 'dim' argument.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[89], line 57\u001b[0m\n\u001b[1;32m     55\u001b[0m     num_responses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(answer\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m### Response:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# answer_prob = torch.tensor(correct_probabilities[i:i+num_responses]).prod().item()\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m answer_prob \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcorrect_probabilities\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m:\u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mnum_responses\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     58\u001b[0m answers_prob[i]\u001b[38;5;241m.\u001b[39mappend(answer_prob)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: min(): Expected reduction dim to be specified for input.numel() == 0. Specify the reduction dim with the 'dim' argument."
     ]
    }
   ],
   "source": [
    "json_directory = \"generated_answers_llama3\"\n",
    "\n",
    "all_answers = load_json_answers(json_directory, num_files = 1)\n",
    "\n",
    "batch_size=1\n",
    "is_llama = True\n",
    "answers_prob = [[] for _ in range(len(all_answers[0]))]\n",
    "\n",
    "good_token = ' +'\n",
    "bad_token = '-'\n",
    "step_tag = ' ки'\n",
    "\n",
    "candidate_tokens = critic_tokenizer.encode(f\"{good_token} {bad_token}\") # [648, 387]\n",
    "step_tag_id = critic_tokenizer.encode(f\"{step_tag}\")[-1] # 12902\n",
    "\n",
    "with torch.no_grad():\n",
    "    for answers in tqdm(all_answers, desc=\"Processing answers\"):\n",
    "        results = []\n",
    "        if is_llama:\n",
    "            for answer in answers:\n",
    "                result = answer.split('assistant\\n\\n')[0].split('You are a helpful assistant to solve math problems step by step user\\n\\n')[1] + '\\n'\n",
    "                responses = answer.split('assistant\\n\\n')[1].split('\\n')\n",
    "                for response in responses:\n",
    "                    result += response + \" ки \\n\"\n",
    "                results.append(result)\n",
    "        else:\n",
    "            for answer in answers:\n",
    "                result = answer.split('### Response:')[0].split('\\n### Input:\\n')[1]\n",
    "                responses = answer.split('### Response:\\n')[1].split('\\n')\n",
    "                for response in responses:\n",
    "                    result += response + \" ки \\n\"\n",
    "                results.append(result)\n",
    "                            \n",
    "        correct_probabilities = []\n",
    "        for i in range(0, len(results), batch_size):\n",
    "            batch_results = results[i:i+batch_size]\n",
    "            \n",
    "            inputs = critic_tokenizer(batch_results, padding=\"max_length\", truncation=True, max_length=256, return_tensors=\"pt\").to(\"cuda\")\n",
    "            logits = critic(**inputs).logits[:,:,candidate_tokens]\n",
    "            scores = logits.softmax(dim=-1)[:,:,0] \n",
    "            step_scores = scores[inputs == step_tag_id]\n",
    "            correct_probabilities.extend(step_scores.tolist())\n",
    "            # score_product = step_scores.prod()\n",
    "            \n",
    "            \n",
    "            # probabilities = F.softmax(outputs.logits, dim=1)\n",
    "            # correct_probability = probabilities[:, 1:]\n",
    "            # correct_probability = torch.sum(correct_probability, dim=1)\n",
    "            # correct_probabilities.extend(correct_probability.tolist())\n",
    "        \n",
    "        for i, answer in enumerate(answers):\n",
    "            if is_llama:\n",
    "                num_responses = len(answer.split('assistant\\n\\n')[1].split('\\n'))\n",
    "            else:\n",
    "                num_responses = len(answer.split('### Response:\\n')[1].split('\\n'))\n",
    "            # answer_prob = torch.tensor(correct_probabilities[i:i+num_responses]).prod().item()\n",
    "            answer_prob = torch.tensor(correct_probabilities[i:i+num_responses]).min().item()\n",
    "            answers_prob[i].append(answer_prob)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5.1036e-07, 3.6671e-09, 2.5728e-08], device='cuda:0',\n",
       "       dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A robe takes 2 bolts of blue fiber and half that much white fiber.  How many bolts in total does it take?\n",
      "It takes 2 bolts of blue fiber and half that much white fiber. 2 * 1/2 = <<2*1/2=1>>1 bolt of white fiber ки \n",
      "So it takes 1 + 2 = <<1+2=3>>3 bolts of fiber ки \n",
      "#### 3 ки \n",
      "\n",
      "torch.Size([1, 256, 128256])\n",
      "tensor([0.3203, 0.0635, 0.2021], device='cuda:0', dtype=torch.bfloat16)\n",
      "0.004119873046875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/ipykernel_728464/3787946468.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  answer_prob = torch.tensor(step_scores).prod().item()\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    result_1 = results[1]\n",
    "    print(result_1)\n",
    "    inputs = critic_tokenizer(result_1, padding=\"max_length\", truncation=True, max_length=256, return_tensors=\"pt\").to(\"cuda\")\n",
    "    print(critic(**inputs).logits.shape)\n",
    "    logits = critic(**inputs).logits[:,:,candidate_tokens]\n",
    "    scores = logits.softmax(dim=-1)[:,:,0] \n",
    "    step_scores = scores[inputs['input_ids'] == step_tag_id]\n",
    "    print(step_scores)\n",
    "    answer_prob = torch.tensor(step_scores).prod().item()\n",
    "    print(answer_prob)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs[0] = result_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 256, 128256])\n",
      "tensor(0.0044, device='cuda:0', dtype=torch.bfloat16)\n",
      "tensor([0.3203, 0.0718, 0.1924, 0.2559, 0.0396, 0.1484, 0.2559, 0.0396, 0.1484],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n"
     ]
    }
   ],
   "source": [
    "question = \"\"\"A robe takes 2 bolts of blue fiber and half that much white fiber.  How many bolts in total does it take?\"\"\"\n",
    "output1 = \"\"\"It takes 2 bolts of blue fiber and half that much white fiber. 2 * 1/2 = <<2*1/2=1>>1 bolt of white fiber ки \n",
    "So it takes 1 + 2 = <<1+2=3>>3 bolts of fiber ки \n",
    "#### 3 ки \n",
    "\"\"\" # 18 is right\n",
    "output2 = \"\"\"It takes 2 bolts of blue fiber and half that much white fiber. 2 * 1/2 = <<2*1/2=1>>1 bolt of white fiber ки \n",
    "So it takes 1 + 2 = <<1+2=3>>3 bolts of fiber ки \n",
    "#### 4 ки \"\"\" # 17 is wrong\n",
    "output3 = \"\"\"It takes 2 bolts of blue fiber and half that much white fiber. 2 * 1/2 = <<2*1/2=1>>1 bolt of white fiber ки \n",
    "So it takes 1 + 2 = <<1+2=3>>3 bolts of fiber ки \n",
    "#### 9 ки \"\"\" # 17 is wrong\n",
    "\n",
    "# inputs = []\n",
    "# for output in [output1, output2,output3]:\n",
    "#     input_for_prm = f\"{question} {output}\"\n",
    "#     inputs.append(input_for_prm)\n",
    "\n",
    "inputs_id = critic_tokenizer(inputs, padding=\"max_length\", truncation=True, max_length=256, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    print(critic(**inputs_id).logits.shape)\n",
    "    logits = critic(**inputs_id).logits[:,:,candidate_tokens]\n",
    "    scores = logits.softmax(dim=-1)[:,:,0] \n",
    "    step_scores = scores[inputs_id['input_ids'] == step_tag_id]\n",
    "    print(step_scores[0:3].prod())\n",
    "    print(step_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A robe takes 2 bolts of blue fiber and half that much white fiber.  How many bolts in total does it take?\\n It takes 2 bolts of blue fiber and half that much white fiber. 2 * 1/2 = <<2*1/2=1>>1 bolt of white fiber ки \\nSo it takes 1 + 2 = <<1+2=3>>3 bolts of fiber ки \\n#### 3 ки '"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A robe takes 2 bolts of blue fiber and half that much white fiber.  How many bolts in total does it take?\\nIt takes 2 bolts of blue fiber and half that much white fiber. 2 * 1/2 = <<2*1/2=1>>1 bolt of white fiber ки \\nSo it takes 1 + 2 = <<1+2=3>>3 bolts of fiber ки \\n#### 3 ки \\n'"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0073, device='cuda:0', dtype=torch.bfloat16)\n",
      "tensor([0.3770, 0.1011, 0.1924], device='cuda:0', dtype=torch.bfloat16)\n",
      "tensor(0.0066, device='cuda:0', dtype=torch.bfloat16)\n",
      "tensor([0.3770, 0.1011, 0.1729], device='cuda:0', dtype=torch.bfloat16)\n"
     ]
    }
   ],
   "source": [
    "# good_token = ' +'\n",
    "# bad_token = '-'\n",
    "# step_tag = ' ки'\n",
    "\n",
    "# candidate_tokens = critic_tokenizer.encode(f\"{good_token} {bad_token}\")[1:] # [648, 387]\n",
    "# step_tag_id = critic_tokenizer.encode(f\"{step_tag}\")[-1] # 12902\n",
    "# print(candidate_tokens)\n",
    "for output in [output1, output2]:\n",
    "    input_for_prm = f\"{question} {output}\"\n",
    "    input_id = torch.tensor([critic_tokenizer.encode(input_for_prm)]).to(\"cuda\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = critic(input_id).logits[:,:,candidate_tokens]\n",
    "        scores = logits.softmax(dim=-1)[:,:,0] \n",
    "        step_scores = scores[input_id == step_tag_id]\n",
    "        print(step_scores.prod())\n",
    "        print(step_scores)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.8150832720791174e-23"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(correct_probabilities).prod().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmrl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
