{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Modeling instead of classification head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The paper [MATH-SHEPHERD](https://huggingface.co/datasets/peiyi9979/Math-Shepherd) present a more elegant solution, which is use language modeling then directly estimate from the turn tokens. Although their code is not released but it is quite simple to implement compared to my previous method for PRM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use native unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jqi/anaconda3/envs/llmrl/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth: Fast Llama patching release 2024.4\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 4090. Max memory: 23.642 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.2.2. CUDA = 8.9. CUDA Toolkit = 11.8.\n",
      "\\        /    Bfloat16 = TRUE. Xformers = 0.0.25.post1. FA = True.\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  2.62it/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Unsloth 2024.4 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "from transformers import LlamaForSequenceClassification, AutoTokenizer, LlamaForCausalLM\n",
    "import torch\n",
    "import wandb\n",
    "import os\n",
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = False # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"checkpoints/llama3-8b-gsm8k-1epoch\", # \"unsloth/tinyllama\" for 16bit loading\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")\n",
    "\n",
    "#Use LoRA to reduce memory usage:\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 32, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 32,\n",
    "    lora_dropout = 0, # Currently only supports dropout = 0\n",
    "    bias = \"none\",    # Currently only supports bias = \"none\"\n",
    "    use_gradient_checkpointing = \"unsloth\", # @@@ IF YOU GET OUT OF MEMORY - set to True @@@\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prompt = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "You are a helpful assistant to solve math problems step by step <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "{}\"\"\"\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    texts = []\n",
    "    \n",
    "    for instruction, responses, next_response, rating in zip(examples['instruction'], examples['responses'], examples['next_response'], examples['rating']):\n",
    "        # Combine all responses and the next response into a single string with newline separation\n",
    "        combined_responses = \" + \\n\".join(responses) + \" + \\n\" + next_response\n",
    "        if rating == -1:\n",
    "            combined_responses = combined_responses + \" - \\n\"\n",
    "        else:\n",
    "            combined_responses = combined_responses + \" + \\n\"\n",
    "        \n",
    "        # Format the text with the prompt template\n",
    "        text = prompt.format(instruction, combined_responses) \n",
    "        texts.append(text)\n",
    "\n",
    "    \n",
    "    return {\"text\": texts,}\n",
    "\n",
    "    # # Tokenize all texts at once using the tokenizer\n",
    "    # model_inputs = tokenizer(texts, padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "    # # Add labels to the model inputs\n",
    "    # model_inputs['labels'] = labels\n",
    "    \n",
    "    # return model_inputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we treat it as a language model task, then the incorrect steps that ends there should also be included in the dataset. It means we still need to append all the incorrect steps.\n",
    "\n",
    "We can use a token to represent each step's correctness, such as + and -.\n",
    "For the prediction of the probability of + and - during inference. We use the special token to represent the correctness of the step. We then use softmax to get the probability of correctness.\n",
    "\n",
    "During training, because decoder is auto-regressive, we don't need to predict the correctness of the step. We can just use the token to represent the correctness of the step. We can use the special token to represent the correctness of the step. We then use softmax to get the probability of the special token, which is the correctness of the step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 1015027/1015027 [00:11<00:00, 89580.28 examples/s]\n",
      "Map: 100%|██████████| 369283/369283 [00:16<00:00, 21922.39 examples/s]\n",
      "Map: 100%|██████████| 369283/369283 [00:01<00:00, 189351.47 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "369283"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "dataset = load_dataset(\"Birchlabs/openai-prm800k-stepwise-critic\", split='train')\n",
    "dataset = dataset.filter(lambda x: x['rating'] is not None)  # Filter entries without ratings\n",
    "\n",
    "#filter out the examples that has 'next_response' in the responses of the solution\n",
    "dataset = dataset.filter(lambda x: not(x['rating'] == 1 and x['is_solution'] == False))\n",
    "\n",
    "#convert ratings of 0 to 1 so we have only binary labels\n",
    "dataset = dataset.map(lambda x: {'rating': 1 if x['rating'] == 0 else x['rating']})\n",
    "\n",
    "dataset = dataset.map(formatting_prompts_func, batched=True)  # Apply the preprocessing function\n",
    "\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "Map (num_proc=2): 100%|██████████| 369283/369283 [00:51<00:00, 7157.29 examples/s] \n",
      "Using auto half precision backend\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from transformers.utils import logging\n",
    "logging.set_verbosity_info()\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False, \n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_ratio = 0.1,\n",
    "        num_train_epochs = 1,\n",
    "        learning_rate = 2e-5,\n",
    "        fp16 = not torch.cuda.is_bf16_supported(),\n",
    "        bf16 = torch.cuda.is_bf16_supported(),\n",
    "        logging_steps = 100,\n",
    "        save_steps= 5000,\n",
    "        save_total_limit=2,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.1,\n",
    "        lr_scheduler_type = \"cosine\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"checkpoints/llama3-8b-critic-lora\",\n",
    "        report_to= \"wandb\"\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #@title Show current memory stats\n",
    "# gpu_stats = torch.cuda.get_device_properties(0)\n",
    "# start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "# max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "# print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "# print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 369,283 | Num Epochs = 1\n",
      "O^O/ \\_/ \\    Batch size per device = 4 | Gradient Accumulation steps = 4\n",
      "\\        /    Total batch size = 16 | Total steps = 23,080\n",
      " \"-____-\"     Number of trainable parameters = 83,886,080\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='23080' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   10/23080 00:32 < 25:39:12, 0.25 it/s, Epoch 0.00/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trainer_stats \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/llmrl/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:361\u001b[0m, in \u001b[0;36mSFTTrainer.train\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneftune_noise_alpha \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer_supports_neftune:\n\u001b[1;32m    359\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trl_activate_neftune(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel)\n\u001b[0;32m--> 361\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;66;03m# After training we make sure to retrieve back the original forward pass method\u001b[39;00m\n\u001b[1;32m    364\u001b[0m \u001b[38;5;66;03m# for the embedding layer by removing the forward post hook.\u001b[39;00m\n\u001b[1;32m    365\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneftune_noise_alpha \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer_supports_neftune:\n",
      "File \u001b[0;32m~/anaconda3/envs/llmrl/lib/python3.10/site-packages/transformers/trainer.py:1780\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1778\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1780\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1781\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1782\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1783\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1784\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1785\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<string>:355\u001b[0m, in \u001b[0;36m_fast_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/llmrl/lib/python3.10/site-packages/transformers/trainer.py:3045\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   3043\u001b[0m         scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m   3044\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3045\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3047\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n",
      "File \u001b[0;32m~/anaconda3/envs/llmrl/lib/python3.10/site-packages/accelerate/accelerator.py:2013\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2011\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   2012\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2013\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/llmrl/lib/python3.10/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/llmrl/lib/python3.10/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"checkpoints/llama3-8b-critic-lora\") # Local saving\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jqi/anaconda3/envs/llmrl/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth: Fast Llama patching release 2024.4\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 4090. Max memory: 23.642 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.2.2. CUDA = 8.9. CUDA Toolkit = 11.8.\n",
      "\\        /    Bfloat16 = TRUE. Xformers = 0.0.25.post1. FA = True.\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  2.60it/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Unsloth 2024.4 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "from transformers import LlamaForSequenceClassification, AutoTokenizer, LlamaForCausalLM\n",
    "import torch\n",
    "import wandb\n",
    "import os\n",
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = False # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"checkpoints/llama3-8b-critic-SFT-id_label\", # \"unsloth/tinyllama\" for 16bit loading\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "tokenizer.padding_side = \"left\" # Padding side for faster inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "good_token = '+'\n",
    "bad_token = '-'\n",
    "step_tag = ' ки'\n",
    "\n",
    "candidate_tokens = tokenizer.encode(f\"{good_token} {bad_token}\") # [648, 387]\n",
    "step_tag_id = tokenizer.encode(f\"{step_tag}\")[-1] # 12902\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.5047e-13, device='cuda:0', dtype=torch.bfloat16)\n",
      "tensor([9.8828e-01, 1.4877e-04, 8.0490e-04, 1.5030e-03, 1.4114e-03],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n",
      "tensor(5.0477e-11, device='cuda:0', dtype=torch.bfloat16)\n",
      "tensor([9.8438e-01, 1.1587e-04, 3.7956e-04, 1.1673e-03], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    }
   ],
   "source": [
    "question = \"\"\"Josh decides to try flipping a house.  He buys a house for $80,000 and then puts in $50,000 in repairs.  This increased the value of the house by 150%.  How much profit did he make \\n\"\"\"\n",
    "output1 = \"\"\"The cost of the house and repairs came out to 80,000+50,000=$<<80000+50000=130000>>130,000 ки\\nHe increased the value of the house by 80,000*1.5=<<80000*1.5=120000>>120,000 ки\\nSo the new value of the house is 120,000+80,000=$<<120000+80000=200000>>200,000 ки\\nSo he made a profit of 200,000-130,000=$<<200000-130000=70000>>70,000 ки\\n#### 70000 ки\"\"\" # 18 is right\n",
    "output2 = \"\"\"The house was worth 80,000 + 50,000 = $<<80000+50000=130000>>130,000 after the repairs ки\\nThe house is now worth 130,000 x 150% = $<<130000*150*.01=195000>>195,000 ки\\nHe made a profit of 195,000 - 130,000 = $<<195000-130000=65000>>65,000 ки\\n#### 65 ки\"\"\" #wrong\n",
    "\n",
    "for output in [output1, output2]:\n",
    "    input_for_prm = f\"{question} {output}\"\n",
    "    input_id = torch.tensor([tokenizer.encode(input_for_prm)]).to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_id).logits[:,:,candidate_tokens]\n",
    "        scores = logits.softmax(dim=-1)[:,:,0] \n",
    "        step_scores = scores[input_id == step_tag_id]\n",
    "        score_product = step_scores.prod()\n",
    "        print(score_product)\n",
    "        print(step_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.7107e-09, device='cuda:0', dtype=torch.bfloat16)\n",
      "tensor([0.2334, 0.0012, 0.0012, 0.0117], device='cuda:0', dtype=torch.bfloat16)\n",
      "tensor(4.4529e-09, device='cuda:0', dtype=torch.bfloat16)\n",
      "tensor([0.2334, 0.0012, 0.0012, 0.0140], device='cuda:0', dtype=torch.bfloat16)\n"
     ]
    }
   ],
   "source": [
    "question = \"\"\"Janet\\u2019s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market? \\n\"\"\"\n",
    "output1 = \"\"\"Step 1: Janet's ducks lay 16 eggs per day. ки\\nStep 2: She eats three for breakfast every morning, so she has 16 - 3 = 13 eggs left. ки\\nStep 3: She bakes muffins for her friends every day with four eggs, so she has 13 - 4 = 9 eggs left. ки\\nStep 4: She sells the remainder at the farmers' market daily for $2 per fresh duck egg, so she makes 9 * $2 = $18 every day at the farmers' market. The answer is: 18 ки\"\"\" # 18 is right\n",
    "output2 = \"\"\"Step 1: Janet's ducks lay 16 eggs per day. ки\\nStep 2: She eats three for breakfast every morning, so she has 16 - 3 = 13 eggs left. ки\\nStep 3: She bakes muffins for her friends every day with four eggs, so she has 13 - 4 = 9 eggs left. ки\\nStep 4: She sells the remainder at the farmers' market daily for $2 per fresh duck egg, so she makes 9 * $2 = $17 every day at the farmers' market. The answer is: 17 ки\"\"\" # 17 is wrong\n",
    "\n",
    "for output in [output1, output2]:\n",
    "    input_for_prm = f\"{question} {output}\"\n",
    "    input_id = torch.tensor([tokenizer.encode(input_for_prm)]).to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_id).logits[:,:,candidate_tokens]\n",
    "        scores = logits.softmax(dim=-1)[:,:,0] \n",
    "        step_scores = scores[input_id == step_tag_id]\n",
    "        score_product = step_scores.prod()\n",
    "        print(score_product)\n",
    "        print(step_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"\"\"Janet\\u2019s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market? \\n\"\"\"\n",
    "output1 = \"\"\"Step 1: Janet's ducks lay 16 eggs per day. ки\\nStep 2: She eats three for breakfast every morning, so she has 16 - 3 = 13 eggs left. ки\\nStep 3: She bakes muffins for her friends every day with four eggs, so she has 13 - 4 = 9 eggs left. ки\\nStep 4: She sells the remainder at the farmers' market daily for $2 per fresh duck egg, so she makes 9 * $2 = $18 every day at the farmers' market. The answer is: 18 ки\"\"\" # 18 is right\n",
    "output2 = \"\"\"Step 1: Janet's ducks lay 16 eggs per day. ки\\nStep 2: She eats three for breakfast every morning, so she has 16 - 3 = 13 eggs left. ки\\nStep 3: She bakes muffins for her friends every day with four eggs, so she has 13 - 4 = 9 eggs left. ки\\nStep 4: She sells the remainder at the farmers' market daily for $2 per fresh duck egg, so she makes 9 * $2 = $17 every day at the farmers' market. The answer is: 17 ки\"\"\" # 17 is wrong\n",
    "input_for_prm = []\n",
    "for output in [output1, output2]:\n",
    "    input_for_prm.append(f\"{question} {output}\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    inputs = critic_tokenizer(input_for_prm, padding=\"max_length\", truncation=True, max_length=256, return_tensors=\"pt\").to(\"cuda\")\n",
    "    outputs = critic(**inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 256, 128256])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.logits[:,:,candidate_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "sys.path.append('Data/prm800k/prm800k')\n",
    "from grading import grader\n",
    "\n",
    "def load_json_answers(directory):\n",
    "    json_files = [file for file in os.listdir(directory) if file.endswith(\".json\")]\n",
    "    all_answers = []\n",
    "    for file in json_files:\n",
    "        file_path = os.path.join(directory, file)\n",
    "        with open(file_path, \"r\") as f:\n",
    "            answers = json.load(f)\n",
    "            all_answers.append(answers)\n",
    "    return all_answers\n",
    "\n",
    "def extract_answers(all_answers):\n",
    "    extracted_answers = [[] for _ in range(len(all_answers[0]))]\n",
    "    for answers in all_answers:\n",
    "        for i, answer in enumerate(answers):\n",
    "            match = re.search(r\"####\\s*(.*)\", answer)\n",
    "            if match:\n",
    "                extracted_answers[i].append(match.group(1).strip())\n",
    "            else:\n",
    "                extracted_answers[i].append(\"\")\n",
    "    return extracted_answers\n",
    "\n",
    "def compute_probabilities(all_answers, critic_tokenizer, critic, batch_size=32):\n",
    "    answers_prob = [[] for _ in range(len(all_answers[0]))]\n",
    "    \n",
    "    good_token = '+'\n",
    "    bad_token = '-'\n",
    "    step_tag = ' ки'\n",
    "\n",
    "    candidate_tokens = critic_tokenizer.encode(f\"{good_token} {bad_token}\") # [648, 387]\n",
    "    step_tag_id = critic_tokenizer.encode(f\"{step_tag}\")[-1] # 12902\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for answers in tqdm(all_answers, desc=\"Processing answers\"):\n",
    "            results = []\n",
    "            for answer in answers:\n",
    "                result = answer.split('assistant\\n\\n')[0].split('You are a helpful assistant to solve math problems step by step user\\n\\n')[1] + '\\n'\n",
    "                responses = answer.split('assistant\\n\\n')[1].split('\\n')\n",
    "                for response in responses:\n",
    "                    result += response + \" ки\\n\"\n",
    "                results.append(result)\n",
    "                                \n",
    "            correct_probabilities = []\n",
    "            for i in range(0, len(results), batch_size):\n",
    "                batch_results = results[i:i+batch_size]\n",
    "                inputs = critic_tokenizer(batch_results, padding=\"max_length\", truncation=True, max_length=256, return_tensors=\"pt\").to(\"cuda\")\n",
    "                logits = critic(**inputs).logits[:,:,candidate_tokens]\n",
    "                scores = logits.softmax(dim=-1)[:,:,0] \n",
    "                step_scores = scores[inputs == step_tag_id]\n",
    "                correct_probabilities.extend(step_scores.tolist())\n",
    "                # score_product = step_scores.prod()\n",
    "                \n",
    "                \n",
    "                # probabilities = F.softmax(outputs.logits, dim=1)\n",
    "                # correct_probability = probabilities[:, 1:]\n",
    "                # correct_probability = torch.sum(correct_probability, dim=1)\n",
    "                # correct_probabilities.extend(correct_probability.tolist())\n",
    "            \n",
    "            for i, answer in enumerate(answers):\n",
    "                num_responses = len(answer.split('assistant\\n\\n')[1].split('\\n'))\n",
    "                answer_prob = torch.tensor(correct_probabilities[i:i+num_responses]).prod().item()\n",
    "                answers_prob[i].append(answer_prob)\n",
    "    \n",
    "    return answers_prob\n",
    "\n",
    "def select_highest_probability_answers(extracted_answers, answers_prob):\n",
    "    highest_probability_answers = []\n",
    "    for i, question_answers in enumerate(extracted_answers):\n",
    "        question_probs = answers_prob[i]\n",
    "        if question_probs:\n",
    "            max_prob_index = question_probs.index(max(question_probs))\n",
    "            highest_probability_answer = question_answers[max_prob_index]\n",
    "        else:\n",
    "            highest_probability_answer = \"\"\n",
    "        highest_probability_answers.append(highest_probability_answer)\n",
    "    return highest_probability_answers\n",
    "\n",
    "def compare_with_ground_truth(majority_answers, ground_truth_answers):\n",
    "    correct_count = 0\n",
    "    for majority_answer, ground_truth_answer in zip(majority_answers, ground_truth_answers):\n",
    "        if grader.grade_answer(majority_answer, ground_truth_answer):\n",
    "            correct_count += 1\n",
    "    accuracy = correct_count / len(ground_truth_answers)\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jqi/anaconda3/envs/llmrl/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth: Fast Llama patching release 2024.4\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 4090. Max memory: 23.642 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.2.2. CUDA = 8.9. CUDA Toolkit = 11.8.\n",
      "\\        /    Bfloat16 = TRUE. Xformers = 0.0.25.post1. FA = True.\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  2.59it/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Unsloth 2024.4 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "from transformers import LlamaForSequenceClassification, AutoTokenizer, LlamaForCausalLM\n",
    "import torch\n",
    "import wandb\n",
    "import os\n",
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = False # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "\n",
    "critic, critic_tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"checkpoints/llama3-8b-critic-lora\", # \"unsloth/tinyllama\" for 16bit loading\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")\n",
    "FastLanguageModel.for_inference(critic) # Enable native 2x faster inference\n",
    "critic_tokenizer.padding_side = \"left\" # Padding side for faster inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing answers: 100%|██████████| 100/100 [1:08:56<00:00, 41.37s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import LlamaForSequenceClassification, AutoTokenizer, LlamaForCausalLM\n",
    "import torch\n",
    "\n",
    "# Usage example\n",
    "json_directory = \"generated_answers_llama3\"\n",
    "\n",
    "\n",
    "all_answers = load_json_answers(json_directory)\n",
    "extracted_answers = extract_answers(all_answers)\n",
    "answers_prob = compute_probabilities(all_answers, critic_tokenizer, critic)\n",
    "highest_probability_answers = select_highest_probability_answers(extracted_answers, answers_prob)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1319/1319 [00:00<00:00, 100377.16 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# alpaca_prompt = You MUST copy from above!\n",
    "prompt = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "You are a helpful assistant to solve math problems step by step <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "{}\"\"\"\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    texts = []\n",
    "    final_answer = []\n",
    "    for instruction, answer in zip(examples['question'], examples['answer']):\n",
    "        # Combine all responses and the next response into a single string with newline separation\n",
    "        extracted_answer = answer.split('### ')[1]\n",
    "        final_answer.append(extracted_answer)\n",
    "        # Format the text with the prompt template\n",
    "        text = prompt.format(instruction, '')\n",
    "        texts.append(text)\n",
    "    \n",
    "    return {'input_text': texts, 'final_answer': final_answer}\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "dataset = load_dataset(\"gsm8k\", 'main', split='test')\n",
    "dataset = dataset.map(formatting_prompts_func, batched=True)  # Apply the preprocessing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy: 0.66\n"
     ]
    }
   ],
   "source": [
    "#change to the other probability\n",
    "accuracy = compare_with_ground_truth(highest_probability_answers, dataset['final_answer'])\n",
    "print(f\"\\nAccuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmrl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
