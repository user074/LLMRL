{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DPO method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This setup of 64 alpha and rank would require: 34696MiB / 40960MiB VRAM in order to proceed. Batch size 2 and accum_grad 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/llmrl/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth: Fast Llama patching release 2024.4\n",
      "   \\\\   /|    GPU: NVIDIA A100-SXM4-40GB. Max memory: 39.394 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.2.2. CUDA = 8.0. CUDA Toolkit = 11.8.\n",
      "\\        /    Bfloat16 = TRUE. Xformers = 0.0.25.post1. FA = True.\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n",
      "Unsloth 2024.4 patched 22 layers with 22 QKV layers, 22 O layers and 22 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "LORA_RANK = 64\n",
    "LORA_ALPHA = 128\n",
    "\n",
    "\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"checkpoints/tinyLlama-GSM8K-10epochs\", # \"unsloth/tinyllama\" for 16bit loading\n",
    "    max_seq_length = 2048,\n",
    "    dtype = None,\n",
    "    load_in_4bit = False,\n",
    ")\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"v_proj\",\n",
    "        \"k_proj\",\n",
    "        \"o_proj\",  # attention (self_attn)\n",
    "        \"gate_proj\",\n",
    "        \"down_proj\",\n",
    "        \"up_proj\",  # FFN (mlp)\n",
    "    ],\n",
    "    r=LORA_RANK,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    lora_dropout=0,\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth: Fast Llama patching release 2024.4\n",
      "   \\\\   /|    GPU: NVIDIA A100-SXM4-40GB. Max memory: 39.394 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.2.2. CUDA = 8.0. CUDA Toolkit = 11.8.\n",
      "\\        /    Bfloat16 = TRUE. Xformers = 0.0.25.post1. FA = True.\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.39s/it]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Unsloth 2024.4 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "critic, critic_tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"/home/jianingqi/LLMRL/checkpoints/llama3-8b-critic-lora-4-29\", # \"unsloth/tinyllama\" for 16bit loading\n",
    "    max_seq_length = 2048,\n",
    "    dtype = None,\n",
    "    load_in_4bit = False,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")\n",
    "FastLanguageModel.for_inference(critic) # Enable native 2x faster inference\n",
    "critic_tokenizer.padding_side = \"left\" # Padding side for faster inference\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rollout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from transformers.utils import logging\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "def generate_answers(input_text, generator, tokenizer, n_answers=2, batch_size=128):    \n",
    "    all_answers_list = []\n",
    "    for n in tqdm(range(0, n_answers), desc=\" Answer Set\", position=0):\n",
    "        all_answers = []\n",
    "        for i in tqdm(range(0, len(input_text), batch_size), desc=\"Answers in Answer Set\", position=1, leave=True):\n",
    "            batch_inputs = input_text[i:i+batch_size]\n",
    "            batch_inputs = tokenizer(batch_inputs, return_tensors='pt', padding=\"max_length\", truncation=True, max_length=256).to(device)\n",
    "            outputs = generator.generate(\n",
    "                **batch_inputs,\n",
    "                max_new_tokens=256,\n",
    "                use_cache=True,\n",
    "                do_sample=True,\n",
    "                temperature=0.5,\n",
    "                top_k=40\n",
    "            )\n",
    "            answers = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "            all_answers.extend(answers)\n",
    "        \n",
    "        print(f\"Generated {len(all_answers)} answers for set {n}.\")\n",
    "        all_answers_list.append(all_answers)\n",
    "    \n",
    "    return all_answers_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_probabilities(all_answers, critic_tokenizer, critic, batch_size=32, is_llama = True):\n",
    "    answers_prob = [[] for _ in range(len(all_answers[0]))]\n",
    "    \n",
    "    good_token = ' +'\n",
    "    bad_token = '-'\n",
    "    step_tag = ' ки'\n",
    "\n",
    "    candidate_tokens = critic_tokenizer.encode(f\"{good_token} {bad_token}\")[1:] # [648, 387]\n",
    "    step_tag_id = critic_tokenizer.encode(f\"{step_tag}\")[-1] # 12902\n",
    "    # print(candidate_tokens)\n",
    "    # print(step_tag_id)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for answers in tqdm(all_answers, desc=\"Processing rewards\", position=0):\n",
    "            results = []\n",
    "            response_counts = []\n",
    "            for answer in answers:\n",
    "                if '### Response:' in answer:\n",
    "                    result = answer.split('### Response:')[0]\n",
    "                    responses = answer.split('### Response:\\n')[1].split('\\n')\n",
    "                    num_responses = len(responses)\n",
    "                    response_counts.append(num_responses)\n",
    "                elif '?' in answer:\n",
    "                    # print(answer)\n",
    "                    result = answer.split('?')[0] + '?'\n",
    "                    responses = answer.split('?')[1].split('\\n')\n",
    "                    num_responses = len(responses)\n",
    "                    response_counts.append(num_responses)\n",
    "                elif '####' in answer:\n",
    "                    result = answer.split('####')[0]\n",
    "                    responses = answer.split('####')[1].split('\\n')\n",
    "                    responses[0] = '####' + responses[0]\n",
    "                    num_responses = len(responses)\n",
    "                    response_counts.append(num_responses)\n",
    "                else:\n",
    "                    result = answer\n",
    "                    responses = ['']\n",
    "                    num_responses = len(responses)\n",
    "                    response_counts.append(num_responses)\n",
    "                    \n",
    "                     \n",
    "                for response in responses:\n",
    "                    result += response + \" ки \\n\"\n",
    "                results.append(result)\n",
    "                                \n",
    "            correct_probabilities = []\n",
    "            for i in tqdm(range(0, len(results), batch_size), desc=\"Processing batch\",position=1,  leave=True):\n",
    "                batch_results = results[i:i+batch_size]\n",
    "                \n",
    "                inputs = critic_tokenizer(batch_results, padding=\"max_length\", truncation=True, max_length=512, return_tensors=\"pt\").to(\"cuda\")\n",
    "                logits = critic(**inputs).logits[:,:,candidate_tokens]\n",
    "                scores = logits.softmax(dim=-1)[:,:,0] \n",
    "                step_scores = scores[inputs['input_ids'] == step_tag_id]\n",
    "                correct_probabilities.extend(step_scores.tolist())\n",
    "            \n",
    "            # response_counts = []\n",
    "            # for answer in answers:\n",
    "            #     num_responses = len(answer.split('### Response:\\n')[1].split('\\n'))\n",
    "            #     response_counts.append(num_responses)\n",
    "            \n",
    "            probability_index = 0\n",
    "            for i, count in enumerate(response_counts):\n",
    "                answer_probs = correct_probabilities[probability_index:probability_index+count]\n",
    "                if answer_probs:\n",
    "                    # answer_prob = min(answer_probs)\n",
    "                    answer_prob = torch.tensor(answer_probs).prod().item()\n",
    "                    answers_prob[i].append(answer_prob)\n",
    "                else:\n",
    "                    print('len of prob')\n",
    "                    print(len(correct_probabilities))\n",
    "                    print('len of responses')\n",
    "                    print(sum(response_counts))\n",
    "                    print('There is a length mismatch')\n",
    "                    print('-----', i)\n",
    "                    print(answers[i])\n",
    "                    answers_prob[i].append(0.0)\n",
    "                probability_index += count\n",
    "    \n",
    "    return answers_prob\n",
    "\n",
    "def select_high_low_probability_answers(all_answers, answers_prob):\n",
    "    highest_probability_answers = []\n",
    "    lowest_probability_answers = []\n",
    "    extracted_answers = [[] for _ in range(len(all_answers[0]))]\n",
    "    for answers in all_answers:\n",
    "        for i, answer in enumerate(answers):\n",
    "            extracted_answers[i].append(answer)\n",
    "                \n",
    "    for i, question_answers in enumerate(extracted_answers):\n",
    "        question_probs = answers_prob[i]\n",
    "        if question_probs:\n",
    "            max_prob_index = question_probs.index(max(question_probs))\n",
    "            highest_probability_answer = question_answers[max_prob_index]\n",
    "            min_prob_index = question_probs.index(min(question_probs))\n",
    "            lowest_probability_answer = question_answers[min_prob_index]\n",
    "        else:\n",
    "            highest_probability_answer = \"\"\n",
    "            lowest_probability_answer = \"\"\n",
    "        highest_probability_answers.append(highest_probability_answer)\n",
    "        lowest_probability_answers.append(lowest_probability_answer)\n",
    "    return highest_probability_answers, lowest_probability_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rollout_to_DPO_dataset(dataset, model, tokenizer, critic_tokenizer, critic, device = \"cuda\"):\n",
    "    model.to(device)\n",
    "    print('Rolling Out from model')\n",
    "    with torch.no_grad():\n",
    "        answers = generate_answers(dataset['prompt'], model, tokenizer, n_answers=2)\n",
    "    print('Roll out completed')\n",
    "    print('Starting to compute rewards')\n",
    "    answers_prob = compute_probabilities(answers, critic_tokenizer, critic)\n",
    "    highest_probability_answers, lowest_probability_answers = select_high_low_probability_answers(answers, answers_prob)\n",
    "\n",
    "    # Add the \"chosen\" column\n",
    "    epoch_dataset = dataset\n",
    "    epoch_dataset = epoch_dataset.add_column(\"chosen\", highest_probability_answers)\n",
    "    # Add the \"rejected\" column\n",
    "    epoch_dataset = epoch_dataset.add_column(\"rejected\", lowest_probability_answers)\n",
    "\n",
    "    # Compute rewards based on answer probabilities\n",
    "    rewards = []\n",
    "    for probs in answers_prob:\n",
    "        if probs:\n",
    "            max_prob = max(probs)\n",
    "            min_prob = min(probs)\n",
    "            rewards.append([max_prob, min_prob])\n",
    "        else:\n",
    "            rewards.append([0.0, 0.0])\n",
    "\n",
    "    return epoch_dataset, rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epoch_dataset, rewards = rollout_to_DPO_dataset(dataset, model, tokenizer, critic_tokenizer, critic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train DPO model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One must patch the DPO Trainer first!\n",
    "from unsloth import PatchDPOTrainer\n",
    "PatchDPOTrainer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"gsm8k\", 'main', split='train')\n",
    "dataset = dataset.rename_column('question', 'prompt')\n",
    "\n",
    "dataset = dataset.remove_columns('answer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, get_scheduler\n",
    "from trl import DPOTrainer\n",
    "from torch.optim import AdamW\n",
    "\n",
    "epochs = 10\n",
    "base_lr = 4e-6\n",
    "total_steps = len(dataset) * epochs\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=base_lr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjq394\u001b[0m (\u001b[33mneurorunner\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_PROJECT=LLMRL\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()\n",
    "%env WANDB_PROJECT=LLMRL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args= TrainingArguments(\n",
    "            per_device_train_batch_size=2,\n",
    "            gradient_accumulation_steps=4,\n",
    "            warmup_ratio=0.1,\n",
    "            num_train_epochs=1,\n",
    "            fp16=not torch.cuda.is_bf16_supported(),\n",
    "            bf16=torch.cuda.is_bf16_supported(),\n",
    "            logging_steps=1,\n",
    "            optim=\"adamw_8bit\",\n",
    "            weight_decay=0.0,\n",
    "            lr_scheduler_type=\"constant\",  # Set the scheduler type to \"constant\"\n",
    "            seed=42,\n",
    "            output_dir=\"checkpoints/dpo-tinyllama-5-1\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rolling Out from model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Answers in Answer Set: 100%|██████████| 11/11 [01:59<00:00, 10.83s/it]\n",
      " Answer Set:  50%|█████     | 1/2 [01:59<01:59, 119.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1319 answers for set 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Answers in Answer Set: 100%|██████████| 11/11 [01:58<00:00, 10.78s/it]\n",
      " Answer Set: 100%|██████████| 2/2 [03:57<00:00, 118.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1319 answers for set 1.\n",
      "Roll out completed\n",
      "Starting to compute rewards\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch: 100%|██████████| 42/42 [01:00<00:00,  1.44s/it]\n",
      "Processing batch: 100%|██████████| 42/42 [01:00<00:00,  1.43s/it]\n",
      "Processing rewards: 100%|██████████| 2/2 [02:00<00:00, 60.26s/it]\n",
      "/opt/conda/envs/llmrl/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:332: UserWarning: When using DPODataCollatorWithPadding, you should set `remove_unused_columns=False` in your TrainingArguments we have set it for you, but you should do it yourself in the future.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jianingqi/LLMRL/wandb/run-20240501_190228-zggo153e</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/neurorunner/LLMRL/runs/zggo153e' target=\"_blank\">confused-monkey-78</a></strong> to <a href='https://wandb.ai/neurorunner/LLMRL' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/neurorunner/LLMRL' target=\"_blank\">https://wandb.ai/neurorunner/LLMRL</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/neurorunner/LLMRL/runs/zggo153e' target=\"_blank\">https://wandb.ai/neurorunner/LLMRL/runs/zggo153e</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6931, 'grad_norm': 5.455300807952881, 'learning_rate': 5e-05, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/rejected': -77.1103515625, 'logps/chosen': -57.6927490234375, 'logits/rejected': -2.551132917404175, 'logits/chosen': -1.8377224206924438, 'epoch': 0.01}\n",
      "{'loss': 0.7039, 'grad_norm': 6.54119873046875, 'learning_rate': 5e-05, 'rewards/chosen': -0.07998597621917725, 'rewards/rejected': -0.06499624252319336, 'rewards/accuracies': 0.25, 'rewards/margins': -0.014989733695983887, 'logps/rejected': -84.58383178710938, 'logps/chosen': -67.00834655761719, 'logits/rejected': -2.7782607078552246, 'logits/chosen': -2.210334539413452, 'epoch': 0.01}\n",
      "{'loss': 0.6919, 'grad_norm': 5.349400997161865, 'learning_rate': 5e-05, 'rewards/chosen': -0.18570928275585175, 'rewards/rejected': -0.19020715355873108, 'rewards/accuracies': 0.5, 'rewards/margins': 0.004497861489653587, 'logps/rejected': -77.52427673339844, 'logps/chosen': -67.13036346435547, 'logits/rejected': -2.7393107414245605, 'logits/chosen': -2.507023572921753, 'epoch': 0.02}\n",
      "{'loss': 0.6352, 'grad_norm': 5.345083236694336, 'learning_rate': 5e-05, 'rewards/chosen': -0.5304134488105774, 'rewards/rejected': -0.6623166799545288, 'rewards/accuracies': 0.75, 'rewards/margins': 0.1319032460451126, 'logps/rejected': -87.64178466796875, 'logps/chosen': -68.2567138671875, 'logits/rejected': -2.9532716274261475, 'logits/chosen': -2.484788417816162, 'epoch': 0.02}\n",
      "{'loss': 0.6833, 'grad_norm': 5.744655132293701, 'learning_rate': 5e-05, 'rewards/chosen': -0.7164534330368042, 'rewards/rejected': -0.7499223947525024, 'rewards/accuracies': 0.25, 'rewards/margins': 0.033468905836343765, 'logps/rejected': -88.32975769042969, 'logps/chosen': -80.8838882446289, 'logits/rejected': -2.817145824432373, 'logits/chosen': -2.726095199584961, 'epoch': 0.03}\n",
      "{'loss': 0.6314, 'grad_norm': 5.178174018859863, 'learning_rate': 5e-05, 'rewards/chosen': -0.9135171175003052, 'rewards/rejected': -1.1752077341079712, 'rewards/accuracies': 0.625, 'rewards/margins': 0.261690616607666, 'logps/rejected': -75.78726196289062, 'logps/chosen': -79.24082946777344, 'logits/rejected': -2.7552006244659424, 'logits/chosen': -2.4317245483398438, 'epoch': 0.04}\n",
      "{'loss': 0.619, 'grad_norm': 4.911908149719238, 'learning_rate': 5e-05, 'rewards/chosen': -0.9162457585334778, 'rewards/rejected': -1.2221463918685913, 'rewards/accuracies': 0.5, 'rewards/margins': 0.3059006631374359, 'logps/rejected': -75.64364624023438, 'logps/chosen': -70.80606079101562, 'logits/rejected': -2.939053773880005, 'logits/chosen': -2.5218653678894043, 'epoch': 0.04}\n",
      "{'loss': 0.5566, 'grad_norm': 9.069845199584961, 'learning_rate': 5e-05, 'rewards/chosen': -1.7623366117477417, 'rewards/rejected': -2.2623450756073, 'rewards/accuracies': 0.875, 'rewards/margins': 0.5000082850456238, 'logps/rejected': -101.34465026855469, 'logps/chosen': -89.97845458984375, 'logits/rejected': -2.9505558013916016, 'logits/chosen': -2.6599321365356445, 'epoch': 0.05}\n",
      "{'loss': 0.4792, 'grad_norm': 4.602269172668457, 'learning_rate': 5e-05, 'rewards/chosen': -1.397757649421692, 'rewards/rejected': -2.0492591857910156, 'rewards/accuracies': 0.875, 'rewards/margins': 0.6515015363693237, 'logps/rejected': -104.39753723144531, 'logps/chosen': -92.89286041259766, 'logits/rejected': -2.978074312210083, 'logits/chosen': -2.6096243858337402, 'epoch': 0.05}\n",
      "{'loss': 0.327, 'grad_norm': 3.5929293632507324, 'learning_rate': 5e-05, 'rewards/chosen': -1.089510202407837, 'rewards/rejected': -2.2799735069274902, 'rewards/accuracies': 1.0, 'rewards/margins': 1.1904630661010742, 'logps/rejected': -98.09518432617188, 'logps/chosen': -60.189910888671875, 'logits/rejected': -2.9517900943756104, 'logits/chosen': -2.0359046459198, 'epoch': 0.06}\n",
      "{'loss': 0.4318, 'grad_norm': 3.814915418624878, 'learning_rate': 5e-05, 'rewards/chosen': -2.0180482864379883, 'rewards/rejected': -2.7247297763824463, 'rewards/accuracies': 0.875, 'rewards/margins': 0.706681489944458, 'logps/rejected': -94.8321533203125, 'logps/chosen': -79.42340087890625, 'logits/rejected': -2.6353540420532227, 'logits/chosen': -2.4343175888061523, 'epoch': 0.07}\n",
      "{'loss': 0.5106, 'grad_norm': 4.773945331573486, 'learning_rate': 5e-05, 'rewards/chosen': -2.0372157096862793, 'rewards/rejected': -2.8059191703796387, 'rewards/accuracies': 0.625, 'rewards/margins': 0.7687035799026489, 'logps/rejected': -100.4085922241211, 'logps/chosen': -74.86798095703125, 'logits/rejected': -2.5032927989959717, 'logits/chosen': -2.0565764904022217, 'epoch': 0.07}\n",
      "{'loss': 0.763, 'grad_norm': 8.220871925354004, 'learning_rate': 5e-05, 'rewards/chosen': -3.1822242736816406, 'rewards/rejected': -3.467791795730591, 'rewards/accuracies': 0.5, 'rewards/margins': 0.2855674624443054, 'logps/rejected': -110.82389831542969, 'logps/chosen': -106.20203399658203, 'logits/rejected': -3.1273231506347656, 'logits/chosen': -2.923309564590454, 'epoch': 0.08}\n",
      "{'loss': 0.1913, 'grad_norm': 3.4719340801239014, 'learning_rate': 5e-05, 'rewards/chosen': -2.6489696502685547, 'rewards/rejected': -4.804222106933594, 'rewards/accuracies': 1.0, 'rewards/margins': 2.155252456665039, 'logps/rejected': -120.28166961669922, 'logps/chosen': -90.45932006835938, 'logits/rejected': -3.0838592052459717, 'logits/chosen': -2.8165743350982666, 'epoch': 0.08}\n",
      "{'loss': 0.2607, 'grad_norm': 4.667884349822998, 'learning_rate': 5e-05, 'rewards/chosen': -2.665111541748047, 'rewards/rejected': -5.379462242126465, 'rewards/accuracies': 0.875, 'rewards/margins': 2.714351177215576, 'logps/rejected': -139.8134307861328, 'logps/chosen': -78.47021484375, 'logits/rejected': -2.6065640449523926, 'logits/chosen': -2.0005273818969727, 'epoch': 0.09}\n",
      "{'loss': 0.3195, 'grad_norm': 4.4020609855651855, 'learning_rate': 5e-05, 'rewards/chosen': -4.462107181549072, 'rewards/rejected': -6.617636680603027, 'rewards/accuracies': 0.75, 'rewards/margins': 2.1555299758911133, 'logps/rejected': -146.23538208007812, 'logps/chosen': -116.81698608398438, 'logits/rejected': -3.034804344177246, 'logits/chosen': -2.73653507232666, 'epoch': 0.1}\n",
      "{'loss': 0.2504, 'grad_norm': 3.490856170654297, 'learning_rate': 5e-05, 'rewards/chosen': -4.62159538269043, 'rewards/rejected': -7.347806930541992, 'rewards/accuracies': 1.0, 'rewards/margins': 2.7262117862701416, 'logps/rejected': -144.07151794433594, 'logps/chosen': -107.74075317382812, 'logits/rejected': -3.1634912490844727, 'logits/chosen': -2.7825560569763184, 'epoch': 0.1}\n",
      "{'loss': 0.5688, 'grad_norm': 7.565128326416016, 'learning_rate': 5e-05, 'rewards/chosen': -5.266849040985107, 'rewards/rejected': -8.931611061096191, 'rewards/accuracies': 0.875, 'rewards/margins': 3.664762258529663, 'logps/rejected': -165.9716796875, 'logps/chosen': -109.19352722167969, 'logits/rejected': -2.8669517040252686, 'logits/chosen': -2.4345145225524902, 'epoch': 0.11}\n",
      "{'loss': 1.3508, 'grad_norm': 14.281598091125488, 'learning_rate': 5e-05, 'rewards/chosen': -5.54697847366333, 'rewards/rejected': -6.550018310546875, 'rewards/accuracies': 0.625, 'rewards/margins': 1.003040075302124, 'logps/rejected': -147.3758544921875, 'logps/chosen': -124.10862731933594, 'logits/rejected': -2.7890405654907227, 'logits/chosen': -2.5040321350097656, 'epoch': 0.12}\n",
      "{'loss': 0.1754, 'grad_norm': 2.4942939281463623, 'learning_rate': 5e-05, 'rewards/chosen': -3.809986114501953, 'rewards/rejected': -6.515473365783691, 'rewards/accuracies': 1.0, 'rewards/margins': 2.7054872512817383, 'logps/rejected': -134.51708984375, 'logps/chosen': -99.83198547363281, 'logits/rejected': -2.877366065979004, 'logits/chosen': -2.5195698738098145, 'epoch': 0.12}\n",
      "{'loss': 0.5922, 'grad_norm': 8.562644004821777, 'learning_rate': 5e-05, 'rewards/chosen': -5.011059284210205, 'rewards/rejected': -8.294532775878906, 'rewards/accuracies': 0.625, 'rewards/margins': 3.283473014831543, 'logps/rejected': -151.7600860595703, 'logps/chosen': -111.418701171875, 'logits/rejected': -2.9149718284606934, 'logits/chosen': -2.7535338401794434, 'epoch': 0.13}\n",
      "{'loss': 0.4187, 'grad_norm': 6.4440083503723145, 'learning_rate': 5e-05, 'rewards/chosen': -5.529730796813965, 'rewards/rejected': -7.54660701751709, 'rewards/accuracies': 0.625, 'rewards/margins': 2.016875743865967, 'logps/rejected': -140.23727416992188, 'logps/chosen': -118.91667175292969, 'logits/rejected': -2.6070432662963867, 'logits/chosen': -2.515979528427124, 'epoch': 0.13}\n",
      "{'loss': 0.5509, 'grad_norm': 6.179265975952148, 'learning_rate': 5e-05, 'rewards/chosen': -5.288275718688965, 'rewards/rejected': -6.300703048706055, 'rewards/accuracies': 0.625, 'rewards/margins': 1.0124272108078003, 'logps/rejected': -122.48820495605469, 'logps/chosen': -112.60305786132812, 'logits/rejected': -2.512117385864258, 'logits/chosen': -2.46268367767334, 'epoch': 0.14}\n",
      "{'loss': 0.965, 'grad_norm': 10.41500186920166, 'learning_rate': 5e-05, 'rewards/chosen': -5.436443328857422, 'rewards/rejected': -6.288511276245117, 'rewards/accuracies': 0.75, 'rewards/margins': 0.8520684242248535, 'logps/rejected': -122.20924377441406, 'logps/chosen': -110.74800872802734, 'logits/rejected': -2.6061060428619385, 'logits/chosen': -2.4236578941345215, 'epoch': 0.15}\n",
      "{'loss': 0.4977, 'grad_norm': 8.784908294677734, 'learning_rate': 5e-05, 'rewards/chosen': -5.148006439208984, 'rewards/rejected': -7.0742950439453125, 'rewards/accuracies': 0.75, 'rewards/margins': 1.9262878894805908, 'logps/rejected': -129.57118225097656, 'logps/chosen': -103.1967544555664, 'logits/rejected': -2.82930326461792, 'logits/chosen': -2.444911479949951, 'epoch': 0.15}\n",
      "{'loss': 0.7916, 'grad_norm': 12.379436492919922, 'learning_rate': 5e-05, 'rewards/chosen': -7.226245880126953, 'rewards/rejected': -9.731201171875, 'rewards/accuracies': 0.75, 'rewards/margins': 2.5049548149108887, 'logps/rejected': -161.69534301757812, 'logps/chosen': -140.42884826660156, 'logits/rejected': -2.5835790634155273, 'logits/chosen': -2.302976608276367, 'epoch': 0.16}\n",
      "{'loss': 0.5252, 'grad_norm': 9.190042495727539, 'learning_rate': 5e-05, 'rewards/chosen': -6.54005241394043, 'rewards/rejected': -9.823204040527344, 'rewards/accuracies': 0.75, 'rewards/margins': 3.283151865005493, 'logps/rejected': -170.13519287109375, 'logps/chosen': -133.37887573242188, 'logits/rejected': -2.841452121734619, 'logits/chosen': -2.4966988563537598, 'epoch': 0.16}\n",
      "{'loss': 0.8495, 'grad_norm': 8.782910346984863, 'learning_rate': 5e-05, 'rewards/chosen': -6.180018424987793, 'rewards/rejected': -8.83653450012207, 'rewards/accuracies': 0.625, 'rewards/margins': 2.6565163135528564, 'logps/rejected': -155.97650146484375, 'logps/chosen': -124.55023193359375, 'logits/rejected': -2.8074684143066406, 'logits/chosen': -2.4120311737060547, 'epoch': 0.17}\n",
      "{'loss': 0.4813, 'grad_norm': 7.194350242614746, 'learning_rate': 5e-05, 'rewards/chosen': -5.310359001159668, 'rewards/rejected': -7.503706455230713, 'rewards/accuracies': 0.75, 'rewards/margins': 2.1933469772338867, 'logps/rejected': -136.27224731445312, 'logps/chosen': -110.13829040527344, 'logits/rejected': -2.8711018562316895, 'logits/chosen': -2.532456398010254, 'epoch': 0.18}\n",
      "{'loss': 0.4243, 'grad_norm': 6.969100475311279, 'learning_rate': 5e-05, 'rewards/chosen': -6.5884246826171875, 'rewards/rejected': -8.939640045166016, 'rewards/accuracies': 0.875, 'rewards/margins': 2.351215124130249, 'logps/rejected': -166.6541290283203, 'logps/chosen': -135.6643524169922, 'logits/rejected': -2.5641109943389893, 'logits/chosen': -2.253270149230957, 'epoch': 0.18}\n",
      "{'loss': 1.0074, 'grad_norm': 9.789429664611816, 'learning_rate': 5e-05, 'rewards/chosen': -6.8941521644592285, 'rewards/rejected': -6.969696521759033, 'rewards/accuracies': 0.5, 'rewards/margins': 0.07554394006729126, 'logps/rejected': -141.94204711914062, 'logps/chosen': -125.31291198730469, 'logits/rejected': -2.9188578128814697, 'logits/chosen': -2.6787924766540527, 'epoch': 0.19}\n",
      "{'loss': 0.9149, 'grad_norm': 10.06775188446045, 'learning_rate': 5e-05, 'rewards/chosen': -7.238751411437988, 'rewards/rejected': -10.0717134475708, 'rewards/accuracies': 0.5, 'rewards/margins': 2.8329622745513916, 'logps/rejected': -172.1880340576172, 'logps/chosen': -151.77822875976562, 'logits/rejected': -2.6953577995300293, 'logits/chosen': -2.6258106231689453, 'epoch': 0.19}\n",
      "{'loss': 0.2865, 'grad_norm': 3.7312192916870117, 'learning_rate': 5e-05, 'rewards/chosen': -5.1419782638549805, 'rewards/rejected': -7.73922061920166, 'rewards/accuracies': 0.875, 'rewards/margins': 2.5972423553466797, 'logps/rejected': -146.3855438232422, 'logps/chosen': -106.69590759277344, 'logits/rejected': -3.04805850982666, 'logits/chosen': -2.4854211807250977, 'epoch': 0.2}\n",
      "{'loss': 0.5346, 'grad_norm': 6.89055871963501, 'learning_rate': 5e-05, 'rewards/chosen': -5.5991339683532715, 'rewards/rejected': -11.445632934570312, 'rewards/accuracies': 0.75, 'rewards/margins': 5.846498966217041, 'logps/rejected': -191.66339111328125, 'logps/chosen': -116.67744445800781, 'logits/rejected': -3.094191074371338, 'logits/chosen': -2.4629294872283936, 'epoch': 0.21}\n",
      "{'loss': 0.5023, 'grad_norm': 5.400197982788086, 'learning_rate': 5e-05, 'rewards/chosen': -4.4868316650390625, 'rewards/rejected': -8.90957260131836, 'rewards/accuracies': 0.875, 'rewards/margins': 4.422740459442139, 'logps/rejected': -163.3542938232422, 'logps/chosen': -110.16175842285156, 'logits/rejected': -2.8811075687408447, 'logits/chosen': -2.401292562484741, 'epoch': 0.21}\n",
      "{'loss': 0.6088, 'grad_norm': 5.673061847686768, 'learning_rate': 5e-05, 'rewards/chosen': -5.811330318450928, 'rewards/rejected': -6.5163421630859375, 'rewards/accuracies': 0.75, 'rewards/margins': 0.7050113677978516, 'logps/rejected': -121.09406280517578, 'logps/chosen': -114.33639526367188, 'logits/rejected': -2.9842915534973145, 'logits/chosen': -2.7850594520568848, 'epoch': 0.22}\n",
      "{'loss': 0.7418, 'grad_norm': 14.911849975585938, 'learning_rate': 5e-05, 'rewards/chosen': -7.921902179718018, 'rewards/rejected': -10.372701644897461, 'rewards/accuracies': 0.625, 'rewards/margins': 2.450800895690918, 'logps/rejected': -179.39117431640625, 'logps/chosen': -151.01441955566406, 'logits/rejected': -2.9662280082702637, 'logits/chosen': -2.6618568897247314, 'epoch': 0.22}\n",
      "{'loss': 0.1221, 'grad_norm': 1.7815256118774414, 'learning_rate': 5e-05, 'rewards/chosen': -5.859271049499512, 'rewards/rejected': -9.39398193359375, 'rewards/accuracies': 1.0, 'rewards/margins': 3.5347108840942383, 'logps/rejected': -169.84278869628906, 'logps/chosen': -131.7462615966797, 'logits/rejected': -2.9859836101531982, 'logits/chosen': -2.6332204341888428, 'epoch': 0.23}\n",
      "{'loss': 1.2685, 'grad_norm': 13.215359687805176, 'learning_rate': 5e-05, 'rewards/chosen': -12.26715087890625, 'rewards/rejected': -14.113038063049316, 'rewards/accuracies': 0.625, 'rewards/margins': 1.8458858728408813, 'logps/rejected': -233.04501342773438, 'logps/chosen': -216.33702087402344, 'logits/rejected': -3.054696798324585, 'logits/chosen': -2.7237746715545654, 'epoch': 0.24}\n",
      "{'loss': 0.6635, 'grad_norm': 4.70165491104126, 'learning_rate': 5e-05, 'rewards/chosen': -6.091518402099609, 'rewards/rejected': -9.170480728149414, 'rewards/accuracies': 0.875, 'rewards/margins': 3.078962564468384, 'logps/rejected': -167.85690307617188, 'logps/chosen': -119.239501953125, 'logits/rejected': -2.8498477935791016, 'logits/chosen': -2.4883954524993896, 'epoch': 0.24}\n",
      "{'loss': 0.4012, 'grad_norm': 6.761986255645752, 'learning_rate': 5e-05, 'rewards/chosen': -7.835361957550049, 'rewards/rejected': -10.559561729431152, 'rewards/accuracies': 0.625, 'rewards/margins': 2.7241997718811035, 'logps/rejected': -184.9964599609375, 'logps/chosen': -149.59378051757812, 'logits/rejected': -2.9647889137268066, 'logits/chosen': -2.6800944805145264, 'epoch': 0.25}\n",
      "{'loss': 0.3959, 'grad_norm': 6.189965724945068, 'learning_rate': 5e-05, 'rewards/chosen': -5.70065975189209, 'rewards/rejected': -8.953266143798828, 'rewards/accuracies': 0.75, 'rewards/margins': 3.2526063919067383, 'logps/rejected': -151.51588439941406, 'logps/chosen': -116.2624282836914, 'logits/rejected': -2.963870048522949, 'logits/chosen': -2.3872005939483643, 'epoch': 0.25}\n",
      "{'loss': 0.5498, 'grad_norm': 7.272400379180908, 'learning_rate': 5e-05, 'rewards/chosen': -8.3275728225708, 'rewards/rejected': -9.57773208618164, 'rewards/accuracies': 0.625, 'rewards/margins': 1.2501585483551025, 'logps/rejected': -179.07244873046875, 'logps/chosen': -160.6851806640625, 'logits/rejected': -3.0229153633117676, 'logits/chosen': -2.9090023040771484, 'epoch': 0.26}\n",
      "{'loss': 0.9468, 'grad_norm': 7.561083793640137, 'learning_rate': 5e-05, 'rewards/chosen': -6.437136173248291, 'rewards/rejected': -7.12939453125, 'rewards/accuracies': 0.625, 'rewards/margins': 0.6922582387924194, 'logps/rejected': -136.2489013671875, 'logps/chosen': -129.8406982421875, 'logits/rejected': -2.6644222736358643, 'logits/chosen': -2.4294321537017822, 'epoch': 0.27}\n",
      "{'loss': 1.0479, 'grad_norm': 9.081722259521484, 'learning_rate': 5e-05, 'rewards/chosen': -7.008357524871826, 'rewards/rejected': -7.662009239196777, 'rewards/accuracies': 0.375, 'rewards/margins': 0.6536516547203064, 'logps/rejected': -151.49874877929688, 'logps/chosen': -141.97402954101562, 'logits/rejected': -2.5871760845184326, 'logits/chosen': -2.387359380722046, 'epoch': 0.27}\n",
      "{'loss': 0.7944, 'grad_norm': 15.28150749206543, 'learning_rate': 5e-05, 'rewards/chosen': -7.898746967315674, 'rewards/rejected': -10.548552513122559, 'rewards/accuracies': 0.75, 'rewards/margins': 2.6498050689697266, 'logps/rejected': -197.11703491210938, 'logps/chosen': -160.41864013671875, 'logits/rejected': -3.0540854930877686, 'logits/chosen': -2.744326591491699, 'epoch': 0.28}\n",
      "{'loss': 0.2045, 'grad_norm': 2.2037508487701416, 'learning_rate': 5e-05, 'rewards/chosen': -4.530797004699707, 'rewards/rejected': -7.561971664428711, 'rewards/accuracies': 1.0, 'rewards/margins': 3.031174659729004, 'logps/rejected': -139.95494079589844, 'logps/chosen': -97.02613830566406, 'logits/rejected': -3.143000364303589, 'logits/chosen': -2.7957849502563477, 'epoch': 0.28}\n",
      "{'loss': 0.1082, 'grad_norm': 1.6296582221984863, 'learning_rate': 5e-05, 'rewards/chosen': -4.833505630493164, 'rewards/rejected': -9.84743595123291, 'rewards/accuracies': 1.0, 'rewards/margins': 5.013930797576904, 'logps/rejected': -195.68858337402344, 'logps/chosen': -118.33184814453125, 'logits/rejected': -3.175118923187256, 'logits/chosen': -2.482093572616577, 'epoch': 0.29}\n",
      "{'loss': 0.4218, 'grad_norm': 5.2656989097595215, 'learning_rate': 5e-05, 'rewards/chosen': -4.501236915588379, 'rewards/rejected': -7.5078349113464355, 'rewards/accuracies': 0.75, 'rewards/margins': 3.0065975189208984, 'logps/rejected': -157.00912475585938, 'logps/chosen': -104.85676574707031, 'logits/rejected': -3.070429801940918, 'logits/chosen': -2.5826756954193115, 'epoch': 0.3}\n",
      "{'loss': 0.7083, 'grad_norm': 7.449126720428467, 'learning_rate': 5e-05, 'rewards/chosen': -6.619283199310303, 'rewards/rejected': -8.70478343963623, 'rewards/accuracies': 0.5, 'rewards/margins': 2.085500955581665, 'logps/rejected': -162.59844970703125, 'logps/chosen': -147.66372680664062, 'logits/rejected': -2.617766857147217, 'logits/chosen': -2.3953163623809814, 'epoch': 0.3}\n",
      "{'loss': 0.5966, 'grad_norm': 7.9899139404296875, 'learning_rate': 5e-05, 'rewards/chosen': -6.919010162353516, 'rewards/rejected': -7.9401397705078125, 'rewards/accuracies': 0.75, 'rewards/margins': 1.021129846572876, 'logps/rejected': -147.44720458984375, 'logps/chosen': -143.01171875, 'logits/rejected': -2.3110499382019043, 'logits/chosen': -2.446866512298584, 'epoch': 0.31}\n",
      "{'loss': 0.3524, 'grad_norm': 2.3469369411468506, 'learning_rate': 5e-05, 'rewards/chosen': -4.472296714782715, 'rewards/rejected': -6.108305931091309, 'rewards/accuracies': 0.75, 'rewards/margins': 1.6360089778900146, 'logps/rejected': -120.53358459472656, 'logps/chosen': -101.89094543457031, 'logits/rejected': -2.9846136569976807, 'logits/chosen': -2.8384652137756348, 'epoch': 0.32}\n",
      "{'loss': 0.1917, 'grad_norm': 2.8822286128997803, 'learning_rate': 5e-05, 'rewards/chosen': -5.106678009033203, 'rewards/rejected': -7.4167070388793945, 'rewards/accuracies': 1.0, 'rewards/margins': 2.3100290298461914, 'logps/rejected': -143.93841552734375, 'logps/chosen': -105.95165252685547, 'logits/rejected': -2.9060590267181396, 'logits/chosen': -2.5755867958068848, 'epoch': 0.32}\n",
      "{'loss': 0.3017, 'grad_norm': 3.9736921787261963, 'learning_rate': 5e-05, 'rewards/chosen': -5.84140157699585, 'rewards/rejected': -10.114480972290039, 'rewards/accuracies': 0.75, 'rewards/margins': 4.273078918457031, 'logps/rejected': -186.73580932617188, 'logps/chosen': -130.048583984375, 'logits/rejected': -3.0112979412078857, 'logits/chosen': -2.9076108932495117, 'epoch': 0.33}\n",
      "{'loss': 0.3282, 'grad_norm': 3.3607747554779053, 'learning_rate': 5e-05, 'rewards/chosen': -5.544118881225586, 'rewards/rejected': -7.385275840759277, 'rewards/accuracies': 0.75, 'rewards/margins': 1.8411569595336914, 'logps/rejected': -138.47645568847656, 'logps/chosen': -113.13642883300781, 'logits/rejected': -2.708699941635132, 'logits/chosen': -2.497742176055908, 'epoch': 0.33}\n",
      "{'loss': 0.3109, 'grad_norm': 3.276928424835205, 'learning_rate': 5e-05, 'rewards/chosen': -2.598423957824707, 'rewards/rejected': -5.414120674133301, 'rewards/accuracies': 0.875, 'rewards/margins': 2.815697193145752, 'logps/rejected': -115.98297119140625, 'logps/chosen': -86.21273803710938, 'logits/rejected': -2.998117446899414, 'logits/chosen': -2.9344053268432617, 'epoch': 0.34}\n",
      "{'loss': 0.5118, 'grad_norm': 4.15488338470459, 'learning_rate': 5e-05, 'rewards/chosen': -5.302677631378174, 'rewards/rejected': -7.642411708831787, 'rewards/accuracies': 0.75, 'rewards/margins': 2.339733839035034, 'logps/rejected': -155.4111785888672, 'logps/chosen': -116.9572982788086, 'logits/rejected': -2.956737995147705, 'logits/chosen': -2.700050115585327, 'epoch': 0.35}\n",
      "{'loss': 0.2685, 'grad_norm': 3.2927911281585693, 'learning_rate': 5e-05, 'rewards/chosen': -6.152742385864258, 'rewards/rejected': -8.176775932312012, 'rewards/accuracies': 1.0, 'rewards/margins': 2.024033546447754, 'logps/rejected': -151.77980041503906, 'logps/chosen': -130.54847717285156, 'logits/rejected': -3.196596145629883, 'logits/chosen': -2.852047920227051, 'epoch': 0.35}\n",
      "{'loss': 0.1519, 'grad_norm': 2.4044361114501953, 'learning_rate': 5e-05, 'rewards/chosen': -5.6309003829956055, 'rewards/rejected': -10.494232177734375, 'rewards/accuracies': 1.0, 'rewards/margins': 4.863332748413086, 'logps/rejected': -199.13552856445312, 'logps/chosen': -123.5266342163086, 'logits/rejected': -2.825549840927124, 'logits/chosen': -2.423301935195923, 'epoch': 0.36}\n",
      "{'loss': 0.5619, 'grad_norm': 5.553656101226807, 'learning_rate': 5e-05, 'rewards/chosen': -5.176966190338135, 'rewards/rejected': -9.058504104614258, 'rewards/accuracies': 0.75, 'rewards/margins': 3.8815383911132812, 'logps/rejected': -170.92822265625, 'logps/chosen': -111.6988296508789, 'logits/rejected': -3.186516284942627, 'logits/chosen': -2.603668689727783, 'epoch': 0.36}\n",
      "{'loss': 0.1906, 'grad_norm': 3.017324447631836, 'learning_rate': 5e-05, 'rewards/chosen': -6.926374435424805, 'rewards/rejected': -9.944762229919434, 'rewards/accuracies': 0.875, 'rewards/margins': 3.018388271331787, 'logps/rejected': -179.39743041992188, 'logps/chosen': -133.65542602539062, 'logits/rejected': -3.0600857734680176, 'logits/chosen': -2.685054063796997, 'epoch': 0.37}\n",
      "{'loss': 0.4594, 'grad_norm': 6.657885551452637, 'learning_rate': 5e-05, 'rewards/chosen': -4.618402481079102, 'rewards/rejected': -8.647729873657227, 'rewards/accuracies': 0.875, 'rewards/margins': 4.029327392578125, 'logps/rejected': -158.08189392089844, 'logps/chosen': -106.37446594238281, 'logits/rejected': -2.858999252319336, 'logits/chosen': -2.4115707874298096, 'epoch': 0.38}\n",
      "{'loss': 0.2554, 'grad_norm': 4.318819999694824, 'learning_rate': 5e-05, 'rewards/chosen': -6.896549224853516, 'rewards/rejected': -11.907642364501953, 'rewards/accuracies': 0.875, 'rewards/margins': 5.011093616485596, 'logps/rejected': -206.88528442382812, 'logps/chosen': -148.20828247070312, 'logits/rejected': -2.8808770179748535, 'logits/chosen': -2.524747133255005, 'epoch': 0.38}\n",
      "{'loss': 0.1268, 'grad_norm': 1.0226274728775024, 'learning_rate': 5e-05, 'rewards/chosen': -4.3157758712768555, 'rewards/rejected': -7.302644729614258, 'rewards/accuracies': 0.875, 'rewards/margins': 2.986868381500244, 'logps/rejected': -146.2355194091797, 'logps/chosen': -105.14239501953125, 'logits/rejected': -2.774507522583008, 'logits/chosen': -2.416527509689331, 'epoch': 0.39}\n",
      "{'loss': 0.2806, 'grad_norm': 3.3367135524749756, 'learning_rate': 5e-05, 'rewards/chosen': -4.452334403991699, 'rewards/rejected': -8.302282333374023, 'rewards/accuracies': 0.75, 'rewards/margins': 3.849947452545166, 'logps/rejected': -167.41651916503906, 'logps/chosen': -110.93165588378906, 'logits/rejected': -3.1635332107543945, 'logits/chosen': -2.9573707580566406, 'epoch': 0.39}\n",
      "{'loss': 0.2148, 'grad_norm': 4.153518199920654, 'learning_rate': 5e-05, 'rewards/chosen': -5.982933044433594, 'rewards/rejected': -12.451301574707031, 'rewards/accuracies': 0.875, 'rewards/margins': 6.468368053436279, 'logps/rejected': -216.35670471191406, 'logps/chosen': -119.36211395263672, 'logits/rejected': -3.254986047744751, 'logits/chosen': -2.4663805961608887, 'epoch': 0.4}\n",
      "{'loss': 0.0607, 'grad_norm': 0.9957414269447327, 'learning_rate': 5e-05, 'rewards/chosen': -6.999597549438477, 'rewards/rejected': -11.35735034942627, 'rewards/accuracies': 1.0, 'rewards/margins': 4.357752799987793, 'logps/rejected': -191.6348876953125, 'logps/chosen': -127.61137390136719, 'logits/rejected': -2.990997076034546, 'logits/chosen': -2.593287944793701, 'epoch': 0.41}\n",
      "{'loss': 0.216, 'grad_norm': 2.3668477535247803, 'learning_rate': 5e-05, 'rewards/chosen': -6.365504264831543, 'rewards/rejected': -10.008914947509766, 'rewards/accuracies': 0.875, 'rewards/margins': 3.643411159515381, 'logps/rejected': -163.5583953857422, 'logps/chosen': -122.92189025878906, 'logits/rejected': -3.2613301277160645, 'logits/chosen': -2.75069522857666, 'epoch': 0.41}\n",
      "{'loss': 1.4794, 'grad_norm': 9.063566207885742, 'learning_rate': 5e-05, 'rewards/chosen': -5.682624816894531, 'rewards/rejected': -7.911573886871338, 'rewards/accuracies': 0.875, 'rewards/margins': 2.2289488315582275, 'logps/rejected': -140.00685119628906, 'logps/chosen': -123.90092468261719, 'logits/rejected': -2.800774097442627, 'logits/chosen': -2.8231000900268555, 'epoch': 0.42}\n",
      "{'loss': 0.2415, 'grad_norm': 2.0132124423980713, 'learning_rate': 5e-05, 'rewards/chosen': -7.085474967956543, 'rewards/rejected': -10.121278762817383, 'rewards/accuracies': 1.0, 'rewards/margins': 3.03580379486084, 'logps/rejected': -165.9154510498047, 'logps/chosen': -126.01699829101562, 'logits/rejected': -2.901546001434326, 'logits/chosen': -2.5841219425201416, 'epoch': 0.42}\n",
      "{'loss': 0.3822, 'grad_norm': 4.679472923278809, 'learning_rate': 5e-05, 'rewards/chosen': -6.090185165405273, 'rewards/rejected': -9.125771522521973, 'rewards/accuracies': 0.875, 'rewards/margins': 3.035585880279541, 'logps/rejected': -154.40988159179688, 'logps/chosen': -129.55386352539062, 'logits/rejected': -3.1729884147644043, 'logits/chosen': -2.8269851207733154, 'epoch': 0.43}\n",
      "{'loss': 0.2969, 'grad_norm': 3.4827375411987305, 'learning_rate': 5e-05, 'rewards/chosen': -6.835831642150879, 'rewards/rejected': -10.274179458618164, 'rewards/accuracies': 0.875, 'rewards/margins': 3.4383466243743896, 'logps/rejected': -174.68661499023438, 'logps/chosen': -125.37388610839844, 'logits/rejected': -3.1285009384155273, 'logits/chosen': -2.8219923973083496, 'epoch': 0.44}\n",
      "{'loss': 0.2559, 'grad_norm': 4.751662731170654, 'learning_rate': 5e-05, 'rewards/chosen': -5.503273010253906, 'rewards/rejected': -10.192852020263672, 'rewards/accuracies': 0.875, 'rewards/margins': 4.689578056335449, 'logps/rejected': -171.07229614257812, 'logps/chosen': -113.86246490478516, 'logits/rejected': -3.0829405784606934, 'logits/chosen': -2.4534366130828857, 'epoch': 0.44}\n",
      "{'loss': 0.1636, 'grad_norm': 4.81428861618042, 'learning_rate': 5e-05, 'rewards/chosen': -7.584419250488281, 'rewards/rejected': -12.214446067810059, 'rewards/accuracies': 1.0, 'rewards/margins': 4.630025863647461, 'logps/rejected': -199.6660614013672, 'logps/chosen': -152.40223693847656, 'logits/rejected': -2.930044651031494, 'logits/chosen': -2.686237335205078, 'epoch': 0.45}\n",
      "{'loss': 0.713, 'grad_norm': 9.186105728149414, 'learning_rate': 5e-05, 'rewards/chosen': -7.499142169952393, 'rewards/rejected': -9.573204040527344, 'rewards/accuracies': 0.75, 'rewards/margins': 2.0740623474121094, 'logps/rejected': -165.16497802734375, 'logps/chosen': -135.36752319335938, 'logits/rejected': -2.8815741539001465, 'logits/chosen': -2.5979597568511963, 'epoch': 0.45}\n",
      "{'loss': 0.607, 'grad_norm': 5.115172863006592, 'learning_rate': 5e-05, 'rewards/chosen': -6.615602493286133, 'rewards/rejected': -9.267004013061523, 'rewards/accuracies': 0.75, 'rewards/margins': 2.6514015197753906, 'logps/rejected': -167.95065307617188, 'logps/chosen': -124.59161376953125, 'logits/rejected': -3.1383676528930664, 'logits/chosen': -2.509538412094116, 'epoch': 0.46}\n",
      "{'loss': 1.3919, 'grad_norm': 14.33342170715332, 'learning_rate': 5e-05, 'rewards/chosen': -10.153093338012695, 'rewards/rejected': -10.881048202514648, 'rewards/accuracies': 0.25, 'rewards/margins': 0.7279554009437561, 'logps/rejected': -171.705078125, 'logps/chosen': -160.245849609375, 'logits/rejected': -2.9958457946777344, 'logits/chosen': -2.662508249282837, 'epoch': 0.47}\n",
      "{'loss': 0.4988, 'grad_norm': 5.446096897125244, 'learning_rate': 5e-05, 'rewards/chosen': -7.34898567199707, 'rewards/rejected': -9.495168685913086, 'rewards/accuracies': 0.75, 'rewards/margins': 2.1461822986602783, 'logps/rejected': -165.65464782714844, 'logps/chosen': -135.73178100585938, 'logits/rejected': -2.964632034301758, 'logits/chosen': -2.6409761905670166, 'epoch': 0.47}\n",
      "{'loss': 1.0775, 'grad_norm': 8.186660766601562, 'learning_rate': 5e-05, 'rewards/chosen': -9.17341423034668, 'rewards/rejected': -12.287168502807617, 'rewards/accuracies': 0.875, 'rewards/margins': 3.1137537956237793, 'logps/rejected': -202.53834533691406, 'logps/chosen': -164.09048461914062, 'logits/rejected': -3.192798137664795, 'logits/chosen': -2.6014344692230225, 'epoch': 0.48}\n",
      "{'loss': 0.0471, 'grad_norm': 0.9611560106277466, 'learning_rate': 5e-05, 'rewards/chosen': -4.882659435272217, 'rewards/rejected': -9.554697036743164, 'rewards/accuracies': 1.0, 'rewards/margins': 4.672036170959473, 'logps/rejected': -166.94158935546875, 'logps/chosen': -109.76434326171875, 'logits/rejected': -2.9761404991149902, 'logits/chosen': -2.5631041526794434, 'epoch': 0.48}\n",
      "{'loss': 0.2625, 'grad_norm': 3.409926414489746, 'learning_rate': 5e-05, 'rewards/chosen': -6.395245552062988, 'rewards/rejected': -8.782548904418945, 'rewards/accuracies': 0.875, 'rewards/margins': 2.3873038291931152, 'logps/rejected': -172.85629272460938, 'logps/chosen': -128.25341796875, 'logits/rejected': -3.148137092590332, 'logits/chosen': -2.521206855773926, 'epoch': 0.49}\n",
      "{'loss': 0.0873, 'grad_norm': 1.6655910015106201, 'learning_rate': 5e-05, 'rewards/chosen': -6.915851593017578, 'rewards/rejected': -10.55037784576416, 'rewards/accuracies': 1.0, 'rewards/margins': 3.634526014328003, 'logps/rejected': -176.84954833984375, 'logps/chosen': -129.9864959716797, 'logits/rejected': -3.2403597831726074, 'logits/chosen': -2.847454786300659, 'epoch': 0.5}\n",
      "{'loss': 0.556, 'grad_norm': 4.128019332885742, 'learning_rate': 5e-05, 'rewards/chosen': -6.723089218139648, 'rewards/rejected': -7.518762111663818, 'rewards/accuracies': 0.75, 'rewards/margins': 0.7956722974777222, 'logps/rejected': -139.99525451660156, 'logps/chosen': -127.8644790649414, 'logits/rejected': -2.7786247730255127, 'logits/chosen': -2.7096357345581055, 'epoch': 0.5}\n",
      "{'loss': 0.3957, 'grad_norm': 4.499178409576416, 'learning_rate': 5e-05, 'rewards/chosen': -4.2880425453186035, 'rewards/rejected': -9.376843452453613, 'rewards/accuracies': 0.875, 'rewards/margins': 5.088801860809326, 'logps/rejected': -171.09132385253906, 'logps/chosen': -107.68946838378906, 'logits/rejected': -2.843968629837036, 'logits/chosen': -2.2485523223876953, 'epoch': 0.51}\n",
      "{'loss': 1.3902, 'grad_norm': 9.847135543823242, 'learning_rate': 5e-05, 'rewards/chosen': -8.444174766540527, 'rewards/rejected': -7.788225173950195, 'rewards/accuracies': 0.5, 'rewards/margins': -0.6559491157531738, 'logps/rejected': -143.9735565185547, 'logps/chosen': -139.22836303710938, 'logits/rejected': -3.0016822814941406, 'logits/chosen': -2.744436264038086, 'epoch': 0.52}\n",
      "{'loss': 0.2948, 'grad_norm': 4.434615612030029, 'learning_rate': 5e-05, 'rewards/chosen': -6.314733982086182, 'rewards/rejected': -8.407586097717285, 'rewards/accuracies': 0.875, 'rewards/margins': 2.0928518772125244, 'logps/rejected': -138.82687377929688, 'logps/chosen': -129.9069061279297, 'logits/rejected': -2.5534627437591553, 'logits/chosen': -2.643724203109741, 'epoch': 0.52}\n",
      "{'loss': 0.2631, 'grad_norm': 3.358877658843994, 'learning_rate': 5e-05, 'rewards/chosen': -6.171545505523682, 'rewards/rejected': -10.310201644897461, 'rewards/accuracies': 0.75, 'rewards/margins': 4.1386566162109375, 'logps/rejected': -181.7921600341797, 'logps/chosen': -115.84612274169922, 'logits/rejected': -3.082343339920044, 'logits/chosen': -2.3767290115356445, 'epoch': 0.53}\n",
      "{'loss': 0.4111, 'grad_norm': 5.485206604003906, 'learning_rate': 5e-05, 'rewards/chosen': -8.504463195800781, 'rewards/rejected': -11.675149917602539, 'rewards/accuracies': 0.625, 'rewards/margins': 3.170687437057495, 'logps/rejected': -187.6425018310547, 'logps/chosen': -154.17471313476562, 'logits/rejected': -3.1021111011505127, 'logits/chosen': -2.738950252532959, 'epoch': 0.53}\n",
      "{'loss': 0.5872, 'grad_norm': 5.99038553237915, 'learning_rate': 5e-05, 'rewards/chosen': -9.052549362182617, 'rewards/rejected': -13.288223266601562, 'rewards/accuracies': 0.875, 'rewards/margins': 4.235673427581787, 'logps/rejected': -216.74978637695312, 'logps/chosen': -161.79885864257812, 'logits/rejected': -2.906155824661255, 'logits/chosen': -2.656245470046997, 'epoch': 0.54}\n",
      "{'loss': 0.0987, 'grad_norm': 1.9439661502838135, 'learning_rate': 5e-05, 'rewards/chosen': -7.763387203216553, 'rewards/rejected': -16.04882049560547, 'rewards/accuracies': 0.875, 'rewards/margins': 8.285435676574707, 'logps/rejected': -268.3193664550781, 'logps/chosen': -135.18295288085938, 'logits/rejected': -3.095017194747925, 'logits/chosen': -2.4104995727539062, 'epoch': 0.55}\n",
      "{'loss': 0.3062, 'grad_norm': 2.82125186920166, 'learning_rate': 5e-05, 'rewards/chosen': -8.283945083618164, 'rewards/rejected': -11.61496353149414, 'rewards/accuracies': 0.75, 'rewards/margins': 3.3310179710388184, 'logps/rejected': -182.565673828125, 'logps/chosen': -137.30812072753906, 'logits/rejected': -3.1021974086761475, 'logits/chosen': -2.7537591457366943, 'epoch': 0.55}\n",
      "{'loss': 1.5486, 'grad_norm': 14.646079063415527, 'learning_rate': 5e-05, 'rewards/chosen': -7.546525001525879, 'rewards/rejected': -9.407659530639648, 'rewards/accuracies': 0.625, 'rewards/margins': 1.861134648323059, 'logps/rejected': -166.8574676513672, 'logps/chosen': -142.4439697265625, 'logits/rejected': -3.003371238708496, 'logits/chosen': -2.7913689613342285, 'epoch': 0.56}\n",
      "{'loss': 0.5559, 'grad_norm': 4.079158782958984, 'learning_rate': 5e-05, 'rewards/chosen': -4.567357063293457, 'rewards/rejected': -7.221887111663818, 'rewards/accuracies': 0.625, 'rewards/margins': 2.654529571533203, 'logps/rejected': -128.20025634765625, 'logps/chosen': -94.59127807617188, 'logits/rejected': -2.666977643966675, 'logits/chosen': -2.4745025634765625, 'epoch': 0.56}\n",
      "{'loss': 0.9132, 'grad_norm': 18.04939842224121, 'learning_rate': 5e-05, 'rewards/chosen': -6.579941272735596, 'rewards/rejected': -10.000718116760254, 'rewards/accuracies': 0.75, 'rewards/margins': 3.420776128768921, 'logps/rejected': -187.04696655273438, 'logps/chosen': -125.86782836914062, 'logits/rejected': -3.1004490852355957, 'logits/chosen': -2.397189140319824, 'epoch': 0.57}\n",
      "{'loss': 0.5031, 'grad_norm': 7.979042053222656, 'learning_rate': 5e-05, 'rewards/chosen': -6.49030876159668, 'rewards/rejected': -8.66709041595459, 'rewards/accuracies': 0.75, 'rewards/margins': 2.1767818927764893, 'logps/rejected': -152.9096221923828, 'logps/chosen': -126.46443939208984, 'logits/rejected': -2.8783669471740723, 'logits/chosen': -2.6148345470428467, 'epoch': 0.58}\n",
      "{'loss': 0.8827, 'grad_norm': 9.644489288330078, 'learning_rate': 5e-05, 'rewards/chosen': -7.1263837814331055, 'rewards/rejected': -9.052261352539062, 'rewards/accuracies': 0.75, 'rewards/margins': 1.9258779287338257, 'logps/rejected': -171.8954620361328, 'logps/chosen': -152.4293975830078, 'logits/rejected': -2.946531295776367, 'logits/chosen': -2.807748317718506, 'epoch': 0.58}\n",
      "{'loss': 0.4009, 'grad_norm': 3.4681689739227295, 'learning_rate': 5e-05, 'rewards/chosen': -5.746746063232422, 'rewards/rejected': -7.126264572143555, 'rewards/accuracies': 0.75, 'rewards/margins': 1.3795185089111328, 'logps/rejected': -133.02545166015625, 'logps/chosen': -122.29710388183594, 'logits/rejected': -2.980997085571289, 'logits/chosen': -2.7420525550842285, 'epoch': 0.59}\n",
      "{'loss': 0.4361, 'grad_norm': 3.6317379474639893, 'learning_rate': 5e-05, 'rewards/chosen': -3.842477798461914, 'rewards/rejected': -5.162485122680664, 'rewards/accuracies': 0.75, 'rewards/margins': 1.3200078010559082, 'logps/rejected': -127.90252685546875, 'logps/chosen': -104.10958862304688, 'logits/rejected': -2.5803420543670654, 'logits/chosen': -2.368274211883545, 'epoch': 0.59}\n",
      "{'loss': 0.5444, 'grad_norm': 3.9785890579223633, 'learning_rate': 5e-05, 'rewards/chosen': -4.483721733093262, 'rewards/rejected': -6.49058723449707, 'rewards/accuracies': 0.875, 'rewards/margins': 2.0068655014038086, 'logps/rejected': -142.0125732421875, 'logps/chosen': -102.61173248291016, 'logits/rejected': -2.962761640548706, 'logits/chosen': -2.790895700454712, 'epoch': 0.6}\n",
      "{'loss': 0.1878, 'grad_norm': 3.7036757469177246, 'learning_rate': 5e-05, 'rewards/chosen': -4.7027788162231445, 'rewards/rejected': -7.676112651824951, 'rewards/accuracies': 0.875, 'rewards/margins': 2.9733338356018066, 'logps/rejected': -163.6650390625, 'logps/chosen': -109.46749877929688, 'logits/rejected': -3.129603862762451, 'logits/chosen': -2.5914270877838135, 'epoch': 0.61}\n",
      "{'loss': 0.599, 'grad_norm': 5.98953104019165, 'learning_rate': 5e-05, 'rewards/chosen': -5.8643388748168945, 'rewards/rejected': -7.020290851593018, 'rewards/accuracies': 0.5, 'rewards/margins': 1.1559529304504395, 'logps/rejected': -159.0821990966797, 'logps/chosen': -132.90342712402344, 'logits/rejected': -2.916348457336426, 'logits/chosen': -2.7811341285705566, 'epoch': 0.61}\n",
      "{'loss': 0.3446, 'grad_norm': 1.994760513305664, 'learning_rate': 5e-05, 'rewards/chosen': -4.35922384262085, 'rewards/rejected': -7.558616638183594, 'rewards/accuracies': 0.75, 'rewards/margins': 3.199392795562744, 'logps/rejected': -143.4265594482422, 'logps/chosen': -96.84024810791016, 'logits/rejected': -2.8622934818267822, 'logits/chosen': -2.326592445373535, 'epoch': 0.62}\n",
      "{'loss': 0.0681, 'grad_norm': 0.6747170686721802, 'learning_rate': 5e-05, 'rewards/chosen': -3.9893646240234375, 'rewards/rejected': -7.551807403564453, 'rewards/accuracies': 1.0, 'rewards/margins': 3.5624427795410156, 'logps/rejected': -155.27835083007812, 'logps/chosen': -100.8070068359375, 'logits/rejected': -3.2028889656066895, 'logits/chosen': -2.7759509086608887, 'epoch': 0.62}\n",
      "{'loss': 0.3611, 'grad_norm': 3.7799689769744873, 'learning_rate': 5e-05, 'rewards/chosen': -4.376346111297607, 'rewards/rejected': -6.269284725189209, 'rewards/accuracies': 0.875, 'rewards/margins': 1.8929381370544434, 'logps/rejected': -122.4670639038086, 'logps/chosen': -100.42715454101562, 'logits/rejected': -2.7914299964904785, 'logits/chosen': -2.8143088817596436, 'epoch': 0.63}\n",
      "{'loss': 0.9532, 'grad_norm': 8.5698881149292, 'learning_rate': 5e-05, 'rewards/chosen': -6.122151851654053, 'rewards/rejected': -8.491292953491211, 'rewards/accuracies': 0.5, 'rewards/margins': 2.369140386581421, 'logps/rejected': -176.35916137695312, 'logps/chosen': -142.99893188476562, 'logits/rejected': -3.048616647720337, 'logits/chosen': -2.818854808807373, 'epoch': 0.64}\n",
      "{'loss': 1.224, 'grad_norm': 7.893852233886719, 'learning_rate': 5e-05, 'rewards/chosen': -7.171537399291992, 'rewards/rejected': -7.7776947021484375, 'rewards/accuracies': 0.625, 'rewards/margins': 0.6061569452285767, 'logps/rejected': -149.0540313720703, 'logps/chosen': -134.7442626953125, 'logits/rejected': -2.8280375003814697, 'logits/chosen': -2.6010236740112305, 'epoch': 0.64}\n",
      "{'loss': 0.2151, 'grad_norm': 2.5440797805786133, 'learning_rate': 5e-05, 'rewards/chosen': -4.749321937561035, 'rewards/rejected': -8.80739974975586, 'rewards/accuracies': 1.0, 'rewards/margins': 4.058078289031982, 'logps/rejected': -171.9795379638672, 'logps/chosen': -120.2693862915039, 'logits/rejected': -3.0774636268615723, 'logits/chosen': -2.6903295516967773, 'epoch': 0.65}\n",
      "{'loss': 0.4939, 'grad_norm': 4.019665718078613, 'learning_rate': 5e-05, 'rewards/chosen': -5.179903507232666, 'rewards/rejected': -8.797633171081543, 'rewards/accuracies': 0.75, 'rewards/margins': 3.617729663848877, 'logps/rejected': -151.76878356933594, 'logps/chosen': -106.31268310546875, 'logits/rejected': -2.988203763961792, 'logits/chosen': -2.4798760414123535, 'epoch': 0.65}\n",
      "{'loss': 0.8128, 'grad_norm': 6.823897361755371, 'learning_rate': 5e-05, 'rewards/chosen': -5.917236328125, 'rewards/rejected': -7.670560836791992, 'rewards/accuracies': 0.75, 'rewards/margins': 1.7533248662948608, 'logps/rejected': -164.31546020507812, 'logps/chosen': -134.8976593017578, 'logits/rejected': -3.0797972679138184, 'logits/chosen': -2.8059170246124268, 'epoch': 0.66}\n",
      "{'loss': 0.7068, 'grad_norm': 4.453166961669922, 'learning_rate': 5e-05, 'rewards/chosen': -5.4874444007873535, 'rewards/rejected': -6.479259014129639, 'rewards/accuracies': 0.75, 'rewards/margins': 0.991814374923706, 'logps/rejected': -135.2191162109375, 'logps/chosen': -119.52412414550781, 'logits/rejected': -3.0811045169830322, 'logits/chosen': -2.842392683029175, 'epoch': 0.67}\n",
      "{'loss': 0.2642, 'grad_norm': 4.641201972961426, 'learning_rate': 5e-05, 'rewards/chosen': -5.729490280151367, 'rewards/rejected': -8.585930824279785, 'rewards/accuracies': 0.875, 'rewards/margins': 2.8564398288726807, 'logps/rejected': -164.43800354003906, 'logps/chosen': -109.40037536621094, 'logits/rejected': -2.89355731010437, 'logits/chosen': -2.3246381282806396, 'epoch': 0.67}\n",
      "{'loss': 0.4068, 'grad_norm': 5.826000690460205, 'learning_rate': 5e-05, 'rewards/chosen': -6.216002464294434, 'rewards/rejected': -7.6732177734375, 'rewards/accuracies': 0.875, 'rewards/margins': 1.4572148323059082, 'logps/rejected': -154.60157775878906, 'logps/chosen': -127.7154541015625, 'logits/rejected': -2.8922910690307617, 'logits/chosen': -2.6031997203826904, 'epoch': 0.68}\n",
      "{'loss': 0.1885, 'grad_norm': 3.5602219104766846, 'learning_rate': 5e-05, 'rewards/chosen': -6.950170993804932, 'rewards/rejected': -10.116726875305176, 'rewards/accuracies': 0.875, 'rewards/margins': 3.1665561199188232, 'logps/rejected': -176.46710205078125, 'logps/chosen': -123.2624282836914, 'logits/rejected': -3.000401020050049, 'logits/chosen': -2.585236072540283, 'epoch': 0.68}\n",
      "{'loss': 1.0432, 'grad_norm': 5.327934741973877, 'learning_rate': 5e-05, 'rewards/chosen': -5.238482475280762, 'rewards/rejected': -6.375120639801025, 'rewards/accuracies': 0.625, 'rewards/margins': 1.1366381645202637, 'logps/rejected': -124.37043762207031, 'logps/chosen': -113.42932891845703, 'logits/rejected': -3.0530054569244385, 'logits/chosen': -3.079882860183716, 'epoch': 0.69}\n",
      "{'loss': 0.5145, 'grad_norm': 4.159125328063965, 'learning_rate': 5e-05, 'rewards/chosen': -3.5385942459106445, 'rewards/rejected': -5.036971569061279, 'rewards/accuracies': 0.75, 'rewards/margins': 1.4983776807785034, 'logps/rejected': -105.87692260742188, 'logps/chosen': -98.42097473144531, 'logits/rejected': -2.706083297729492, 'logits/chosen': -2.674978733062744, 'epoch': 0.7}\n",
      "{'loss': 0.2048, 'grad_norm': 1.6836638450622559, 'learning_rate': 5e-05, 'rewards/chosen': -5.4730329513549805, 'rewards/rejected': -9.866296768188477, 'rewards/accuracies': 0.875, 'rewards/margins': 4.393264293670654, 'logps/rejected': -182.04254150390625, 'logps/chosen': -124.03170776367188, 'logits/rejected': -3.0802829265594482, 'logits/chosen': -2.8288121223449707, 'epoch': 0.7}\n",
      "{'loss': 0.2241, 'grad_norm': 2.857558012008667, 'learning_rate': 5e-05, 'rewards/chosen': -5.859481334686279, 'rewards/rejected': -8.623064041137695, 'rewards/accuracies': 0.875, 'rewards/margins': 2.7635836601257324, 'logps/rejected': -162.20870971679688, 'logps/chosen': -128.45016479492188, 'logits/rejected': -3.0649311542510986, 'logits/chosen': -2.72324800491333, 'epoch': 0.71}\n",
      "{'loss': 0.26, 'grad_norm': 1.9698841571807861, 'learning_rate': 5e-05, 'rewards/chosen': -4.635370254516602, 'rewards/rejected': -6.5157012939453125, 'rewards/accuracies': 0.875, 'rewards/margins': 1.880331039428711, 'logps/rejected': -136.579833984375, 'logps/chosen': -99.78176879882812, 'logits/rejected': -2.928485870361328, 'logits/chosen': -3.0362257957458496, 'epoch': 0.72}\n",
      "{'loss': 0.2218, 'grad_norm': 1.7710341215133667, 'learning_rate': 5e-05, 'rewards/chosen': -4.468191623687744, 'rewards/rejected': -7.396290302276611, 'rewards/accuracies': 0.875, 'rewards/margins': 2.928098678588867, 'logps/rejected': -150.93209838867188, 'logps/chosen': -113.57151794433594, 'logits/rejected': -3.050299644470215, 'logits/chosen': -2.723402500152588, 'epoch': 0.72}\n",
      "{'loss': 0.3436, 'grad_norm': 3.0253522396087646, 'learning_rate': 5e-05, 'rewards/chosen': -6.001706600189209, 'rewards/rejected': -7.893802642822266, 'rewards/accuracies': 0.875, 'rewards/margins': 1.8920960426330566, 'logps/rejected': -145.81814575195312, 'logps/chosen': -130.69613647460938, 'logits/rejected': -2.963897466659546, 'logits/chosen': -2.8378114700317383, 'epoch': 0.73}\n",
      "{'loss': 0.3118, 'grad_norm': 3.0396595001220703, 'learning_rate': 5e-05, 'rewards/chosen': -4.927054405212402, 'rewards/rejected': -7.690804958343506, 'rewards/accuracies': 0.875, 'rewards/margins': 2.7637503147125244, 'logps/rejected': -136.88577270507812, 'logps/chosen': -102.55022430419922, 'logits/rejected': -2.9371910095214844, 'logits/chosen': -2.5864405632019043, 'epoch': 0.73}\n",
      "{'loss': 0.1227, 'grad_norm': 1.215921401977539, 'learning_rate': 5e-05, 'rewards/chosen': -4.932708740234375, 'rewards/rejected': -7.847411632537842, 'rewards/accuracies': 1.0, 'rewards/margins': 2.914702892303467, 'logps/rejected': -158.2307586669922, 'logps/chosen': -110.54147338867188, 'logits/rejected': -3.1933770179748535, 'logits/chosen': -2.813216209411621, 'epoch': 0.74}\n",
      "{'loss': 0.162, 'grad_norm': 2.146409511566162, 'learning_rate': 5e-05, 'rewards/chosen': -4.750860691070557, 'rewards/rejected': -8.142187118530273, 'rewards/accuracies': 0.875, 'rewards/margins': 3.391327142715454, 'logps/rejected': -145.9727783203125, 'logps/chosen': -94.0007095336914, 'logits/rejected': -2.8387928009033203, 'logits/chosen': -2.4450762271881104, 'epoch': 0.75}\n",
      "{'loss': 0.4512, 'grad_norm': 3.7253470420837402, 'learning_rate': 5e-05, 'rewards/chosen': -5.062144756317139, 'rewards/rejected': -7.398921012878418, 'rewards/accuracies': 0.75, 'rewards/margins': 2.3367760181427, 'logps/rejected': -141.68482971191406, 'logps/chosen': -103.84182739257812, 'logits/rejected': -2.936208963394165, 'logits/chosen': -2.6605489253997803, 'epoch': 0.75}\n",
      "{'loss': 0.5047, 'grad_norm': 4.378828525543213, 'learning_rate': 5e-05, 'rewards/chosen': -4.371239185333252, 'rewards/rejected': -6.26031494140625, 'rewards/accuracies': 0.875, 'rewards/margins': 1.889075756072998, 'logps/rejected': -143.0916290283203, 'logps/chosen': -109.50286865234375, 'logits/rejected': -2.834615468978882, 'logits/chosen': -2.686004638671875, 'epoch': 0.76}\n",
      "{'loss': 0.4275, 'grad_norm': 2.593106985092163, 'learning_rate': 5e-05, 'rewards/chosen': -4.314787864685059, 'rewards/rejected': -6.5967278480529785, 'rewards/accuracies': 0.625, 'rewards/margins': 2.281940221786499, 'logps/rejected': -132.6118927001953, 'logps/chosen': -100.38523864746094, 'logits/rejected': -3.198228359222412, 'logits/chosen': -3.178292989730835, 'epoch': 0.76}\n",
      "{'loss': 0.2637, 'grad_norm': 5.681703567504883, 'learning_rate': 5e-05, 'rewards/chosen': -6.8406596183776855, 'rewards/rejected': -9.579423904418945, 'rewards/accuracies': 0.875, 'rewards/margins': 2.7387638092041016, 'logps/rejected': -184.93630981445312, 'logps/chosen': -144.23223876953125, 'logits/rejected': -2.990593910217285, 'logits/chosen': -2.7845070362091064, 'epoch': 0.77}\n",
      "{'loss': 0.1829, 'grad_norm': 1.458754301071167, 'learning_rate': 5e-05, 'rewards/chosen': -3.7596492767333984, 'rewards/rejected': -6.101197719573975, 'rewards/accuracies': 0.875, 'rewards/margins': 2.341548442840576, 'logps/rejected': -124.08914184570312, 'logps/chosen': -92.56138610839844, 'logits/rejected': -2.8386762142181396, 'logits/chosen': -2.488943099975586, 'epoch': 0.78}\n",
      "{'loss': 0.2795, 'grad_norm': 2.9397711753845215, 'learning_rate': 5e-05, 'rewards/chosen': -5.634970664978027, 'rewards/rejected': -7.353579998016357, 'rewards/accuracies': 0.875, 'rewards/margins': 1.718609094619751, 'logps/rejected': -150.72088623046875, 'logps/chosen': -137.33407592773438, 'logits/rejected': -2.8562867641448975, 'logits/chosen': -2.804215908050537, 'epoch': 0.78}\n",
      "{'loss': 0.4003, 'grad_norm': 3.3518433570861816, 'learning_rate': 5e-05, 'rewards/chosen': -4.049439430236816, 'rewards/rejected': -6.284955024719238, 'rewards/accuracies': 0.75, 'rewards/margins': 2.235515594482422, 'logps/rejected': -123.11233520507812, 'logps/chosen': -88.1152572631836, 'logits/rejected': -2.9854772090911865, 'logits/chosen': -2.675539493560791, 'epoch': 0.79}\n",
      "{'loss': 0.2655, 'grad_norm': 3.8151965141296387, 'learning_rate': 5e-05, 'rewards/chosen': -4.578749656677246, 'rewards/rejected': -8.359016418457031, 'rewards/accuracies': 0.875, 'rewards/margins': 3.780266761779785, 'logps/rejected': -155.109130859375, 'logps/chosen': -116.33735656738281, 'logits/rejected': -3.1271681785583496, 'logits/chosen': -2.6690926551818848, 'epoch': 0.79}\n",
      "{'loss': 0.1365, 'grad_norm': 1.5781793594360352, 'learning_rate': 5e-05, 'rewards/chosen': -3.917178153991699, 'rewards/rejected': -9.286632537841797, 'rewards/accuracies': 0.875, 'rewards/margins': 5.3694539070129395, 'logps/rejected': -171.8687744140625, 'logps/chosen': -97.58328247070312, 'logits/rejected': -3.131824016571045, 'logits/chosen': -2.5831985473632812, 'epoch': 0.8}\n",
      "{'loss': 0.5911, 'grad_norm': 4.67830228805542, 'learning_rate': 5e-05, 'rewards/chosen': -5.612990379333496, 'rewards/rejected': -9.492721557617188, 'rewards/accuracies': 0.75, 'rewards/margins': 3.879730463027954, 'logps/rejected': -182.20220947265625, 'logps/chosen': -121.43267059326172, 'logits/rejected': -3.0673444271087646, 'logits/chosen': -2.701646089553833, 'epoch': 0.81}\n",
      "{'loss': 0.8253, 'grad_norm': 4.588218688964844, 'learning_rate': 5e-05, 'rewards/chosen': -5.78836727142334, 'rewards/rejected': -8.216690063476562, 'rewards/accuracies': 0.75, 'rewards/margins': 2.4283227920532227, 'logps/rejected': -155.96044921875, 'logps/chosen': -120.13024139404297, 'logits/rejected': -2.7581915855407715, 'logits/chosen': -2.612860679626465, 'epoch': 0.81}\n",
      "{'loss': 0.1162, 'grad_norm': 1.1465836763381958, 'learning_rate': 5e-05, 'rewards/chosen': -3.885528087615967, 'rewards/rejected': -8.205649375915527, 'rewards/accuracies': 1.0, 'rewards/margins': 4.3201212882995605, 'logps/rejected': -150.06568908691406, 'logps/chosen': -89.58772277832031, 'logits/rejected': -2.7366690635681152, 'logits/chosen': -2.4002037048339844, 'epoch': 0.82}\n",
      "{'loss': 0.6729, 'grad_norm': 13.450770378112793, 'learning_rate': 5e-05, 'rewards/chosen': -5.7792792320251465, 'rewards/rejected': -7.302522659301758, 'rewards/accuracies': 0.5, 'rewards/margins': 1.523242712020874, 'logps/rejected': -129.31979370117188, 'logps/chosen': -105.5752944946289, 'logits/rejected': -2.612880229949951, 'logits/chosen': -2.4201085567474365, 'epoch': 0.82}\n",
      "{'loss': 0.3132, 'grad_norm': 4.539251327514648, 'learning_rate': 5e-05, 'rewards/chosen': -4.54112434387207, 'rewards/rejected': -9.273759841918945, 'rewards/accuracies': 0.875, 'rewards/margins': 4.732635974884033, 'logps/rejected': -180.7115936279297, 'logps/chosen': -115.85521697998047, 'logits/rejected': -2.96515154838562, 'logits/chosen': -2.6251139640808105, 'epoch': 0.83}\n",
      "{'loss': 0.7157, 'grad_norm': 4.83209753036499, 'learning_rate': 5e-05, 'rewards/chosen': -6.4859724044799805, 'rewards/rejected': -9.165651321411133, 'rewards/accuracies': 0.875, 'rewards/margins': 2.6796793937683105, 'logps/rejected': -172.53106689453125, 'logps/chosen': -127.2177734375, 'logits/rejected': -2.9266436100006104, 'logits/chosen': -2.7204885482788086, 'epoch': 0.84}\n",
      "{'loss': 0.3375, 'grad_norm': 3.440912961959839, 'learning_rate': 5e-05, 'rewards/chosen': -4.2503252029418945, 'rewards/rejected': -8.35920524597168, 'rewards/accuracies': 0.875, 'rewards/margins': 4.108879566192627, 'logps/rejected': -154.69027709960938, 'logps/chosen': -107.90416717529297, 'logits/rejected': -2.9717276096343994, 'logits/chosen': -2.4199652671813965, 'epoch': 0.84}\n",
      "{'loss': 0.6049, 'grad_norm': 6.940927505493164, 'learning_rate': 5e-05, 'rewards/chosen': -7.162737846374512, 'rewards/rejected': -9.857253074645996, 'rewards/accuracies': 0.625, 'rewards/margins': 2.694516181945801, 'logps/rejected': -176.0081787109375, 'logps/chosen': -142.1431427001953, 'logits/rejected': -2.6031298637390137, 'logits/chosen': -2.5011448860168457, 'epoch': 0.85}\n",
      "{'loss': 0.3029, 'grad_norm': 2.4060544967651367, 'learning_rate': 5e-05, 'rewards/chosen': -4.56355619430542, 'rewards/rejected': -6.5229387283325195, 'rewards/accuracies': 0.875, 'rewards/margins': 1.9593822956085205, 'logps/rejected': -139.611083984375, 'logps/chosen': -104.74745178222656, 'logits/rejected': -3.033052921295166, 'logits/chosen': -2.8581509590148926, 'epoch': 0.85}\n",
      "{'loss': 0.2866, 'grad_norm': 3.0636658668518066, 'learning_rate': 5e-05, 'rewards/chosen': -3.791919231414795, 'rewards/rejected': -5.915747165679932, 'rewards/accuracies': 0.875, 'rewards/margins': 2.1238279342651367, 'logps/rejected': -133.2216796875, 'logps/chosen': -95.33390808105469, 'logits/rejected': -2.9589293003082275, 'logits/chosen': -2.528749465942383, 'epoch': 0.86}\n",
      "{'loss': 0.1178, 'grad_norm': 0.7126455903053284, 'learning_rate': 5e-05, 'rewards/chosen': -4.2762627601623535, 'rewards/rejected': -9.053987503051758, 'rewards/accuracies': 0.875, 'rewards/margins': 4.777724742889404, 'logps/rejected': -175.21444702148438, 'logps/chosen': -97.10565185546875, 'logits/rejected': -2.991990327835083, 'logits/chosen': -2.5892863273620605, 'epoch': 0.87}\n",
      "{'loss': 0.0986, 'grad_norm': 1.371400237083435, 'learning_rate': 5e-05, 'rewards/chosen': -5.092578887939453, 'rewards/rejected': -8.578509330749512, 'rewards/accuracies': 1.0, 'rewards/margins': 3.4859304428100586, 'logps/rejected': -166.09974670410156, 'logps/chosen': -126.3786392211914, 'logits/rejected': -2.8560945987701416, 'logits/chosen': -2.6429834365844727, 'epoch': 0.87}\n",
      "{'loss': 0.2827, 'grad_norm': 2.811769485473633, 'learning_rate': 5e-05, 'rewards/chosen': -3.50290584564209, 'rewards/rejected': -6.200432777404785, 'rewards/accuracies': 0.75, 'rewards/margins': 2.697526454925537, 'logps/rejected': -129.53530883789062, 'logps/chosen': -92.23843383789062, 'logits/rejected': -2.846439838409424, 'logits/chosen': -2.537078380584717, 'epoch': 0.88}\n",
      "{'loss': 0.4863, 'grad_norm': 4.828886032104492, 'learning_rate': 5e-05, 'rewards/chosen': -6.435159206390381, 'rewards/rejected': -8.485748291015625, 'rewards/accuracies': 0.75, 'rewards/margins': 2.0505895614624023, 'logps/rejected': -159.27113342285156, 'logps/chosen': -137.80377197265625, 'logits/rejected': -2.915578603744507, 'logits/chosen': -2.8154714107513428, 'epoch': 0.88}\n",
      "{'loss': 1.4976, 'grad_norm': 8.071904182434082, 'learning_rate': 5e-05, 'rewards/chosen': -6.180373191833496, 'rewards/rejected': -6.95355224609375, 'rewards/accuracies': 0.625, 'rewards/margins': 0.7731792330741882, 'logps/rejected': -145.5701446533203, 'logps/chosen': -148.97169494628906, 'logits/rejected': -2.8317503929138184, 'logits/chosen': -2.7221598625183105, 'epoch': 0.89}\n",
      "{'loss': 0.0633, 'grad_norm': 1.3564146757125854, 'learning_rate': 5e-05, 'rewards/chosen': -7.535355567932129, 'rewards/rejected': -11.871635437011719, 'rewards/accuracies': 1.0, 'rewards/margins': 4.336280345916748, 'logps/rejected': -201.093994140625, 'logps/chosen': -139.66143798828125, 'logits/rejected': -2.8763582706451416, 'logits/chosen': -2.4183664321899414, 'epoch': 0.9}\n",
      "{'loss': 0.4289, 'grad_norm': 5.230525493621826, 'learning_rate': 5e-05, 'rewards/chosen': -5.051965713500977, 'rewards/rejected': -7.092528343200684, 'rewards/accuracies': 0.625, 'rewards/margins': 2.0405631065368652, 'logps/rejected': -147.49610900878906, 'logps/chosen': -115.7188949584961, 'logits/rejected': -2.787980556488037, 'logits/chosen': -2.5713486671447754, 'epoch': 0.9}\n",
      "{'loss': 0.5522, 'grad_norm': 6.025397777557373, 'learning_rate': 5e-05, 'rewards/chosen': -6.6141862869262695, 'rewards/rejected': -8.957948684692383, 'rewards/accuracies': 0.75, 'rewards/margins': 2.3437612056732178, 'logps/rejected': -165.79345703125, 'logps/chosen': -125.05804443359375, 'logits/rejected': -2.7520854473114014, 'logits/chosen': -2.428062677383423, 'epoch': 0.91}\n",
      "{'loss': 0.4691, 'grad_norm': 4.456544876098633, 'learning_rate': 5e-05, 'rewards/chosen': -4.770246505737305, 'rewards/rejected': -6.761186599731445, 'rewards/accuracies': 0.75, 'rewards/margins': 1.9909402132034302, 'logps/rejected': -129.49765014648438, 'logps/chosen': -112.42323303222656, 'logits/rejected': -2.6683292388916016, 'logits/chosen': -2.697470188140869, 'epoch': 0.92}\n",
      "{'loss': 0.2747, 'grad_norm': 4.2619709968566895, 'learning_rate': 5e-05, 'rewards/chosen': -6.116191387176514, 'rewards/rejected': -8.880291938781738, 'rewards/accuracies': 0.875, 'rewards/margins': 2.7641005516052246, 'logps/rejected': -164.43951416015625, 'logps/chosen': -129.91238403320312, 'logits/rejected': -2.680708646774292, 'logits/chosen': -2.5288352966308594, 'epoch': 0.92}\n",
      "{'loss': 0.2934, 'grad_norm': 3.785684108734131, 'learning_rate': 5e-05, 'rewards/chosen': -5.501931667327881, 'rewards/rejected': -7.678359508514404, 'rewards/accuracies': 0.875, 'rewards/margins': 2.1764276027679443, 'logps/rejected': -138.95252990722656, 'logps/chosen': -108.11842346191406, 'logits/rejected': -2.784637451171875, 'logits/chosen': -2.702442169189453, 'epoch': 0.93}\n",
      "{'loss': 0.2148, 'grad_norm': 3.437998056411743, 'learning_rate': 5e-05, 'rewards/chosen': -4.823795318603516, 'rewards/rejected': -8.94626235961914, 'rewards/accuracies': 0.875, 'rewards/margins': 4.122467994689941, 'logps/rejected': -170.99765014648438, 'logps/chosen': -102.45762634277344, 'logits/rejected': -2.978987693786621, 'logits/chosen': -2.474208116531372, 'epoch': 0.93}\n",
      "{'loss': 0.3562, 'grad_norm': 3.302635431289673, 'learning_rate': 5e-05, 'rewards/chosen': -4.333223819732666, 'rewards/rejected': -8.398860931396484, 'rewards/accuracies': 0.75, 'rewards/margins': 4.065637588500977, 'logps/rejected': -172.240478515625, 'logps/chosen': -98.06483459472656, 'logits/rejected': -2.5411407947540283, 'logits/chosen': -2.039616823196411, 'epoch': 0.94}\n",
      "{'loss': 0.4715, 'grad_norm': 4.475975513458252, 'learning_rate': 5e-05, 'rewards/chosen': -5.566821098327637, 'rewards/rejected': -7.913703918457031, 'rewards/accuracies': 0.875, 'rewards/margins': 2.3468825817108154, 'logps/rejected': -155.24069213867188, 'logps/chosen': -132.2612762451172, 'logits/rejected': -2.681856632232666, 'logits/chosen': -2.6998841762542725, 'epoch': 0.95}\n",
      "{'loss': 0.2263, 'grad_norm': 2.019681215286255, 'learning_rate': 5e-05, 'rewards/chosen': -4.691864490509033, 'rewards/rejected': -7.651336669921875, 'rewards/accuracies': 1.0, 'rewards/margins': 2.959472417831421, 'logps/rejected': -136.85986328125, 'logps/chosen': -104.93702697753906, 'logits/rejected': -2.7910704612731934, 'logits/chosen': -2.7286698818206787, 'epoch': 0.95}\n",
      "{'loss': 1.186, 'grad_norm': 6.127829074859619, 'learning_rate': 5e-05, 'rewards/chosen': -6.379758358001709, 'rewards/rejected': -8.125290870666504, 'rewards/accuracies': 0.625, 'rewards/margins': 1.7455323934555054, 'logps/rejected': -159.12608337402344, 'logps/chosen': -127.7406234741211, 'logits/rejected': -2.878361225128174, 'logits/chosen': -2.539562463760376, 'epoch': 0.96}\n",
      "{'loss': 0.4103, 'grad_norm': 4.679502010345459, 'learning_rate': 5e-05, 'rewards/chosen': -4.011816024780273, 'rewards/rejected': -7.655566215515137, 'rewards/accuracies': 0.75, 'rewards/margins': 3.6437501907348633, 'logps/rejected': -149.63673400878906, 'logps/chosen': -103.71424865722656, 'logits/rejected': -2.939368724822998, 'logits/chosen': -2.5174317359924316, 'epoch': 0.96}\n",
      "{'loss': 0.2562, 'grad_norm': 2.002990484237671, 'learning_rate': 5e-05, 'rewards/chosen': -4.100393295288086, 'rewards/rejected': -6.672746658325195, 'rewards/accuracies': 1.0, 'rewards/margins': 2.5723531246185303, 'logps/rejected': -152.52011108398438, 'logps/chosen': -107.19315338134766, 'logits/rejected': -2.8522608280181885, 'logits/chosen': -2.6477298736572266, 'epoch': 0.97}\n",
      "{'loss': 0.4615, 'grad_norm': 3.2420616149902344, 'learning_rate': 5e-05, 'rewards/chosen': -5.87458610534668, 'rewards/rejected': -7.592700004577637, 'rewards/accuracies': 0.75, 'rewards/margins': 1.7181131839752197, 'logps/rejected': -140.13438415527344, 'logps/chosen': -125.03250885009766, 'logits/rejected': -2.7923781871795654, 'logits/chosen': -2.7519140243530273, 'epoch': 0.98}\n",
      "{'loss': 0.3231, 'grad_norm': 3.711704730987549, 'learning_rate': 5e-05, 'rewards/chosen': -6.308465003967285, 'rewards/rejected': -8.119221687316895, 'rewards/accuracies': 0.875, 'rewards/margins': 1.8107571601867676, 'logps/rejected': -153.76773071289062, 'logps/chosen': -126.65989685058594, 'logits/rejected': -2.9020979404449463, 'logits/chosen': -2.651433229446411, 'epoch': 0.98}\n",
      "{'loss': 1.0843, 'grad_norm': 6.345090389251709, 'learning_rate': 5e-05, 'rewards/chosen': -6.071057319641113, 'rewards/rejected': -9.439014434814453, 'rewards/accuracies': 0.75, 'rewards/margins': 3.3679566383361816, 'logps/rejected': -163.44073486328125, 'logps/chosen': -123.40155029296875, 'logits/rejected': -2.7260468006134033, 'logits/chosen': -2.4011406898498535, 'epoch': 0.99}\n",
      "{'loss': 0.6702, 'grad_norm': 4.564029693603516, 'learning_rate': 5e-05, 'rewards/chosen': -5.198566913604736, 'rewards/rejected': -6.313239574432373, 'rewards/accuracies': 0.75, 'rewards/margins': 1.1146728992462158, 'logps/rejected': -127.54964447021484, 'logps/chosen': -111.0273208618164, 'logits/rejected': -2.9113831520080566, 'logits/chosen': -2.8149898052215576, 'epoch': 0.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [08:27<1:16:04, 507.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3695, 'grad_norm': 3.727492332458496, 'learning_rate': 5e-05, 'rewards/chosen': -6.891343116760254, 'rewards/rejected': -9.00625228881836, 'rewards/accuracies': 0.75, 'rewards/margins': 2.1149091720581055, 'logps/rejected': -170.78305053710938, 'logps/chosen': -151.42901611328125, 'logits/rejected': -2.930176258087158, 'logits/chosen': -2.9180445671081543, 'epoch': 1.0}\n",
      "{'train_runtime': 148.4917, 'train_samples_per_second': 8.883, 'train_steps_per_second': 1.111, 'train_loss': 0.4887764495656346, 'epoch': 1.0}\n",
      "Rolling Out from model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Answers in Answer Set:   9%|▉         | 1/11 [04:53<48:56, 293.62s/it]\n",
      " Answer Set:   0%|          | 0/2 [04:53<?, ?it/s]\n",
      " 10%|█         | 1/10 [13:20<2:00:07, 800.84s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(epochs)):\n\u001b[0;32m----> 2\u001b[0m     epoch_dataset, rewards \u001b[38;5;241m=\u001b[39m \u001b[43mrollout_to_DPO_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcritic_tokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcritic\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m     model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m      5\u001b[0m     dpo_trainer \u001b[38;5;241m=\u001b[39m DPOTrainer(\n\u001b[1;32m      6\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m      7\u001b[0m         ref_model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m         max_prompt_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m,\n\u001b[1;32m     14\u001b[0m     )\n",
      "Cell \u001b[0;32mIn[7], line 5\u001b[0m, in \u001b[0;36mrollout_to_DPO_dataset\u001b[0;34m(dataset, model, tokenizer, critic_tokenizer, critic, device)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRolling Out from model\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m----> 5\u001b[0m     answers \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_answers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mprompt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_answers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRoll out completed\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStarting to compute rewards\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[5], line 12\u001b[0m, in \u001b[0;36mgenerate_answers\u001b[0;34m(input_text, generator, tokenizer, n_answers, batch_size)\u001b[0m\n\u001b[1;32m     10\u001b[0m batch_inputs \u001b[38;5;241m=\u001b[39m input_text[i:i\u001b[38;5;241m+\u001b[39mbatch_size]\n\u001b[1;32m     11\u001b[0m batch_inputs \u001b[38;5;241m=\u001b[39m tokenizer(batch_inputs, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_length\u001b[39m\u001b[38;5;124m\"\u001b[39m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 12\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbatch_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m40\u001b[39;49m\n\u001b[1;32m     19\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m answers \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mbatch_decode(outputs, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     21\u001b[0m all_answers\u001b[38;5;241m.\u001b[39mextend(answers)\n",
      "File \u001b[0;32m/opt/conda/envs/llmrl/lib/python3.10/site-packages/peft/peft_model.py:1190\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.generate\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_peft_forward_hooks(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1189\u001b[0m         kwargs \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecial_peft_forward_args}\n\u001b[0;32m-> 1190\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1192\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model\u001b[38;5;241m.\u001b[39mgenerate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/conda/envs/llmrl/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/llmrl/lib/python3.10/site-packages/transformers/generation/utils.py:1575\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1567\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1568\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1569\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   1570\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1571\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1572\u001b[0m     )\n\u001b[1;32m   1574\u001b[0m     \u001b[38;5;66;03m# 13. run sample\u001b[39;00m\n\u001b[0;32m-> 1575\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1576\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1577\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1578\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_warper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_warper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1579\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1580\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1581\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1582\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1583\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_logits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_logits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1584\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1585\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1586\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1587\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1588\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1590\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH:\n\u001b[1;32m   1591\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   1592\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   1593\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   1594\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1599\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   1600\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/envs/llmrl/lib/python3.10/site-packages/transformers/generation/utils.py:2735\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, output_logits, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2733\u001b[0m \u001b[38;5;66;03m# sample\u001b[39;00m\n\u001b[1;32m   2734\u001b[0m probs \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39msoftmax(next_token_scores, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m-> 2735\u001b[0m next_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultinomial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m   2737\u001b[0m \u001b[38;5;66;03m# finished sentences should have their next token be a padding token\u001b[39;00m\n\u001b[1;32m   2738\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m eos_token_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in tqdm(range(epochs)):\n",
    "    epoch_dataset, rewards = rollout_to_DPO_dataset(dataset, model, tokenizer, critic_tokenizer, critic)\n",
    "    model.train()\n",
    "    \n",
    "    dpo_trainer = DPOTrainer(\n",
    "        model=model,\n",
    "        ref_model=None,\n",
    "        args=training_args,\n",
    "        beta=0.1,\n",
    "        train_dataset=epoch_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        max_length=512,\n",
    "        max_prompt_length=256,\n",
    "    )\n",
    "    \n",
    "    # Create the learning rate scheduler\n",
    "    lr_scheduler = get_scheduler(\n",
    "        name=\"cosine\",\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=int(total_steps * 0.1),  # 10% of total steps for warmup\n",
    "        num_training_steps=total_steps,\n",
    "    )\n",
    "    \n",
    "    # Train the model for one epoch\n",
    "    dpo_trainer.train()\n",
    "    \n",
    "    # Update the learning rate for the next epoch\n",
    "    lr_scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"checkpoints/dpo-tinyllama-5-1\") # Local saving\n",
    "tokenizer.save_pretrained(\"checkpoints/dpo-tinyllama-5-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmrl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
