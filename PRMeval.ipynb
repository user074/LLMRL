{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PRM Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the paper, the evaluation is using N sampling method. So I need to implement the N sampling method to get the best solution\n",
    "Generator: Generate the entire solution for N times\n",
    "Critic: Evaluate the solution for N times. \n",
    "\n",
    "We can test it with the grading method from the dataset. Also we can directly select solution from the dataset \"is_solution\" column\n",
    "The output of the model is in the format of \n",
    "\n",
    "\n",
    "'### [Solution]'\n",
    "\n",
    "So technically I can just extract the end of the output and use the grader to verify the solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_answer =  \" 320,000\"\n",
    "\n",
    "gt_answer = \"320,000\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data (scrapped since it's PRM and too complex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jqi/anaconda3/envs/llmrl/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"checkpoints/tinyLlama-GSM8K-10epochs\", padding_side='right', use_fast = False)\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token  # End-of-Sequence token\n",
    "prompt = \"\"\"\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    texts = []\n",
    "    final_answer = []\n",
    "    \n",
    "    for instruction, next_response, answer in zip(examples['instruction'], examples['next_response'], examples['answer']):\n",
    "        # Combine all responses and the next response into a single string with newline separation\n",
    "        if answer is not None:\n",
    "            final_answer.append(answer)\n",
    "        else:\n",
    "            cleaned_answer = next_response.split('# Answer')[1]\n",
    "            final_answer.append(cleaned_answer)\n",
    "        \n",
    "        # Format the text with the prompt template\n",
    "        text = prompt.format(instruction, '')\n",
    "        texts.append(text)\n",
    "\n",
    "    # Tokenize all texts at once using the tokenizer\n",
    "    # model_inputs = tokenizer(texts, padding=\"max_length\", truncation=True, max_length=512)\n",
    "    \n",
    "    return {'input_text': texts, 'final_answer': final_answer}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "dataset = load_dataset(\"Birchlabs/openai-prm800k-stepwise-critic\", split='test')\n",
    "# dataset = dataset.filter(lambda x: x['answer'] is not None)  # Filter entries without ratings\n",
    "dataset = dataset.filter(lambda x: x['is_solution'] is True)  # Filter entries without ratings\n",
    "\n",
    "dataset = dataset.map(formatting_prompts_func, batched=True)  # Apply the preprocessing function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator\n",
    "Generator generate the solution for N times. The critic evaluate the solution for N times. The critic will output the score of the solution. The generator will output the solution with the highest score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 2048, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-21): 22 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
       "          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from unsloth import FastLanguageModel\n",
    "from transformers import LlamaForSequenceClassification, AutoTokenizer, LlamaForCausalLM\n",
    "\n",
    "import torch\n",
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = False # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "\n",
    "generator = LlamaForCausalLM.from_pretrained(\"checkpoints/tinyLlama-GSM8K-10epochs\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"checkpoints/tinyLlama-GSM8K-10epochs\", padding_side='right', use_fast = False)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#to cuda\n",
    "\n",
    "generator.to(device)\n",
    "\n",
    "#set generator to inference mode\n",
    "generator.eval()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset preparation\n",
    "input_text = dataset['input_text']\n",
    "inputs = tokenizer(input_text, return_tensors='pt',padding=\"max_length\", truncation=True, max_length=512).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    }
   ],
   "source": [
    "# prompt = \"\"\"\n",
    "# ### Input:\n",
    "# {}\n",
    "\n",
    "# ### Response:\n",
    "# {}\"\"\"\n",
    "\n",
    "# inputs = tokenizer(\n",
    "# [\n",
    "#     prompt.format(\n",
    "#         \"A robe takes 2 bolts of blue fiber and half that much white fiber. How many bolts in total does it take?\", # Question\n",
    "#         \"\", # output - leave this blank for generation!\n",
    "#     )\n",
    "# ], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = generator.generate(**inputs, max_new_tokens = 256, use_cache = True, do_sample=True, temperature=0.5, top_k=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = tokenizer.batch_decode(outputs, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answers(input_text, generator, tokenizer):\n",
    "    inputs = tokenizer(input_text, return_tensors='pt',padding=\"max_length\", truncation=True, max_length=512).to(device)\n",
    "    outputs = generator.generate(**inputs, max_new_tokens = 256, use_cache = True, do_sample=True, temperature=0.5, top_k=40)\n",
    "    answers = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    return answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n### Input:\\n('Three pencils and a jumbo eraser cost $\\\\\\\\$1.24$. Five pencils and a jumbo eraser cost $\\\\\\\\$1.82$. No prices include tax. In cents, what is the cost of a pencil?',)\\n\\n### Response:\\nat $1.24 per pencil, 3 pencils cost 3*1.24 = $<<3*1.24=3.68>>3.68\\nat $1.82 per pencil, 5 pencils cost 5*1.82 = $<<5*1.82=9.64>>9.64\\nIn cents, the cost of a pencil is $3.68 + $9.64 = $<<3.68+9.64=13.32>>13.32\\n#### 1332\""
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#save answers to a local file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'29'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['answer'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_strings_to_file(strings, file_path):\n",
    "    \"\"\"\n",
    "    Saves a list of strings to a file.\n",
    "    \n",
    "    Args:\n",
    "        strings (list): The list of strings to be saved.\n",
    "        file_path (str): The path of the file to save the strings.\n",
    "    \"\"\"\n",
    "    with open(file_path, \"w\") as file:\n",
    "        for string in strings:\n",
    "            # Replace newline characters with a placeholder\n",
    "            string = string.replace(\"\\n\", \"\\\\n\")\n",
    "            file.write(string + \"\\n\")\n",
    "    \n",
    "    print(\"Strings saved to\", file_path)\n",
    "\n",
    "\n",
    "def read_strings_from_file(file_path):\n",
    "    \"\"\"\n",
    "    Reads a list of strings from a file.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): The path of the file to read the strings from.\n",
    "        \n",
    "    Returns:\n",
    "        list: The list of strings read from the file.\n",
    "    \"\"\"\n",
    "    with open(file_path, \"r\") as file:\n",
    "        lines = file.readlines()\n",
    "    \n",
    "    strings = [line.strip().replace(\"\\\\n\", \"\\n\") for line in lines]\n",
    "    \n",
    "    return strings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Strings saved to test1.txt\n"
     ]
    }
   ],
   "source": [
    "save_strings_to_file(answers, \"test1.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers2 = read_strings_from_file(\"test1.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n### Input:\\n('Three pencils and a jumbo eraser cost $\\\\\\\\$1.24$. Five pencils and a jumbo eraser cost $\\\\\\\\$1.82$. No prices include tax. In cents, what is the cost of a pencil?',)\\n\\n### Response:\\nat $1.24 per pencil, 3 pencils cost 3*1.24 = $<<3*1.24=3.68>>3.68\\nat $1.82 per pencil, 5 pencils cost 5*1.82 = $<<5*1.82=9.64>>9.64\\nIn cents, the cost of a pencil is $3.68 + $9.64 = $<<3.68+9.64=13.32>>13.32\\n#### 1332\""
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers2[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Critic\n",
    "First I need to test whether critic actually work for the wrong solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForSequenceClassification(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 2048, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-21): 22 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
       "          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (score): Linear(in_features=2048, out_features=3, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import LlamaForSequenceClassification, AutoTokenizer, LlamaForCausalLM\n",
    "import torch\n",
    "critic = LlamaForSequenceClassification.from_pretrained(\"/home/jqi/Github/LLMRL/checkpoints/tinyLlama-critic\")\n",
    "critic.eval()\n",
    "\n",
    "critic_tokenizer = AutoTokenizer.from_pretrained(\"/home/jqi/Github/LLMRL/checkpoints/tinyLlama-critic\", padding_side='right', use_fast= False)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#to cuda\n",
    "critic.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.4651, -1.4561, -0.1776]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "\n",
    "Question = \"How many positive two-digit integers leave a remainder of 2 when divided by 8?\"\n",
    "Answer = [ \"So if a number leaves a remainder of 2 when divided by 8, it's of the form 8n+2.\", \"So we want to know the number of positive two-digit integers of the form 8n+2.\", \"I think we should just plug in numbers and see what happens.\", \"Ok let's start with n=1.\", \"8*1+2=10 which is a two-digit integer.\", \"Let's try n=2.\", \"8*2+2=18 which is also a two-digit integer.\" ]\n",
    "wrong_answer = \"And if we keep going we'll see that all the numbers of the form 8n+2 are two-digit integers.\"\n",
    "\n",
    "combined_responses = \"\"\n",
    "for i in range(len(Answer)):\n",
    "    combined_responses += Answer[i] + \"\\n\"\n",
    "    \n",
    "combined_wrong_responses = combined_responses + wrong_answer\n",
    "\n",
    "input_text = prompt.format(Question, Answer)\n",
    "inputs = critic_tokenizer(combined_wrong_responses, return_tensors=\"pt\").to(\"cuda\")\n",
    "with torch.no_grad():\n",
    "    outputs = critic(**inputs)\n",
    "print(outputs.logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5980, 0.0876, 0.3145]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "#take the softmax of the logits\n",
    "import torch.nn.functional as F\n",
    "probabilities = F.softmax(outputs.logits, dim=1)\n",
    "print(probabilities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = '### Input:\\nWHow many seconds are in 7.8 minutes?\\n\\n### Response:\\n'\n",
    "responses = [ \"7.8 minutes is the same as 7 minutes and 0.8 minutes.\", \"Right, and since there are 60 seconds in a minute, then there are 60 * 7 = 420 seconds in 7 minutes.\", \"And since there are 60 seconds in a minute, then there are 60 * 0.8 = 48 seconds in 0.8 minutes.\", \"So, in total, there are 420 + 48 = 468 seconds in 7.8 minutes.\" ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n### Input:\\n('Three pencils and a jumbo eraser cost $\\\\\\\\$1.24$. Five pencils and a jumbo eraser cost $\\\\\\\\$1.82$. No prices include tax. In cents, what is the cost of a pencil?',)\\n\\n### Response:\\nat $1.24 per pencil, 3 pencils cost 3*1.24 = $<<3*1.24=3.68>>3.68\\nat $1.82 per pencil, 5 pencils cost 5*1.82 = $<<5*1.82=9.64>>9.64\\nIn cents, the cost of a pencil is $3.68 + $9.64 = $<<3.68+9.64=13.32>>13.32\\n#### 1332\""
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "answers_prob = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for answer in answers:\n",
    "        result = answer.split('### Response:\\n')[0] + '### Response:\\n'\n",
    "        responses = answer.split('### Response:\\n')[1].split('\\n')\n",
    "        correct_probability = 1\n",
    "        incorrect_probability = 1\n",
    "        for response in responses:\n",
    "            result += response + \"\\n\"\n",
    "            inputs = critic_tokenizer(result, return_tensors=\"pt\").to(\"cuda\")\n",
    "            outputs = critic(**inputs)\n",
    "            #take the softmax of the logits\n",
    "            probabilities = F.softmax(outputs.logits, dim=1)\n",
    "            correct_probability *= (probabilities[0][2]+ probabilities[0][1])\n",
    "        answers_prob.append(correct_probability.to(\"cpu\").numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers_prob = [float(i) for i in answers_prob]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'29'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['answer'][-18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Input:\n",
      "('Three pencils and a jumbo eraser cost $\\\\$1.24$. Five pencils and a jumbo eraser cost $\\\\$1.82$. No prices include tax. In cents, what is the cost of a pencil?',)\n",
      "\n",
      "### Response:\n",
      "  Three pencils and a jumbo eraser cost $\\\\$1.24$, so 1.24*3 = $\\\\$3.68\n",
      "  Five pencils and a jumbo eraser cost $\\\\$1.82$, so 1.82*5 = $\\\\$9.64\n",
      "  The cost of a pencil is 3+9 = $\\\\$12.\n",
      "#### 12\n"
     ]
    }
   ],
   "source": [
    "print(answers[-18])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5060761570930481"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers_prob[-18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the predicted labels\n",
    "predicted_labels = torch.argmax(probabilities, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 2, 0, 0, 0, 2, 0, 2, 2, 0, 0, 2, 2, 0, 2, 0, 0, 0, 2, 0, 0, 0, 0, 2,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 2,\n",
       "        0, 0, 0, 0, 0, 2, 2, 0, 0, 0, 2, 2, 0, 2, 2, 0, 2, 0, 0, 0, 2, 0, 0, 0,\n",
       "        0, 0, 0, 2, 0, 0, 0, 2, 0, 2, 0, 2, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 2,\n",
       "        0, 0, 2, 2, 0, 2, 2, 0, 0, 2, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 2,\n",
       "        0, 0, 0, 0, 0, 0], device='cuda:0')"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = ouput_text[0].split('### Response:\\n')[0] + '### Response:\\n'\n",
    "responses = ouput_text[0].split('### Response:\\n')[1].split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1941, 0.0384, 0.7675]], device='cuda:0')\n",
      "tensor([[0.0710, 0.0134, 0.9156]], device='cuda:0')\n",
      "tensor([[0.0393, 0.0118, 0.9489]], device='cuda:0')\n",
      "tensor([[0.0174, 0.0075, 0.9751]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "correct_probability = 1\n",
    "incorrect_probability = 1\n",
    "neutral_probability = 1\n",
    "\n",
    "with torch.no_grad():\n",
    "    for response in responses:\n",
    "        result += response + \"\\n\"\n",
    "        inputs = critic_tokenizer(result, return_tensors=\"pt\").to(\"cuda\")\n",
    "        outputs = critic(**inputs)\n",
    "        #take the softmax of the logits\n",
    "        import torch.nn.functional as F\n",
    "        probabilities = F.softmax(outputs.logits, dim=1)\n",
    "        print(probabilities)\n",
    "        correct_probability *= (probabilities[0][2]+ probabilities[0][1])\n",
    "        incorrect_probability *= probabilities[0][0]\n",
    "        # neutral_probability *= \n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7068, device='cuda:0')\n",
      "tensor(9.4226e-06, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(correct_probability)\n",
    "print(incorrect_probability)\n",
    "# print(neutral_probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['First find how many bolts of blue fiber the robe takes: 2 bolts of blue fiber * 2/2 = <<2*2=4>>4 bolts',\n",
       " 'Then find how many bolts of white fiber the robe takes: 4 bolts of white fiber / 2 = <<4/2=2>>2 bolts',\n",
       " 'Then add the two bolt counts to find the total number of bolts: 2 bolts of blue fiber + 2 bolts of white fiber = <<2+2=4>>4 bolts',\n",
       " '#### 4']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4339, 0.2652, 0.3009]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "inputs = critic_tokenizer(result, return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = critic(**inputs)\n",
    "#take the softmax of the logits\n",
    "import torch.nn.functional as F\n",
    "probabilities = F.softmax(outputs.logits, dim=1)\n",
    "print(probabilities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.6243e-02, 6.9465e-06, 9.4159e-02]], device='cuda:0',\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "probabilities_total = probabilities_total* probabilities\n",
    "probabilities_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.3050e-02, 1.7264e-06, 1.1865e-01]], device='cuda:0',\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "probabilities_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = ouput_text[0].split(\"<s> \\n\")[1].split(\"</s>\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'### Input:\\nA robe takes 2 bolts of blue fiber and half that much white fiber. How many bolts in total does it take?\\n\\n### Response:\\nFirst find the total number of blue fiber: 2 bolts * 2 = <<2*2=4>>4 bolts\\nThen find the total number of white fiber: 2 bolts / 2 = <<2/2=1>>1 bolt\\nThen add the blue and white fiber to find the total number of fiber: 4 bolts + 1 bolt = <<4+1=5>>5 bolts\\n#### 5'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#turn outputs into tokens\n",
    "answer = critic_tokenizer(output, return_tensors=\"pt\").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    1,   835, 10567, 29901,    13, 29909,   696,   915,  4893, 29871,\n",
       "         29906, 15772,  1372,   310,  7254,  5713,   495,   322,  4203,   393,\n",
       "          1568,  4796,  5713,   495, 29889,  1128,  1784, 15772,  1372,   297,\n",
       "          3001,   947,   372,  2125, 29973,    13,    13,  2277, 29937, 13291,\n",
       "         29901,    13,  6730,  1284,   278,  3001,  1353,   310,  7254,  5713,\n",
       "           495, 29901, 29871, 29906, 15772,  1372,   334, 29871, 29906,   353,\n",
       "          3532, 29906, 29930, 29906, 29922, 29946,  6778, 29946, 15772,  1372,\n",
       "            13, 11760,  1284,   278,  3001,  1353,   310,  4796,  5713,   495,\n",
       "         29901, 29871, 29906, 15772,  1372,   847, 29871, 29906,   353,  3532,\n",
       "         29906, 29914, 29906, 29922, 29896,  6778, 29896,   289, 14339,    13,\n",
       "         11760,   788,   278,  7254,   322,  4796,  5713,   495,   304,  1284,\n",
       "           278,  3001,  1353,   310,  5713,   495, 29901, 29871, 29946, 15772,\n",
       "          1372,   718, 29871, 29896,   289, 14339,   353,  3532, 29946, 29974,\n",
       "         29896, 29922, 29945,  6778, 29945, 15772,  1372,    13,  4136, 29871,\n",
       "         29945]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
       "       device='cuda:0')}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'LlamaModel' object has no attribute '_has_no_labels'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n",
      "\u001b[0;32m----> 1\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mcritic\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43manswer\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(outputs\u001b[38;5;241m.\u001b[39mlogits)\n",
      "\n",
      "File \u001b[0;32m~/anaconda3/envs/llmrl/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n",
      "\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[0;32m~/anaconda3/envs/llmrl/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
      "\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n",
      "\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n",
      "\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\n",
      "File \u001b[0;32m~/anaconda3/envs/llmrl/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:1388\u001b[0m, in \u001b[0;36mLlamaForSequenceClassification.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n",
      "\u001b[1;32m   1380\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n",
      "\u001b[1;32m   1381\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n",
      "\u001b[1;32m   1382\u001b[0m \u001b[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n",
      "\u001b[1;32m   1383\u001b[0m \u001b[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n",
      "\u001b[1;32m   1384\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n",
      "\u001b[1;32m   1385\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n",
      "\u001b[1;32m   1386\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n",
      "\u001b[0;32m-> 1388\u001b[0m transformer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[1;32m   1389\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1390\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1391\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1392\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1393\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1394\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1395\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1396\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1397\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1398\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m   1399\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m transformer_outputs[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;32m   1400\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscore(hidden_states)\n",
      "\n",
      "File \u001b[0;32m~/anaconda3/envs/llmrl/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n",
      "\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[0;32m~/anaconda3/envs/llmrl/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
      "\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n",
      "\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n",
      "\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\n",
      "File \u001b[0;32m~/anaconda3/envs/llmrl/lib/python3.10/site-packages/unsloth/models/llama.py:572\u001b[0m, in \u001b[0;36mLlamaModel_fast_forward\u001b[0;34m(self, input_ids, causal_mask, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m    568\u001b[0m \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "\u001b[1;32m    570\u001b[0m \u001b[38;5;66;03m# Fix up attention mask by setting elements to 0\u001b[39;00m\n",
      "\u001b[1;32m    571\u001b[0m \u001b[38;5;66;03m# Specifically for DPO\u001b[39;00m\n",
      "\u001b[0;32m--> 572\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_has_no_labels\u001b[49m \u001b[38;5;129;01mand\u001b[39;00m (attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (past_key_values \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m \\\n",
      "\u001b[1;32m    573\u001b[0m     (\u001b[38;5;129;01mnot\u001b[39;00m train_embed_tokens):\n",
      "\u001b[1;32m    574\u001b[0m     \u001b[38;5;66;03m# Careful for inference the attention_mask is size (1, kv_seq_len)\u001b[39;00m\n",
      "\u001b[1;32m    575\u001b[0m     \u001b[38;5;66;03m# Whilst the input_embeds is size (1, 1, 4096)\u001b[39;00m\n",
      "\u001b[1;32m    576\u001b[0m     inputs_requires_grad \u001b[38;5;241m=\u001b[39m inputs_embeds\u001b[38;5;241m.\u001b[39mrequires_grad\n",
      "\u001b[1;32m    577\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m inputs_embeds\u001b[38;5;241m.\u001b[39mis_leaf:\n",
      "\n",
      "File \u001b[0;32m~/anaconda3/envs/llmrl/lib/python3.10/site-packages/torch/nn/modules/module.py:1688\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n",
      "\u001b[1;32m   1686\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n",
      "\u001b[1;32m   1687\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n",
      "\u001b[0;32m-> 1688\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'LlamaModel' object has no attribute '_has_no_labels'"
     ]
    }
   ],
   "source": [
    "outputs = critic(**answer)\n",
    "print(outputs.logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth: Fast Llama patching release 2024.4\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 4090. Max memory: 23.642 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.2.2. CUDA = 8.9. CUDA Toolkit = 11.8.\n",
      "\\        /    Bfloat16 = TRUE. Xformers = 0.0.25.post1. FA = True.\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  3.73it/s]\n",
      "WARNING:root:Some parameters are on the meta device device because they were offloaded to the cpu.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "#Now if you want to load the LoRA adapters we just saved for inference, set False to True:\n",
    "from unsloth.models.loader import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = False # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "generator, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"checkpoints/llama3-8b-gsm8k-1epoch\", # YOUR MODEL YOU USED FOR TRAINING\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")\n",
    "FastLanguageModel.for_inference(generator) # Enable native 2x faster inference\n",
    "tokenizer.padding_side = \"left\" \n",
    "\n",
    "\n",
    "# alpaca_prompt = You MUST copy from above!\n",
    "prompt = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "You are a helpful assistant to solve math problems step by step <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "{}\"\"\"\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    texts = []\n",
    "    final_answer = []\n",
    "    \n",
    "    for instruction, answer in zip(examples['question'], examples['answer']):\n",
    "        # Combine all responses and the next response into a single string with newline separation\n",
    "        extracted_answer = answer.split('### ')[1]\n",
    "        final_answer.append(extracted_answer)\n",
    "        \n",
    "        # Format the text with the prompt template\n",
    "        text = prompt.format(instruction, '')\n",
    "        texts.append(text)\n",
    "\n",
    "    # Tokenize all texts at once using the tokenizer\n",
    "    # model_inputs = tokenizer(texts, padding=\"max_length\", truncation=True, max_length=512)\n",
    "    \n",
    "    return {'input_text': texts, 'final_answer': final_answer}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1319"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "dataset = load_dataset(\"gsm8k\", 'main', split='test')\n",
    "dataset = dataset.map(formatting_prompts_func, batched=True)  # Apply the preprocessing function\n",
    "\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Generate Answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # from unsloth import FastLanguageModel\n",
    "# from transformers import LlamaForSequenceClassification, AutoTokenizer, LlamaForCausalLM\n",
    "\n",
    "# import torch\n",
    "# max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "# dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "# load_in_4bit = False # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "\n",
    "# generator = LlamaForCausalLM.from_pretrained(\"checkpoints/tinyLlama-GSM8K-10epochs\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"checkpoints/tinyLlama-GSM8K-10epochs\", padding_side='right', use_fast = False)\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# #to cuda\n",
    "\n",
    "# generator.to(device)\n",
    "\n",
    "# #set generator to inference mode\n",
    "# generator.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset preparation\n",
    "input_text = dataset['input_text']\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# inputs = tokenizer(input_text, return_tensors='pt',padding=\"max_length\", truncation=True, max_length=512).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "# Configure the logging level to suppress warnings\n",
    "logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
    "\n",
    "def generate_answers(input_text, generator, tokenizer, output_dir, n_answers=100, batch_size=32):\n",
    "    os.makedirs(output_dir, exist_ok=True)  # Create the output directory if it doesn't exist\n",
    "\n",
    "    # Get the existing JSON file numbers in the directory\n",
    "    existing_files = os.listdir(output_dir)\n",
    "    file_numbers = []\n",
    "    for file_name in existing_files:\n",
    "        match = re.match(r\"generated_answers_(\\d+)\\.json\", file_name)\n",
    "        if match:\n",
    "            file_numbers.append(int(match.group(1)))\n",
    "\n",
    "    # Start generating new answers from the next available number\n",
    "    start_number = max(file_numbers, default=0) + 1\n",
    "\n",
    "    for n in tqdm(range(start_number, start_number + n_answers), desc=\"Generating answers\"):\n",
    "        all_answers = []\n",
    "        for i in tqdm(range(0, len(input_text), batch_size), desc=f\"Processing batch for set {n}\"):\n",
    "            batch_inputs = input_text[i:i+batch_size]\n",
    "            batch_inputs = tokenizer(batch_inputs, return_tensors='pt', padding=\"max_length\", truncation=True, max_length=256).to(\"cuda\")\n",
    "            outputs = generator.generate(\n",
    "                **batch_inputs,\n",
    "                max_new_tokens=256,\n",
    "                use_cache=True,\n",
    "                do_sample=True,\n",
    "                temperature=0.5,\n",
    "                top_k=40,\n",
    "                eos_token_id = tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "            )\n",
    "            answers = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "            all_answers.extend(answers)\n",
    "\n",
    "        # Generate a unique file name for each set of n_answers\n",
    "        file_name = f\"generated_answers_{n}.json\"\n",
    "        file_path = os.path.join(output_dir, file_name)\n",
    "\n",
    "        # Save the answers to a JSON file for each set of n_answers in the specified directory\n",
    "        with open(file_path, 'w') as file:\n",
    "            json.dump(all_answers, file)\n",
    "        print(f\"Generated {len(all_answers)} answers for set {n}.\")\n",
    "\n",
    "    return all_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch for set 1: 100%|██████████| 42/42 [06:43<00:00,  9.60s/it]\n",
      "Generating answers:   1%|          | 1/100 [06:43<11:05:26, 403.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1319 answers for set 1.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch for set 2: 100%|██████████| 42/42 [06:42<00:00,  9.57s/it]\n",
      "Generating answers:   2%|▏         | 2/100 [13:25<10:57:35, 402.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1319 answers for set 2.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch for set 3: 100%|██████████| 42/42 [06:25<00:00,  9.17s/it]\n",
      "Generating answers:   3%|▎         | 3/100 [19:50<10:37:57, 394.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1319 answers for set 3.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch for set 4: 100%|██████████| 42/42 [06:27<00:00,  9.23s/it]\n",
      "Generating answers:   4%|▍         | 4/100 [26:18<10:27:06, 391.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1319 answers for set 4.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch for set 5: 100%|██████████| 42/42 [06:35<00:00,  9.43s/it]\n",
      "Generating answers:   5%|▌         | 5/100 [32:54<10:22:53, 393.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1319 answers for set 5.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch for set 6: 100%|██████████| 42/42 [06:26<00:00,  9.20s/it]\n",
      "Generating answers:   6%|▌         | 6/100 [39:20<10:12:35, 391.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1319 answers for set 6.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch for set 7: 100%|██████████| 42/42 [06:33<00:00,  9.37s/it]\n",
      "Generating answers:   7%|▋         | 7/100 [45:54<10:07:22, 391.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1319 answers for set 7.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch for set 8: 100%|██████████| 42/42 [06:44<00:00,  9.64s/it]\n",
      "Generating answers:   8%|▊         | 8/100 [52:39<10:07:09, 395.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1319 answers for set 8.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch for set 9: 100%|██████████| 42/42 [06:40<00:00,  9.55s/it]\n",
      "Generating answers:   9%|▉         | 9/100 [59:20<10:02:56, 397.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1319 answers for set 9.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch for set 10: 100%|██████████| 42/42 [06:44<00:00,  9.62s/it]\n",
      "Generating answers:  10%|█         | 10/100 [1:06:04<9:59:22, 399.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1319 answers for set 10.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch for set 11: 100%|██████████| 42/42 [06:30<00:00,  9.29s/it]\n",
      "Generating answers:  11%|█         | 11/100 [1:12:34<9:48:29, 396.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1319 answers for set 11.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch for set 12: 100%|██████████| 42/42 [06:37<00:00,  9.45s/it]\n",
      "Generating answers:  12%|█▏        | 12/100 [1:19:11<9:42:01, 396.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1319 answers for set 12.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch for set 13: 100%|██████████| 42/42 [06:41<00:00,  9.56s/it]\n",
      "Generating answers:  13%|█▎        | 13/100 [1:25:52<9:37:22, 398.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1319 answers for set 13.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch for set 14: 100%|██████████| 42/42 [06:26<00:00,  9.21s/it]\n",
      "Generating answers:  14%|█▍        | 14/100 [1:32:19<9:25:51, 394.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1319 answers for set 14.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch for set 15: 100%|██████████| 42/42 [06:42<00:00,  9.59s/it]\n",
      "Generating answers:  15%|█▌        | 15/100 [1:39:02<9:22:41, 397.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1319 answers for set 15.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch for set 16: 100%|██████████| 42/42 [06:14<00:00,  8.91s/it]\n",
      "Generating answers:  16%|█▌        | 16/100 [1:45:17<9:06:28, 390.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1319 answers for set 16.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch for set 17: 100%|██████████| 42/42 [06:25<00:00,  9.18s/it]\n",
      "Generating answers:  17%|█▋        | 17/100 [1:51:42<8:57:56, 388.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1319 answers for set 17.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch for set 18: 100%|██████████| 42/42 [06:37<00:00,  9.47s/it]\n",
      "Generating answers:  18%|█▊        | 18/100 [1:58:20<8:55:10, 391.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1319 answers for set 18.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch for set 19: 100%|██████████| 42/42 [06:47<00:00,  9.70s/it]\n",
      "Generating answers:  19%|█▉        | 19/100 [2:05:07<8:55:01, 396.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1319 answers for set 19.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch for set 20: 100%|██████████| 42/42 [06:42<00:00,  9.59s/it]\n",
      "Generating answers:  20%|██        | 20/100 [2:11:50<8:51:03, 398.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1319 answers for set 20.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch for set 21: 100%|██████████| 42/42 [06:44<00:00,  9.62s/it]\n",
      "Generating answers:  21%|██        | 21/100 [2:18:34<8:46:43, 400.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1319 answers for set 21.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch for set 22: 100%|██████████| 42/42 [06:40<00:00,  9.52s/it]\n",
      "Generating answers:  22%|██▏       | 22/100 [2:25:14<8:40:03, 400.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1319 answers for set 22.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch for set 23: 100%|██████████| 42/42 [06:40<00:00,  9.54s/it]\n",
      "Generating answers:  23%|██▎       | 23/100 [2:31:55<8:33:41, 400.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1319 answers for set 23.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch for set 24: 100%|██████████| 42/42 [06:47<00:00,  9.71s/it]\n",
      "Generating answers:  24%|██▍       | 24/100 [2:38:43<8:29:50, 402.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1319 answers for set 24.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch for set 25: 100%|██████████| 42/42 [06:38<00:00,  9.49s/it]\n",
      "Generating answers:  25%|██▌       | 25/100 [2:45:22<8:21:43, 401.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1319 answers for set 25.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch for set 26: 100%|██████████| 42/42 [06:39<00:00,  9.50s/it]\n",
      "Generating answers:  26%|██▌       | 26/100 [2:52:01<8:14:10, 400.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1319 answers for set 26.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch for set 27: 100%|██████████| 42/42 [06:27<00:00,  9.23s/it]\n",
      "Generating answers:  27%|██▋       | 27/100 [2:58:28<8:02:44, 396.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1319 answers for set 27.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch for set 28: 100%|██████████| 42/42 [06:44<00:00,  9.63s/it]\n",
      "Generating answers:  28%|██▊       | 28/100 [3:05:13<7:58:57, 399.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1319 answers for set 28.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch for set 29: 100%|██████████| 42/42 [06:47<00:00,  9.71s/it]\n",
      "Generating answers:  29%|██▉       | 29/100 [3:12:01<7:55:27, 401.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1319 answers for set 29.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch for set 30: 100%|██████████| 42/42 [06:34<00:00,  9.39s/it]\n",
      "Generating answers:  30%|███       | 30/100 [3:18:35<7:46:06, 399.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1319 answers for set 30.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch for set 31: 100%|██████████| 42/42 [06:54<00:00,  9.87s/it]\n",
      "Generating answers:  31%|███       | 31/100 [3:25:30<7:44:35, 403.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1319 answers for set 31.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch for set 32: 100%|██████████| 42/42 [06:34<00:00,  9.40s/it]\n",
      "Generating answers:  32%|███▏      | 32/100 [3:32:04<7:34:45, 401.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1319 answers for set 32.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch for set 33: 100%|██████████| 42/42 [06:36<00:00,  9.44s/it]\n",
      "Generating answers:  33%|███▎      | 33/100 [3:38:41<7:26:31, 399.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1319 answers for set 33.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch for set 34: 100%|██████████| 42/42 [06:39<00:00,  9.52s/it]\n",
      "Generating answers:  34%|███▍      | 34/100 [3:45:21<7:19:48, 399.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1319 answers for set 34.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch for set 35: 100%|██████████| 42/42 [06:41<00:00,  9.56s/it]\n",
      "Generating answers:  35%|███▌      | 35/100 [3:52:02<7:13:40, 400.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1319 answers for set 35.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch for set 36: 100%|██████████| 42/42 [06:45<00:00,  9.66s/it]\n",
      "Generating answers:  36%|███▌      | 36/100 [3:58:48<7:08:45, 401.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1319 answers for set 36.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch for set 37: 100%|██████████| 42/42 [06:33<00:00,  9.36s/it]\n",
      "Generating answers:  37%|███▋      | 37/100 [4:05:21<6:59:17, 399.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1319 answers for set 37.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch for set 38: 100%|██████████| 42/42 [06:34<00:00,  9.40s/it]\n",
      "Generating answers:  38%|███▊      | 38/100 [4:11:56<6:51:16, 398.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1319 answers for set 38.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch for set 39: 100%|██████████| 42/42 [06:40<00:00,  9.54s/it]\n",
      "Generating answers:  39%|███▉      | 39/100 [4:18:37<6:45:27, 398.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1319 answers for set 39.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch for set 40: 100%|██████████| 42/42 [06:44<00:00,  9.63s/it]\n",
      "Generating answers:  40%|████      | 40/100 [4:25:21<6:40:28, 400.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1319 answers for set 40.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch for set 41: 100%|██████████| 42/42 [06:38<00:00,  9.49s/it]\n",
      "Generating answers:  41%|████      | 41/100 [4:32:00<6:33:12, 399.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1319 answers for set 41.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch for set 42: 100%|██████████| 42/42 [06:40<00:00,  9.54s/it]\n",
      "Generating answers:  42%|████▏     | 42/100 [4:38:40<6:26:48, 400.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1319 answers for set 42.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch for set 43: 100%|██████████| 42/42 [06:34<00:00,  9.39s/it]\n",
      "Generating answers:  43%|████▎     | 43/100 [4:45:15<6:18:30, 398.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1319 answers for set 43.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch for set 44: 100%|██████████| 42/42 [06:40<00:00,  9.54s/it]\n",
      "Generating answers:  44%|████▍     | 44/100 [4:51:56<6:12:30, 399.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1319 answers for set 44.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch for set 45: 100%|██████████| 42/42 [06:32<00:00,  9.36s/it]\n",
      "Generating answers:  45%|████▌     | 45/100 [4:58:29<6:04:09, 397.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1319 answers for set 45.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch for set 46: 100%|██████████| 42/42 [06:31<00:00,  9.33s/it]\n",
      "Generating answers:  46%|████▌     | 46/100 [5:05:00<5:56:03, 395.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1319 answers for set 46.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch for set 47: 100%|██████████| 42/42 [06:34<00:00,  9.39s/it]\n",
      "Generating answers:  47%|████▋     | 47/100 [5:11:35<5:49:06, 395.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1319 answers for set 47.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch for set 48: 100%|██████████| 42/42 [06:34<00:00,  9.39s/it]\n",
      "Generating answers:  48%|████▊     | 48/100 [5:18:09<5:42:17, 394.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1319 answers for set 48.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch for set 49: 100%|██████████| 42/42 [06:31<00:00,  9.32s/it]\n",
      "Generating answers:  49%|████▉     | 49/100 [5:24:40<5:34:49, 393.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1319 answers for set 49.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch for set 50: 100%|██████████| 42/42 [06:28<00:00,  9.25s/it]\n",
      "Generating answers:  50%|█████     | 50/100 [5:31:09<5:26:57, 392.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1319 answers for set 50.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch for set 51: 100%|██████████| 42/42 [06:28<00:00,  9.25s/it]\n",
      "Generating answers:  51%|█████     | 51/100 [5:37:38<5:19:27, 391.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1319 answers for set 51.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch for set 52: 100%|██████████| 42/42 [06:28<00:00,  9.24s/it]\n",
      "Generating answers:  52%|█████▏    | 52/100 [5:44:06<5:12:13, 390.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1319 answers for set 52.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch for set 53: 100%|██████████| 42/42 [06:34<00:00,  9.40s/it]\n",
      "Generating answers:  53%|█████▎    | 53/100 [5:50:40<5:06:44, 391.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1319 answers for set 53.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch for set 54: 100%|██████████| 42/42 [06:43<00:00,  9.60s/it]\n",
      "Generating answers:  54%|█████▍    | 54/100 [5:57:23<5:02:50, 395.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1319 answers for set 54.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch for set 55: 100%|██████████| 42/42 [06:26<00:00,  9.21s/it]\n",
      "Generating answers:  55%|█████▌    | 55/100 [6:03:50<4:54:24, 392.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1319 answers for set 55.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch for set 56: 100%|██████████| 42/42 [06:47<00:00,  9.70s/it]\n",
      "Generating answers:  56%|█████▌    | 56/100 [6:10:37<4:51:06, 396.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1319 answers for set 56.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch for set 57: 100%|██████████| 42/42 [06:31<00:00,  9.32s/it]\n",
      "Generating answers:  57%|█████▋    | 57/100 [6:17:09<4:43:17, 395.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1319 answers for set 57.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch for set 58: 100%|██████████| 42/42 [06:28<00:00,  9.26s/it]\n",
      "Generating answers:  58%|█████▊    | 58/100 [6:23:38<4:35:21, 393.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1319 answers for set 58.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch for set 59: 100%|██████████| 42/42 [06:41<00:00,  9.56s/it]\n",
      "Generating answers:  59%|█████▉    | 59/100 [6:30:19<4:30:28, 395.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1319 answers for set 59.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch for set 60: 100%|██████████| 42/42 [06:29<00:00,  9.28s/it]\n",
      "Generating answers:  60%|██████    | 60/100 [6:36:49<4:22:38, 393.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1319 answers for set 60.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch for set 61: 100%|██████████| 42/42 [06:32<00:00,  9.34s/it]\n",
      "Generating answers:  61%|██████    | 61/100 [6:43:21<4:15:46, 393.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1319 answers for set 61.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch for set 62: 100%|██████████| 42/42 [06:24<00:00,  9.16s/it]\n",
      "Generating answers:  62%|██████▏   | 62/100 [6:49:46<4:07:34, 390.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1319 answers for set 62.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch for set 63: 100%|██████████| 42/42 [06:29<00:00,  9.27s/it]\n",
      "Generating answers:  63%|██████▎   | 63/100 [6:56:16<4:00:47, 390.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1319 answers for set 63.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch for set 64: 100%|██████████| 42/42 [06:37<00:00,  9.46s/it]\n",
      "Generating answers:  64%|██████▍   | 64/100 [7:02:53<3:55:32, 392.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1319 answers for set 64.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch for set 65: 100%|██████████| 42/42 [06:28<00:00,  9.24s/it]\n",
      "Generating answers:  65%|██████▌   | 65/100 [7:09:21<3:48:13, 391.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1319 answers for set 65.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch for set 66: 100%|██████████| 42/42 [06:39<00:00,  9.51s/it]\n",
      "Generating answers:  66%|██████▌   | 66/100 [7:16:01<3:43:07, 393.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1319 answers for set 66.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch for set 67: 100%|██████████| 42/42 [06:48<00:00,  9.73s/it]\n",
      "Generating answers:  67%|██████▋   | 67/100 [7:22:50<3:39:02, 398.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1319 answers for set 67.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch for set 68: 100%|██████████| 42/42 [06:43<00:00,  9.61s/it]\n",
      "Generating answers:  68%|██████▊   | 68/100 [7:29:33<3:33:15, 399.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1319 answers for set 68.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch for set 69: 100%|██████████| 42/42 [06:33<00:00,  9.36s/it]\n",
      "Generating answers:  69%|██████▉   | 69/100 [7:36:06<3:25:34, 397.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1319 answers for set 69.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch for set 70: 100%|██████████| 42/42 [06:31<00:00,  9.33s/it]\n",
      "Generating answers:  70%|███████   | 70/100 [7:42:38<3:18:01, 396.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1319 answers for set 70.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch for set 71: 100%|██████████| 42/42 [06:40<00:00,  9.53s/it]\n",
      "Generating answers:  71%|███████   | 71/100 [7:49:19<3:12:02, 397.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1319 answers for set 71.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch for set 72: 100%|██████████| 42/42 [06:30<00:00,  9.31s/it]\n",
      "Generating answers:  72%|███████▏  | 72/100 [7:55:50<3:04:32, 395.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1319 answers for set 72.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch for set 73: 100%|██████████| 42/42 [06:32<00:00,  9.34s/it]\n",
      "Generating answers:  73%|███████▎  | 73/100 [8:02:22<2:57:29, 394.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1319 answers for set 73.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch for set 74: 100%|██████████| 42/42 [06:46<00:00,  9.68s/it]\n",
      "Generating answers:  74%|███████▍  | 74/100 [8:09:08<2:52:31, 398.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1319 answers for set 74.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch for set 75: 100%|██████████| 42/42 [06:26<00:00,  9.20s/it]\n",
      "Generating answers:  75%|███████▌  | 75/100 [8:15:35<2:44:24, 394.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1319 answers for set 75.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch for set 76: 100%|██████████| 42/42 [06:32<00:00,  9.34s/it]\n",
      "Generating answers:  76%|███████▌  | 76/100 [8:22:07<2:37:34, 393.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1319 answers for set 76.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch for set 77: 100%|██████████| 42/42 [06:30<00:00,  9.30s/it]\n",
      "Generating answers:  77%|███████▋  | 77/100 [8:28:38<2:30:36, 392.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1319 answers for set 77.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch for set 78: 100%|██████████| 42/42 [06:44<00:00,  9.62s/it]\n",
      "Generating answers:  78%|███████▊  | 78/100 [8:35:22<2:25:18, 396.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1319 answers for set 78.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch for set 79: 100%|██████████| 42/42 [06:34<00:00,  9.40s/it]\n",
      "Generating answers:  79%|███████▉  | 79/100 [8:41:57<2:18:33, 395.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1319 answers for set 79.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch for set 80: 100%|██████████| 42/42 [06:27<00:00,  9.23s/it]\n",
      "Generating answers:  80%|████████  | 80/100 [8:48:24<2:11:07, 393.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1319 answers for set 80.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch for set 81: 100%|██████████| 42/42 [06:42<00:00,  9.58s/it]\n",
      "Generating answers:  81%|████████  | 81/100 [8:55:07<2:05:25, 396.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1319 answers for set 81.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch for set 82: 100%|██████████| 42/42 [06:35<00:00,  9.43s/it]\n",
      "Generating answers:  82%|████████▏ | 82/100 [9:01:42<1:58:48, 396.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1319 answers for set 82.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch for set 83: 100%|██████████| 42/42 [06:51<00:00,  9.80s/it]\n",
      "Generating answers:  83%|████████▎ | 83/100 [9:08:34<1:53:32, 400.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1319 answers for set 83.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch for set 84: 100%|██████████| 42/42 [06:28<00:00,  9.25s/it]\n",
      "Generating answers:  84%|████████▍ | 84/100 [9:15:03<1:45:53, 397.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1319 answers for set 84.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch for set 85: 100%|██████████| 42/42 [06:45<00:00,  9.65s/it]\n",
      "Generating answers:  85%|████████▌ | 85/100 [9:21:48<1:39:52, 399.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1319 answers for set 85.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch for set 86: 100%|██████████| 42/42 [06:29<00:00,  9.27s/it]\n",
      "Generating answers:  86%|████████▌ | 86/100 [9:28:17<1:32:29, 396.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1319 answers for set 86.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch for set 87: 100%|██████████| 42/42 [06:36<00:00,  9.43s/it]\n",
      "Generating answers:  87%|████████▋ | 87/100 [9:34:53<1:25:52, 396.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1319 answers for set 87.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch for set 88: 100%|██████████| 42/42 [06:41<00:00,  9.55s/it]\n",
      "Generating answers:  88%|████████▊ | 88/100 [9:41:35<1:19:33, 397.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1319 answers for set 88.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch for set 89: 100%|██████████| 42/42 [06:36<00:00,  9.45s/it]\n",
      "Generating answers:  89%|████████▉ | 89/100 [9:48:11<1:12:52, 397.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1319 answers for set 89.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch for set 90: 100%|██████████| 42/42 [06:35<00:00,  9.42s/it]\n",
      "Generating answers:  90%|█████████ | 90/100 [9:54:47<1:06:09, 396.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1319 answers for set 90.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch for set 91: 100%|██████████| 42/42 [06:33<00:00,  9.38s/it]\n",
      "Generating answers:  91%|█████████ | 91/100 [10:01:21<59:24, 396.05s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1319 answers for set 91.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch for set 92: 100%|██████████| 42/42 [06:32<00:00,  9.34s/it]\n",
      "Generating answers:  92%|█████████▏| 92/100 [10:07:53<52:39, 394.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1319 answers for set 92.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch for set 93: 100%|██████████| 42/42 [06:41<00:00,  9.57s/it]\n",
      "Generating answers:  93%|█████████▎| 93/100 [10:14:35<46:19, 397.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1319 answers for set 93.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch for set 94: 100%|██████████| 42/42 [06:37<00:00,  9.46s/it]\n",
      "Generating answers:  94%|█████████▍| 94/100 [10:21:12<39:42, 397.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1319 answers for set 94.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch for set 95: 100%|██████████| 42/42 [06:26<00:00,  9.21s/it]\n",
      "Generating answers:  95%|█████████▌| 95/100 [10:27:39<32:49, 393.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1319 answers for set 95.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch for set 96: 100%|██████████| 42/42 [06:33<00:00,  9.37s/it]\n",
      "Generating answers:  96%|█████████▌| 96/100 [10:34:13<26:15, 393.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1319 answers for set 96.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch for set 97: 100%|██████████| 42/42 [06:42<00:00,  9.59s/it]\n",
      "Generating answers:  97%|█████████▋| 97/100 [10:40:55<19:49, 396.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1319 answers for set 97.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch for set 98: 100%|██████████| 42/42 [06:29<00:00,  9.27s/it]\n",
      "Generating answers:  98%|█████████▊| 98/100 [10:47:25<13:08, 394.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1319 answers for set 98.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch for set 99: 100%|██████████| 42/42 [06:43<00:00,  9.60s/it]\n",
      "Generating answers:  99%|█████████▉| 99/100 [10:54:08<06:37, 397.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1319 answers for set 99.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch for set 100: 100%|██████████| 42/42 [06:43<00:00,  9.61s/it]\n",
      "Generating answers: 100%|██████████| 100/100 [11:00:52<00:00, 396.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1319 answers for set 100.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "output_directory = \"generated_answers_llama3\"\n",
    "\n",
    "with torch.no_grad():\n",
    "    answers = generate_answers(input_text, generator, tokenizer, output_directory, n_answers=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Majority Voting Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "from collections import Counter\n",
    "import sys\n",
    "sys.path.append('Data/prm800k/prm800k')\n",
    "from grading import grader\n",
    "\n",
    "def load_json_answers(directory):\n",
    "    json_files = [file for file in os.listdir(directory) if file.endswith(\".json\")]\n",
    "    all_answers = []\n",
    "    for file in json_files:\n",
    "        file_path = os.path.join(directory, file)\n",
    "        with open(file_path, \"r\") as f:\n",
    "            answers = json.load(f)\n",
    "            all_answers.append(answers)\n",
    "    return all_answers\n",
    "\n",
    "def extract_answers(all_answers):\n",
    "    extracted_answers = [[] for _ in range(len(all_answers[0]))]\n",
    "    for answers in all_answers:\n",
    "        for i, answer in enumerate(answers):\n",
    "            match = re.search(r\"####\\s*(.*)\", answer)\n",
    "            if match:\n",
    "                extracted_answers[i].append(match.group(1).strip())\n",
    "            else:\n",
    "                extracted_answers[i].append(\"\")\n",
    "    return extracted_answers\n",
    "\n",
    "def majority_vote(extracted_answers):\n",
    "    majority_answers = []\n",
    "    for question_answers in extracted_answers:\n",
    "        valid_answers = [answer for answer in question_answers if answer and answer.strip()]\n",
    "        if valid_answers:\n",
    "            counter = Counter(valid_answers)\n",
    "            majority_answer = counter.most_common(1)[0][0]\n",
    "        else:\n",
    "            majority_answer = \"\"\n",
    "        majority_answers.append(majority_answer)\n",
    "    return majority_answers\n",
    "\n",
    "def compare_with_ground_truth(majority_answers, ground_truth_answers):\n",
    "    correct_count = 0\n",
    "    for majority_answer, ground_truth_answer in zip(majority_answers, ground_truth_answers):\n",
    "        if grader.grade_answer(majority_answer, ground_truth_answer):\n",
    "            correct_count += 1\n",
    "    accuracy = correct_count / len(ground_truth_answers)\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Usage example\n",
    "json_directory = \"generated_answers_llama3\"\n",
    "all_answers = load_json_answers(json_directory)\n",
    "extracted_answers = extract_answers(all_answers)\n",
    "majority_answers = majority_vote(extracted_answers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy: 0.83\n"
     ]
    }
   ],
   "source": [
    "accuracy = compare_with_ground_truth(majority_answers, dataset['final_answer'])\n",
    "\n",
    "# print(\"Majority Answers:\")\n",
    "# for answer in majority_answers:\n",
    "#     print(answer)\n",
    "\n",
    "print(f\"\\nAccuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PRM Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "sys.path.append('Data/prm800k/prm800k')\n",
    "from grading import grader\n",
    "\n",
    "def load_json_answers(directory):\n",
    "    json_files = [file for file in os.listdir(directory) if file.endswith(\".json\")]\n",
    "    all_answers = []\n",
    "    for file in json_files:\n",
    "        file_path = os.path.join(directory, file)\n",
    "        with open(file_path, \"r\") as f:\n",
    "            answers = json.load(f)\n",
    "            all_answers.append(answers)\n",
    "    return all_answers\n",
    "\n",
    "def extract_answers(all_answers):\n",
    "    extracted_answers = [[] for _ in range(len(all_answers[0]))]\n",
    "    for answers in all_answers:\n",
    "        for i, answer in enumerate(answers):\n",
    "            match = re.search(r\"####\\s*(.*)\", answer)\n",
    "            if match:\n",
    "                extracted_answers[i].append(match.group(1).strip())\n",
    "            else:\n",
    "                extracted_answers[i].append(\"\")\n",
    "    return extracted_answers\n",
    "\n",
    "def compute_probabilities(all_answers, critic_tokenizer, critic, batch_size=128):\n",
    "    answers_prob = [[] for _ in range(len(all_answers[0]))]\n",
    "    with torch.no_grad():\n",
    "        for answers in tqdm(all_answers, desc=\"Processing answers\"):\n",
    "            results = []\n",
    "            for answer in answers:\n",
    "                result = '\\n### Input:' + answer.split('assistant\\n\\n')[0].split('You are a helpful assistant to solve math problems step by step user\\n\\n')[1] + '### Response:\\n'\n",
    "                responses = answer.split('assistant\\n\\n')[1].split('\\n')\n",
    "                for response in responses:\n",
    "                    result += response + \"\\n\"\n",
    "                    results.append(result)\n",
    "                                \n",
    "            correct_probabilities = []\n",
    "            for i in range(0, len(results), batch_size):\n",
    "                batch_results = results[i:i+batch_size]\n",
    "                inputs = critic_tokenizer(batch_results, padding=\"max_length\", truncation=True, max_length=256, return_tensors=\"pt\").to(\"cuda\")\n",
    "                outputs = critic(**inputs)\n",
    "                probabilities = F.softmax(outputs.logits, dim=1)\n",
    "                correct_probability = probabilities[:, 1:]\n",
    "                correct_probability = torch.sum(correct_probability, dim=1)\n",
    "                correct_probabilities.extend(correct_probability.tolist())\n",
    "            \n",
    "            for i, answer in enumerate(answers):\n",
    "                num_responses = len(answer.split('assistant\\n\\n')[1].split('\\n'))\n",
    "                answer_prob = torch.tensor(correct_probabilities[i:i+num_responses]).prod().item()\n",
    "                answers_prob[i].append(answer_prob)\n",
    "    \n",
    "    return answers_prob\n",
    "\n",
    "def select_highest_probability_answers(extracted_answers, answers_prob):\n",
    "    highest_probability_answers = []\n",
    "    for i, question_answers in enumerate(extracted_answers):\n",
    "        question_probs = answers_prob[i]\n",
    "        if question_probs:\n",
    "            max_prob_index = question_probs.index(max(question_probs))\n",
    "            highest_probability_answer = question_answers[max_prob_index]\n",
    "        else:\n",
    "            highest_probability_answer = \"\"\n",
    "        highest_probability_answers.append(highest_probability_answer)\n",
    "    return highest_probability_answers\n",
    "\n",
    "def compare_with_ground_truth(majority_answers, ground_truth_answers):\n",
    "    correct_count = 0\n",
    "    for majority_answer, ground_truth_answer in zip(majority_answers, ground_truth_answers):\n",
    "        if grader.grade_answer(majority_answer, ground_truth_answer):\n",
    "            correct_count += 1\n",
    "    accuracy = correct_count / len(ground_truth_answers)\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch.nn.functional as F\n",
    "# answers_prob = []\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     for answer in answers:\n",
    "#         result = answer.split('### Response:\\n')[0] + '### Response:\\n'\n",
    "#         responses = answer.split('### Response:\\n')[1].split('\\n')\n",
    "#         correct_probability = 1\n",
    "#         incorrect_probability = 1\n",
    "#         results = []\n",
    "#         for response in responses:\n",
    "#             result += response + \"\\n\"\n",
    "#             results.append(result)\n",
    "#         inputs = critic_tokenizer(results, padding=\"max_length\", truncation=True, max_length=256, return_tensors=\"pt\").to(\"cuda\")\n",
    "#         outputs = critic(**inputs)\n",
    "#         #take the softmax of the logits\n",
    "#         probabilities = F.softmax(outputs.logits, dim=1)\n",
    "#         correct_probability = probabilities[:, 1:]\n",
    "#         correct_probability = torch.sum(correct_probability, dim=1)\n",
    "#         correct_probability = torch.prod(correct_probability)\n",
    "#         # answers_prob.append(correct_probability.to(\"cpu\").numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1915, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# correct_probability = 1 \n",
    "# for i in probabilities:\n",
    "#     correct_probability *= (i[2]+ i[1])\n",
    "# print(correct_probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correct_probability = probabilities[:, 1:]\n",
    "# correct_probability = torch.sum(correct_probability, dim=1)\n",
    "# correct_probability = torch.prod(correct_probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1915, device='cuda:0')"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# correct_probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# answer = \"system\\n\\nYou are a helpful assistant to solve math problems step by step user\\n\\nA robe takes 2 bolts of blue fiber and half that much white fiber.  How many bolts in total does it take?assistant\\n\\nIt takes 4 bolts of blue fiber 4/2=<<4/2=2>>2 bolt of white fiber.\\nSo it takes 4+2=<<4+2=6>>6 bolts total.\\n#### 3\"\n",
    "\n",
    "# results = []\n",
    "# result = '\\n### Input:' + answer.split('assistant\\n\\n')[0].split('You are a helpful assistant to solve math problems step by step user\\n\\n')[1] + '### Response:\\n'\n",
    "# responses = answer.split('assistant\\n\\n')[1].split('\\n')\n",
    "# results.extend([result + response + \"\\n\" for response in responses])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.012118585407733917"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import torch.nn.functional as F\n",
    "\n",
    "# correct_probabilities = []\n",
    "# inputs = critic_tokenizer(results, padding=\"max_length\", truncation=True, max_length=256, return_tensors=\"pt\").to(\"cuda\")\n",
    "# outputs = critic(**inputs)\n",
    "# probabilities = F.softmax(outputs.logits, dim=1)\n",
    "# correct_probability = probabilities[:, 1:]\n",
    "# correct_probability = torch.sum(correct_probability, dim=1)\n",
    "# correct_probabilities.extend(correct_probability.tolist())\n",
    "            \n",
    "# num_responses = len(answer.split('assistant\\n\\n')[1].split('\\n'))\n",
    "# answer_prob = torch.tensor(correct_probabilities).prod().item()\n",
    "# answer_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jqi/anaconda3/envs/llmrl/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForSequenceClassification(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForSequenceClassification(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(32000, 2048, padding_idx=0)\n",
       "        (layers): ModuleList(\n",
       "          (0-21): 22 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaSdpaAttention(\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2048, out_features=256, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=256, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2048, out_features=256, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=256, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2048, out_features=5632, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=5632, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2048, out_features=5632, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=5632, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=5632, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5632, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm()\n",
       "            (post_attention_layernorm): LlamaRMSNorm()\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm()\n",
       "      )\n",
       "      (score): ModulesToSaveWrapper(\n",
       "        (original_module): Linear(in_features=2048, out_features=3, bias=False)\n",
       "        (modules_to_save): ModuleDict(\n",
       "          (default): Linear(in_features=2048, out_features=3, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import LlamaForSequenceClassification, AutoTokenizer, LlamaForCausalLM\n",
    "import torch\n",
    "\n",
    "# Usage example\n",
    "json_directory = \"generated_answers_llama3\"\n",
    "# critic = LlamaForSequenceClassification.from_pretrained(\"/home/jqi/Github/LLMRL/checkpoints/tinyLlama-critic\")\n",
    "# critic.eval()\n",
    "\n",
    "# critic_tokenizer = AutoTokenizer.from_pretrained(\"/home/jqi/Github/LLMRL/checkpoints/tinyLlama-critic\", padding_side='left', use_fast= False)\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# #to cuda\n",
    "# critic.to(device)\n",
    "\n",
    "\n",
    "from peft import AutoPeftModelForSequenceClassification\n",
    "\n",
    "critic = AutoPeftModelForSequenceClassification.from_pretrained('/home/jqi/Github/LLMRL/checkpoints/tinyLlama-critic-lora-2epoch', num_labels = 3)\n",
    "critic.eval()\n",
    "\n",
    "critic_tokenizer = AutoTokenizer.from_pretrained(\"/home/jqi/Github/LLMRL/checkpoints/tinyLlama-critic\", padding_side='left', use_fast= False)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#to cuda\n",
    "critic.to(device)\n",
    "# print(\"Highest Probability Answers:\")\n",
    "# for answer in highest_probability_answers:\n",
    "#     print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing answers: 100%|██████████| 100/100 [2:58:16<00:00, 106.96s/it] \n"
     ]
    }
   ],
   "source": [
    "\n",
    "all_answers = load_json_answers(json_directory)\n",
    "extracted_answers = extract_answers(all_answers)\n",
    "answers_prob = compute_probabilities(all_answers, critic_tokenizer, critic)\n",
    "highest_probability_answers = select_highest_probability_answers(extracted_answers, answers_prob)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy: 0.59\n"
     ]
    }
   ],
   "source": [
    "accuracy = compare_with_ground_truth(highest_probability_answers, dataset['final_answer'])\n",
    "\n",
    "# print(\"Majority Answers:\")\n",
    "# for answer in majority_answers:\n",
    "#     print(answer)\n",
    "\n",
    "print(f\"\\nAccuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Another round of evaluation of the correct input for 1 epoch finetuning of classification\n",
    "#tiny llama Accuracy: 0.57\n",
    "#llama3 achieves Accuracy: 0.58"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy: 0.59\n"
     ]
    }
   ],
   "source": [
    "#change to the other probability\n",
    "accuracy = compare_with_ground_truth(highest_probability_answers, dataset['final_answer'])\n",
    "print(f\"\\nAccuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PRM llama3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "sys.path.append('Data/prm800k/prm800k')\n",
    "from grading import grader\n",
    "\n",
    "def load_json_answers(directory):\n",
    "    json_files = [file for file in os.listdir(directory) if file.endswith(\".json\")]\n",
    "    all_answers = []\n",
    "    for file in json_files:\n",
    "        file_path = os.path.join(directory, file)\n",
    "        with open(file_path, \"r\") as f:\n",
    "            answers = json.load(f)\n",
    "            all_answers.append(answers)\n",
    "    return all_answers\n",
    "\n",
    "def extract_answers(all_answers):\n",
    "    extracted_answers = [[] for _ in range(len(all_answers[0]))]\n",
    "    for answers in all_answers:\n",
    "        for i, answer in enumerate(answers):\n",
    "            match = re.search(r\"####\\s*(.*)\", answer)\n",
    "            if match:\n",
    "                extracted_answers[i].append(match.group(1).strip())\n",
    "            else:\n",
    "                extracted_answers[i].append(\"\")\n",
    "    return extracted_answers\n",
    "\n",
    "def compute_probabilities(all_answers, critic_tokenizer, critic, batch_size=128):\n",
    "    answers_prob = [[] for _ in range(len(all_answers[0]))]\n",
    "    with torch.no_grad():\n",
    "        for answers in tqdm(all_answers, desc=\"Processing answers\"):\n",
    "            results = []\n",
    "            for answer in answers:\n",
    "                result = answer.split('assistant\\n\\n')[0] + 'assistant\\n\\n'\n",
    "                responses = answer.split('assistant\\n\\n')[1].split('\\n')\n",
    "                for response in responses:\n",
    "                    result += response + \"\\n\"\n",
    "                    results.append(result)\n",
    "            \n",
    "            correct_probabilities = []\n",
    "            for i in range(0, len(results), batch_size):\n",
    "                batch_results = results[i:i+batch_size]\n",
    "                inputs = critic_tokenizer(batch_results, padding=\"max_length\", truncation=True, max_length=256, return_tensors=\"pt\").to(\"cuda\")\n",
    "                outputs = critic(**inputs)\n",
    "                probabilities = F.softmax(outputs.logits[:,-1], dim=1)\n",
    "                correct_probability = probabilities[:, 1:]\n",
    "                correct_probability = torch.sum(correct_probability, dim=1)\n",
    "                correct_probabilities.extend(correct_probability.tolist())\n",
    "            \n",
    "            for i, answer in enumerate(answers):\n",
    "                num_responses = len(answer.split('assistant\\n\\n')[1].split('\\n'))\n",
    "                answer_prob = torch.tensor(correct_probabilities[i:i+num_responses]).prod().item()\n",
    "                answers_prob[i].append(answer_prob)\n",
    "    \n",
    "    return answers_prob\n",
    "\n",
    "def select_highest_probability_answers(extracted_answers, answers_prob):\n",
    "    highest_probability_answers = []\n",
    "    for i, question_answers in enumerate(extracted_answers):\n",
    "        question_probs = answers_prob[i]\n",
    "        if question_probs:\n",
    "            max_prob_index = question_probs.index(max(question_probs))\n",
    "            highest_probability_answer = question_answers[max_prob_index]\n",
    "        else:\n",
    "            highest_probability_answer = \"\"\n",
    "        highest_probability_answers.append(highest_probability_answer)\n",
    "    return highest_probability_answers\n",
    "\n",
    "def compare_with_ground_truth(majority_answers, ground_truth_answers):\n",
    "    correct_count = 0\n",
    "    for majority_answer, ground_truth_answer in zip(majority_answers, ground_truth_answers):\n",
    "        if grader.grade_answer(majority_answer, ground_truth_answer):\n",
    "            correct_count += 1\n",
    "    accuracy = correct_count / len(ground_truth_answers)\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LlamaForSequenceClassification, AutoTokenizer, LlamaForCausalLM\n",
    "import torch\n",
    "\n",
    "# Usage example\n",
    "json_directory = \"generated_answers_llama3\"\n",
    "\n",
    "from unsloth.models.loader import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = False # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "\n",
    "critic, critic_tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"checkpoints/llama3-critic-1epoch\", # \"unsloth/tinyllama\" for 16bit loading\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    sequence_classification = True,\n",
    "    # num_labels = 3,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")\n",
    "# FastLanguageModel.for_inference(critic) # Enable native 2x faster inference\n",
    "critic.eval()\n",
    "critic.to(\"cuda\")\n",
    "critic_tokenizer.padding_side = \"left\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    all_answers = load_json_answers(json_directory)\n",
    "    extracted_answers = extract_answers(all_answers)\n",
    "    answers_prob = compute_probabilities(all_answers, critic_tokenizer, critic, batch_size=64)\n",
    "    highest_probability_answers = select_highest_probability_answers(extracted_answers, answers_prob)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmrl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
