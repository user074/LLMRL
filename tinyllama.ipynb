{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth: Fast Llama patching release 2024.4\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 4090. Max memory: 23.641 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.2.2. CUDA = 8.9. CUDA Toolkit = 11.8.\n",
      "\\        /    Bfloat16 = TRUE. Xformers = 0.0.25.post1. FA = True.\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = False # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
    "# fourbit_models = [\n",
    "#     \"unsloth/mistral-7b-bnb-4bit\",\n",
    "#     \"unsloth/mistral-7b-instruct-v0.2-bnb-4bit\",\n",
    "#     \"unsloth/llama-2-7b-bnb-4bit\",\n",
    "#     \"unsloth/llama-2-13b-bnb-4bit\",\n",
    "#     \"unsloth/codellama-34b-bnb-4bit\",\n",
    "#     \"unsloth/tinyllama-bnb-4bit\",\n",
    "#     \"unsloth/gemma-7b-bnb-4bit\", # New Google 6 trillion tokens model 2.5x faster!\n",
    "#     \"unsloth/gemma-2b-bnb-4bit\",\n",
    "# ] # More models at https://huggingface.co/unsloth\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"/home/jianing/Github/LLMRL/checkpoints/tinyllama-MetaMath\", # \"unsloth/tinyllama\" for 16bit loading\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "tokenizer.padding_side = \"left\" # Padding side for faster inference\n",
    "\n",
    "#well for llama3 'pre-trained models usually do not stop completions naturally.'\n",
    "#https://github.com/meta-llama/llama3/blob/main/example_text_completion.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> Eliza's rate per hour for the first 40 hours she works each week is $10. She also receives an overtime pay of 1.2 times her regular hourly rate. If Eliza worked for 45 hours this week, how much are her earnings for this week? Eliza's regular hourly rate is $10, so for the first 40 hours she works, she earns 40 * $10 = $400.\n",
      "For the overtime hours, she earns 1.2 * $10 = $12 per hour.\n",
      "For the overtime hours, she works 45 - 40 = 5 hours.\n",
      "So, her earnings for the overtime hours is 5 * $12 = $60.\n",
      "Therefore, her total earnings for the week is $400 + $60 = $460.\n",
      "#### 460\n",
      "The answer is: 460</s>\n"
     ]
    }
   ],
   "source": [
    "# alpaca_prompt = Copied from above\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    \"Eliza's rate per hour for the first 40 hours she works each week is $10. She also receives an overtime pay of 1.2 times her regular hourly rate. If Eliza worked for 45 hours this week, how much are her earnings for this week?\"\n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 256, do_sample=True, temperature=0.2, top_k=40, eos_token_id = tokenizer.eos_token_id)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"<s> Eliza's rate per hour for the first 40 hours she works each week is $10. She also receives an overtime pay of 1.2 times her regular hourly rate. If Eliza worked for 45 hours this week, how much are her earnings for this week? Eliza's regular hourly rate is $10, so for the first 40 hours she works, she earns 40 * $10 = $400.\\nFor the overtime hours, she earns 1.2 times her regular hourly rate, so she earns 1.2 * $10 = $12 per hour.\\nFor the overtime hours, she earns $12 * 45 = $520.\\nIn total, Eliza earns $400 + $520 = $920 for this week.\\n#### 920\\nThe answer is: 920</s>\"]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = model.generate(**inputs, max_new_tokens = 256, use_cache = True)\n",
    "tokenizer.batch_decode(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "checkpoint 2000 is the best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmrl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
